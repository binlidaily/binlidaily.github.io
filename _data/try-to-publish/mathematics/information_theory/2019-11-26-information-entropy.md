---
layout: post
title: Information Entropy
subtitle: 信息熵
author: Bin Li
tags: [Information Theory]
image: 
comments: true
published: true
---

　　本文从信息量开始，拓展到信息熵，希望能够搞清楚信息论里面的这些概念。

## 1. 信息量（Information Content）
　　信息量是一个**对信息的度量**，就好比时分秒对时间的度量一样，信息是相对于信息源对接收者接收到的信息而言的。那么如何衡量当一个事件发生时，我们能够接受到多少信息量呢？我们可以从随机事件发生概率的角度来看待这个衡量方式。

　　一个比较容易接受的是，如果**一个事件发生的概率越小，其发生后我们获得信息是越多的**。比如像太阳明天会从东方升起这样的大概率（为真，概率为 $1$）事件对我们来说基本上没有什么信息量，但是如果有一天太阳从西边出来了这种可以说概率极低的事件发生了信息量就巨大了。

　　如此，我们可以将衡量随机事件 $E$ 发生所包含信息量多少的方式看成是一个针对事件发生概率的函数 $h(E)$ ，且事件发生的概率 $P(E)$ 越小，其所包含的信息量 $h(E)$ 越大。即**要求$h(E)$函数要是连续且递减的。**

　　另外，这里需要强调一下信息量的独立性。对于相互独立的两个事件，我们观察到他们同时发生所包含的信息量和分别观察到他们单独发生时所包含的信息量应该是一样的。

　　那么，对于衡量随机事件 $E$ 的发生所包含信息量函数 $h(E)$ 必须要满足以下三个条件：
1. 信息量函数 $h(E)$ 必须是连续且递减的。
2. 当信息量 $h(E)$ 为 $0$ 时，事件发生的概率 $P(E)$ 为 $1$。
3. 当事件 $E$ 和事件 $F$ 相互独立时
    * $h(E, F)=h(E)+h(F)$
    * $P(E, F) = P(E) \times P(F)$

　　而满足以上三条公理的只有如下的对数函数，则信息量的表达式是：

$$
h(E)=\log{1\over{P(E)}}=-\log P(E)
$$

　　对于这样的信息量公式，其实底数可以取很多（比如，2、e、3、10等），一般按照香农的取法采用底数 $2$。

### 2. 信息熵 （Information Entropy）
　　信息量所度量的是一个事件发生后所包含的信息量，那么当信息接收者**面对所有可能发生的独立事件**时该如何衡量接收者接收到的信息量呢？借用数学的概念，可以用**所有可能发生的事件所包含的信息量的期望值**，即所谓的**信息熵**来衡量。

　　具体定义信息熵 $H(X)$ 前我们先做一些设定，一个离散的随机变量 $X$ 的取值为 $\\{x_1, x_2, \dots ,x_n\\}$，且取到每个值的概率对应为 $p_i=\\{X=x_i\\}$；那么对所有可能取值 $\\{X=x_1\\}, \\{X=x_2\\}, \dots, \\{X=x_n\\}$ 的信息量期望，即信息熵 $H(X)$ 为：

$$
H(X)=-\sum_{i=1}^n p_i \log p_i
$$

　　对于信息熵而言，值得注意的是：
1. $n$ 可以是无穷大。
2. 信息熵 $H(X)$ 只跟 $X$ 的分布有关，跟其具体的取值无关。
3. 如果 $n$ 是有限的，那么当 $X$ 时均匀分布时，即 $p_i={1\over n}$ $\forall i \in \\{1,2,\dots,n\\}$，$H(X)$ 取最大值 $\log n$。
   * 这里就可以从对于熵的不确定性解释上理解了（信息熵描述的就是对不确定性的一种度量，信息熵越大，不确定性就越大），当所有可能取值的取值概率一样时，观察者就根本没有办法有猜哪种取值最有可能的依据，那么这个时候取值结果当然是最不确定的，也可以说此时整体的信息量是最大的。
   * 所有取值之间的差距越大，取值结果就越有偏向，那么不确定性就越低。
4. $H(X)=0$ 等价于 $p_i = 0 \text{ and } p_j = 1, \forall i \neq j$。