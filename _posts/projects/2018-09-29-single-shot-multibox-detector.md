---
layout: post
title: Single Shot MultiBox Detector
subtitle:
author: Bin Li
tags: [Computer Vision]
image: 
comments: true
published: true
typora-root-url: ../../../binlidaily.github.io
typora-copy-images-to: ../../img/media
---


## SSD Tensorflow Implementation
Github ä¸Šæ—©å·²ç»å¼€æºäº† [SSD TensorFlow](https://github.com/balancap/SSD-Tensorflow) ç‰ˆï¼Œå› ä¸ºå…¶ä¸Šæ²¡æœ‰è¯¦ç»†è¯´æ˜è¯¥å¦‚ä½•è®­ç»ƒè‡ªå·±çš„æ•°æ®ï¼Œäºæ˜¯ç†Ÿæ‚‰ä¹‹ååœ¨æ­¤åšä¸€äº›è®°å½•ï¼Œæ–¹ä¾¿åæœŸæŸ¥çœ‹ã€‚

å°†é¡¹ç›® clone ä¸‹æ¥ä¹‹åï¼Œé¦–å…ˆå¯ä»¥è·‘ä¸€è·‘ demoï¼Œçœ‹ä¸‹ SSD æ£€æµ‹çš„æ•ˆæœï¼Œè§£å‹å·²ç»è®­ç»ƒå¥½åä¿å­˜ä¸‹æ¥çš„ SSD æ¨¡å‹çš„ checkpoint æ–‡ä»¶ï¼ˆåœ¨ ./checkpoint æ–‡ä»¶å¤¹ä¸­ï¼‰ï¼š

```shell
unzip ssd_300_vgg.ckpt.zip
```

æ¥ç€å°±å¯ä»¥å›åˆ°é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ demo çš„ jupyter äº†ï¼š
```shell
jupyter notebook notebooks/ssd_notebook.ipynb
```

å¦‚æœå‡ºç°æ‰¾ä¸åˆ° datasets çš„æŠ¥é”™ï¼Œç¡®ä¿ datasets æ–‡ä»¶å¤¹ä¸‹æœ‰ `__init__.py` æ–‡ä»¶ï¼Œå¹¶ä¸”åœ¨ jupyter ä¸­åŠ å…¥äº† `sys` å¯¼å…¥å½“å‰ç›®å½•ã€‚

åœ¨ jupyter ä¸­å¯ä»¥çœ‹åˆ°æµ‹è¯•çš„æ•ˆæœï¼Œç»™ä¸€å¼ ç…§ç‰‡èƒ½å¤Ÿè¾ƒä¸ºå‡†ç¡®åœ°æ‰¾åˆ°è¦æ£€æµ‹çš„ç›®æ ‡ã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯æµ‹è¯•ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨æ•°æ®è®­ç»ƒå¯¹åº”çš„æ¨¡å‹å‘¢ï¼Ÿ

### æ•°æ®å¤„ç†
é¦–å…ˆè¦å»[å®˜ç½‘](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)ä¸‹è½½ Pascal VOC 2007 æ•°æ®ï¼Œç„¶åå°†è¯¥æ•°æ®è½¬æ¢æˆ TensorFlow èƒ½å¤Ÿå¤„ç† TF-Records æ ¼å¼ï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»º tfrecords æ–‡ä»¶å¤¹ï¼Œç„¶åå¯¹åº”åˆ°ä½ å­˜æ”¾ä¸‹è½½å¥½çš„ voc2007 æ•°æ®é›†çš„åœ°å€ï¼š
```shell
DATASET_DIR=./VOC2007/
OUTPUT_DIR=./tfrecords
python tf_convert_data.py \
    --dataset_name=pascalvoc \
    --dataset_dir=${DATASET_DIR} \
    --output_name=voc_2007_train \
    --output_dir=${OUTPUT_DIR}
```

è¿™é‡ŒæŒ‡çš„æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¦è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„è¯ï¼Œé¦–å…ˆä¹Ÿè¦å°†æ•°æ®æ•´ç†æˆ Pascal VOC 2007 çš„æ ¼å¼ã€‚

> * Annotations  
> * ImageSets  
> * JPEGImages  
> * SegmentationClass 
> * SegmentationObject

train å’Œ val å…±5012 å¼ å›¾ç‰‡ï¼Œçœ‹äº†ä¸‹å…¶ä¸­çš„å›¾ç‰‡ï¼Œå‘ç°å¤§å°æ˜¯ä¸ä¸€æ ·çš„ï¼Œè€Œä¸”æ˜¯å·²ç»æ ‡æ³¨å¥½çš„ã€‚å¯ä»¥çœ‹åˆ°é€šè¿‡è½¬æ¢ï¼Œå°†æ‰€æœ‰ jpg æ ¼å¼çš„å›¾ç‰‡éƒ½è½¬æ¢æˆ TensorFlow ä¸­çš„ tf-records ç±»å‹çš„æ•°æ®ï¼š

![-w603](/img/media/15451909983753.jpg)

æƒ³ç”¨ä¸€ä¸‹å‘½ä»¤æ¥æµ‹è¯•ä¸‹ Pascal VOC 2007 çš„æ¨¡å‹ï¼š
```shell
DATASET_DIR=./tfrecords
EVAL_DIR=./logs/
CHECKPOINT_PATH=./checkpoints/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt
python eval_ssd_network.py \
    --eval_dir=${EVAL_DIR} \
    --dataset_dir=${DATASET_DIR} \
    --dataset_name=pascalvoc_2007 \
    --dataset_split_name=test \
    --model_name=ssd_300_vgg \
    --checkpoint_path=${CHECKPOINT_PATH} \
    --batch_size=1
```

æŠ¥äº†ä¸€ä¸‹é”™è¯¯ï¼š
```shell
ValueError: No data files found in ./tfrecords/voc_2007_test_*.tfrecord
```

å¾—å» [Pascal VOC å®˜ç½‘](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)å†ä¸‹è½½æµ‹è¯•çš„æ•°æ®ï¼ŒæŒ‰ä¸Šè¿°æµç¨‹å°†æµ‹è¯•æ•°æ®ä¹Ÿè½¬æ¢æˆ tf-records æ ¼å¼ï¼š

```shell
DATASET_DIR=./VOC2007/test/
OUTPUT_DIR=./tfrecords
python tf_convert_data.py \
    --dataset_name=pascalvoc \
    --dataset_dir=${DATASET_DIR} \
    --output_name=voc_2007_test \
    --output_dir=${OUTPUT_DIR}
```

æ¥ç€åˆæŠ¥äº†ä¸€ä¸ªé”™è¯¯ï¼š

```shell
Traceback (most recent call last):
  File "eval_ssd_network.py", line 346, in <module>
    tf.app.run()
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 125, in run
    _sys.exit(main(argv))
  File "eval_ssd_network.py", line 240, in main
    tp_fp_metric = tfe.streaming_tp_fp_arrays(num_gbboxes, tp, fp, rscores)
  File "/home/binli/SSD-Tensorflow/tf_extended/metrics.py", line 151, in streaming_tp_fp_arrays
    name=scope)
  File "/home/binli/SSD-Tensorflow/tf_extended/metrics.py", line 178, in streaming_tp_fp_arrays
    v_nobjects = _create_local('v_num_gbboxes', shape=[], dtype=tf.int64)
  File "/home/binli/SSD-Tensorflow/tf_extended/metrics.py", line 56, in _create_local
    validate_shape=validate_shape)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py", line 185, in __call__
    return cls._variable_v2_call(*args, **kwargs)
TypeError: _variable_v2_call() got an unexpected keyword argument 'collections'
```
é™çº§ conda ä¸­çš„ TensorFlow åˆ°1.6ç‰ˆæœ¬åï¼š
```shell
conda install tensorflow-1.6.0-py27_0.tar.bz2
```
åˆå†’å‡ºé”™è¯¯ï¼š
```shell
Traceback (most recent call last):
  File "eval_ssd_network.py", line 346, in <module>
    tf.app.run()
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 126, in run
    _sys.exit(main(argv))
  File "eval_ssd_network.py", line 320, in main
    session_config=config)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py", line 212, in evaluate_once
    config=session_config)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py", line 188, in _evaluate_once
    eval_step_value = _get_latest_eval_step_value(eval_ops)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py", line 76, in _get_latest_eval_step_value
    with ops.control_dependencies(update_ops):
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4746, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4446, in control_dependencies
    c = self.as_graph_element(c)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3459, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3548, in _as_graph_element_locked
    types_str))
TypeError: Can not convert a tuple into a Tensor or Operation.
```

ç„¶ååšä¸€ä¸‹å¦‚ä¸‹æ“ä½œï¼š
```python
# import
from compiler.ast import flatten

# change two eval_op
eval_op=list(names_to_updates.values()),
eval_op=flatten(list(names_to_updates.values())),
```

ç„¶ååˆæŠ¥é”™äº†â€¦â€¦
```shell
NotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```

è¿™è¦å°±æˆ‘ä»¬æŠŠ VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt.zip è§£å‹å‡ºæ¥çš„ä¸¤ä¸ªæ–‡ä»¶æ”¾åˆ°ä¸€ä¸ªåå« `VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt` çš„æ–‡ä»¶å¤¹é‡Œï¼Œå¹¶è‡³äº checkpoints æ–‡ä»¶å¤¹ä¸‹ã€‚
```shell
CHECKPOINT_PATH=./checkpoints/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt
python eval_ssd_network.py \
    --eval_dir=${EVAL_DIR} \
    --dataset_dir=${DATASET_DIR} \
    --dataset_name=pascalvoc_2007 \
    --dataset_split_name=test \
    --model_name=ssd_300_vgg \
    --checkpoint_path=${CHECKPOINT_PATH} \
    --batch_size=1
```

ç„¶åç»ˆäºå¯ä»¥æµ‹è¯•äº†ğŸ¤£ã€‚

è“é¹…ï¼Œå¤ªå¤©çœŸäº†ï¼Œè·‘å‡ºäº† mAP åæœç„¶åˆæŠ¥é”™äº†ï¼š
```shell
Traceback (most recent call last):
  File "eval_ssd_network.py", line 346, in <module>
    tf.app.run()
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 126, in run
    _sys.exit(main(argv))
  File "eval_ssd_network.py", line 320, in main
    session_config=config)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py", line 212, in evaluate_once
    config=session_config)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 658, in __exit__
    self._close_internal(exception_type)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 690, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py", line 311, in end
    summary_str = session.run(self._summary_op, self._feed_dict)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1355, in _do_run
    options, run_metadata)
  File "/home/binli/anaconda2/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate tag ssd_losses/cross_entropy_pos/value_1 found in summary inputs
	 [[Node: Merge/MergeSummary = MergeSummary[N=93, _device="/job:localhost/replica:0/task:0/device:CPU:0"](pascalvoc_2007_data_provider/parallel_read/filenames/fraction_of_32_full, pascalvoc_2007_data_provider/parallel_read/fraction_of_2_full, batch/fraction_of_5_full, ssd_losses/cross_entropy_pos/value_1, ssd_losses/cross_entropy_pos/value_1, ssd_losses/localization/value_1, ssd_losses/localization/value_1, ssd_losses/cross_entropy_neg/value_1, ssd_losses/cross_entropy_neg/value_1, AP_VOC07/1, AP_VOC07/1, AP_VOC12/1, AP_VOC12/1, AP_VOC07/2, AP_VOC07/2, AP_VOC12/2, AP_VOC12/2, AP_VOC07/3, AP_VOC07/3, AP_VOC12/3, AP_VOC12/3, AP_VOC07/4, AP_VOC07/4, AP_VOC12/4, AP_VOC12/4, AP_VOC07/5, AP_VOC07/5, AP_VOC12/5, AP_VOC12/5, AP_VOC07/6, AP_VOC07/6, AP_VOC12/6, AP_VOC12/6, AP_VOC07/7, AP_VOC07/7, AP_VOC12/7, AP_VOC12/7, AP_VOC07/8, AP_VOC07/8, AP_VOC12/8, AP_VOC12/8, AP_VOC07/9, AP_VOC07/9, AP_VOC12/9, AP_VOC12/9, AP_VOC07/10, AP_VOC07/10, AP_VOC12/10, AP_VOC12/10, AP_VOC07/11, AP_VOC07/11, AP_VOC12/11, AP_VOC12/11, AP_VOC07/12, AP_VOC07/12, AP_VOC12/12, AP_VOC12/12, AP_VOC07/13, AP_VOC07/13, AP_VOC12/13, AP_VOC12/13, AP_VOC07/14, AP_VOC07/14, AP_VOC12/14, AP_VOC12/14, AP_VOC07/15, AP_VOC07/15, AP_VOC12/15, AP_VOC12/15, AP_VOC07/16, AP_VOC07/16, AP_VOC12/16, AP_VOC12/16, AP_VOC07/17, AP_VOC07/17, AP_VOC12/17, AP_VOC12/17, AP_VOC07/18, AP_VOC07/18, AP_VOC12/18, AP_VOC12/18, AP_VOC07/19, AP_VOC07/19, AP_VOC12/19, AP_VOC12/19, AP_VOC07/20, AP_VOC07/20, AP_VOC12/20, AP_VOC12/20, AP_VOC07/mAP, Print, AP_VOC12/mAP, Print_1)]]
```
æš‚ä¸”å…ˆä¸ç®¡äº†å§ï¼Œç”¨è‡ªå·±çš„æ•°æ®å‡ºé—®é¢˜äº†å†è¯´â€¦â€¦ğŸ˜“

### æ¨¡å‹è®­ç»ƒ
#### Fine-tuning existing SSD checkpoints
```shell
DATASET_DIR=./tfrecords
TRAIN_DIR=./logs/
CHECKPOINT_PATH=./checkpoints/ssd_300_vgg.ckpt
python train_ssd_network.py \
    --train_dir=${TRAIN_DIR} \
    --dataset_dir=${DATASET_DIR} \
    --dataset_name=pascalvoc_2012 \
    --dataset_split_name=train \
    --model_name=ssd_300_vgg \
    --checkpoint_path=${CHECKPOINT_PATH} \
    --save_summaries_secs=60 \
    --save_interval_secs=600 \
    --weight_decay=0.0005 \
    --optimizer=adam \
    --learning_rate=0.001 \
    --batch_size=32
```

å¦‚æœæƒ³è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼Œå³ä»å¤´å¼€å§‹è®­ç»ƒçš„è¯ï¼Œå°±å–æ¶ˆ CHECKPOINT_PATH çš„è®¾ç½®å³å¯ã€‚

```python
DATASET_DIR=./tfrecords
TRAIN_DIR=./logs/
python train_ssd_network.py \
    --train_dir=${TRAIN_DIR} \
    --dataset_dir=${DATASET_DIR} \
    --dataset_name=pascalvoc_2007 \
    --dataset_split_name=train \
    --model_name=ssd_300_vgg \
    --save_summaries_secs=60 \
    --save_interval_secs=600 \
    --weight_decay=0.0005 \
    --optimizer=adam \
    --learning_rate=0.001 \
    --batch_size=32
```

nvcc å’Œ nvdia-smi éƒ½æå®šåï¼Œè·‘ train çš„å‘½ä»¤è¡Œï¼Œåˆå‡ºç°äº†é”™è¯¯ï¼š
```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Default MaxPoolingOp only supports NHWC on device type CPU
```

## åˆ©ç”¨è‡ªå·±çš„æ•°æ®è¿›è¡Œè®­ç»ƒ
### è½¬åŒ–æ•°æ®
æ•°æ®æ”¾åœ¨äº† DATASETæ•°æ®æ”¾åœ¨äº† DATASET_DIR=/mnt/archive/binli/oil_thermostat/ ä¸­ï¼Œé¦–å…ˆå°†æ•°æ®é‡å‘½åï¼Œç„¶ååˆ’åˆ†å‡ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚



Mac ä¸Šå¦‚ä½•å…³æ‰è¿™ä¸ªç«¯å£å‘¢ï¼Ÿ
```shell
sudo lsof -nPi :yourPortNumber
# then
sudo kill -9 yourPIDnumber
```

`nvidia-smi` æ˜¯ç”± cuda å’Œé©±åŠ¨æä¾›çš„ï¼Œäºæ˜¯å°±è¦å¯¹åº”çš„æ‰¾èµ„æºå°è¯•å®‰è£… cuda_10.0.130_410.48_linux.run å’Œ NVIDIA-Linux-x86_64-410.78.runã€‚

ç»“æœåˆæŠ¥é”™äº†ï¼Œæç¤ºæ²¡æœ‰è£… gccï¼Œäºæ˜¯ä¸‹äº† gcc-4.8.5-7.tar.bz2 ç”¨ conda ç¦»çº¿å®‰è£…ï¼ŒæŠ¥å¦‚ä¸‹é”™è¯¯ï¼š

```shell
Can't install the gcc package unless your system has crtXXX.o.
```

å› ä¸ºæœåŠ¡å™¨æ²¡è”ç½‘æ€»æ˜¯ç”¨ç¦»çº¿åŒ…è£…ï¼Œè€å‡ºé—®é¢˜ï¼Œç„¶åæƒ³åŠæ³•è¿ä¸Šäº†ç½‘ï¼Œé‡è£…äº†ç³»ç»Ÿçš„ gcc å°± OK äº†ã€‚

ç„¶åå¼€å§‹è£… cuda çš„é©±åŠ¨ï¼Œåä¸€ä¸ªå…¶å®å¯ä»¥ä¸ç”¨è£…ï¼Œå¤´ä¸€ä¸ªä¸Šå·²ç»å›Šæ‹¬äº†ã€‚è£…å¥½äº†è¦æ‰‹åŠ¨é…ç½®ä¸‹è·¯å¾„ï¼š

```shell
export PATH=$PATH:/usr/local/cuda-10.0/bin
export CUDADIR=/usr/local/cuda-10.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64
```

ç„¶åå°±å¯ä»¥ç”¨ `nvcc` å’Œ `nvdia-smi` æ¥æµ‹è¯•ä¸‹ç¯å¢ƒå’Œé©±åŠ¨æœ‰æ²¡æœ‰è£…å¥½ã€‚


### åŸºäºè‡ªå·±çš„æ•°æ®è®­ç»ƒæ¨¡å‹
#### æ•°æ®è½¬åŒ–æˆ PasalVOC æ•°æ®æ ¼å¼
é¦–å…ˆç¬¬ä¸€æ­¥æ˜¯å»ºç«‹å¥½æ–‡ä»¶å¤¹ï¼š

> * Annotations  
> * ImageSets  
> * JPEGImages  
> * SegmentationClass 
> * SegmentationObject

**1ã€é‡å‘½å**

ç„¶åè¦å¯¹æ–‡ä»¶åè¿›è¡Œé‡å‘½åï¼Œxml å’Œ jpg æ–‡ä»¶åå­—è¦å¯¹åº”ä¸Šï¼Œå›¾çœäº‹å°±ç›´æ¥ç”¨äº†ç°æˆçš„ä»£ç ï¼š
```python
import os  
      
class BatchRename():  
      
        def __init__(self):  

            self.path = '../'  
            self.type = '.jpg'
      
        def rename(self):  
            filelist = os.listdir(self.path)  
            total_num = len(filelist)  
            i = 1  
            n = 6  
            for item in filelist:  
                if item.endswith(self.type):  
                    n = 6 - len(str(i))  
                    src = os.path.join(os.path.abspath(self.path), item)  
                    dst = os.path.join(os.path.abspath(self.path), str(0)*n + str(i) + self.type)  
                    try:  
                        os.rename(src, dst)  
                        print 'converting %s to %s ...' % (src, dst)  
                        i = i + 1  
                  
                    except:  
                        continue  
            print 'total %d to rename & converted %d jpgs' % (total_num, i)  
      
if __name__ == '__main__':  
        demo = BatchRename()  
        demo.rename()  
```

é‡å‘½åéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ€å¥½åœ¨æ‰€æœ‰çš„ xml å’Œ jpg éƒ½åœ¨åŒ ä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œçš„æ—¶å€™è¿›è¡Œé‡å‘½åæ“ä½œã€‚

**2ã€åˆ’åˆ†æ•°æ®**

å› ä¸ºå·²ç»æ ‡è®°å¥½äº†æ•°æ®ï¼Œæ¥ä¸‹æ¥å°±æ˜¯è¦ç”Ÿæˆå¯¹åº”çš„è®­ç»ƒéªŒè¯æ•°æ®çš„æ–‡ä»¶ï¼Œæ–¹ä¾¿æ¨¡å‹æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ï¼š

```python
import os  
import random   
  
xmlfilepath=r'/Users/Bin/Downloads/oil/Annotations'  
ImageSetsPath=r"/Users/Bin/Downloads/oil/ImageSets/"  
  
trainval_percent=0.7 
train_percent=0.7  
total_xml = os.listdir(xmlfilepath)  
num=len(total_xml)    
list=range(num)    
tv=int(num*trainval_percent)    
tr=int(tv*train_percent)    
trainval= random.sample(list,tv)    
train=random.sample(trainval,tr)    
  
print("train and val size",tv)  
print("train size",tr)  
ftrainval = open(os.path.join(ImageSetsPath,'Main/trainval.txt'), 'w')    
ftest = open(os.path.join(ImageSetsPath,'Main/test.txt'), 'w')    
ftrain = open(os.path.join(ImageSetsPath,'Main/train.txt'), 'w')    
fval = open(os.path.join(ImageSetsPath,'Main/val.txt'), 'w')    
  
for i  in list:    
    name=total_xml[i][:-4]+'\n'    
    if i in trainval:    
        ftrainval.write(name)    
        if i in train:    
            ftrain.write(name)    
        else:    
            fval.write(name)    
    else:    
        ftest.write(name)    
    
ftrainval.close()    
ftrain.close()    
fval.close()    
ftest .close() 
```

3ã€ä¿®æ”¹æ–‡ä»¶

**ä¿®æ”¹ /datasets/pascalvoc_common.py æ–‡ä»¶**

ä¿®æ”¹ç±»åˆ«ï¼Œæ”¹æˆè‡ªå·±æ•°æ®çš„ç±»åˆ«ï¼š

```python
# before
# VOC_LABELS = {
#     'none': (0, 'Background'),
#     'aeroplane': (1, 'Vehicle'),
#     'bicycle': (2, 'Vehicle'),
#     'bird': (3, 'Animal'),
#     'boat': (4, 'Vehicle'),
#     'bottle': (5, 'Indoor'),
#     'bus': (6, 'Vehicle'),
#     'car': (7, 'Vehicle'),
#     'cat': (8, 'Animal'),
#     'chair': (9, 'Indoor'),
#     'cow': (10, 'Animal'),
#     'diningtable': (11, 'Indoor'),
#     'dog': (12, 'Animal'),
#     'horse': (13, 'Animal'),
#     'motorbike': (14, 'Vehicle'),
#     'person': (15, 'Person'),
#     'pottedplant': (16, 'Indoor'),
#     'sheep': (17, 'Animal'),
#     'sofa': (18, 'Indoor'),
#     'train': (19, 'Vehicle'),
#     'tvmonitor': (20, 'Indoor'),
# }

# after changing
VOC_LABELS = {
    'none': (0, 'Background'),
    'oil_thermostat': (1, 'oil_thermostat'),
    'white_indicator': (2, 'indicator'),
    'red_indicator': (3, 'indicator')
}
```

**ä¿®æ”¹ datasets/pascalvoc_to_tfrecords.py æ–‡ä»¶**

ä¿®æ”¹ 83ã€84 è¡Œï¼Œå›¾ç‰‡ç±»å‹ä»¥åŠæ–‡ä»¶æ ¼å¼ã€‚
```python
# Read the image file.
    filename = directory + DIRECTORY_IMAGES + name + '.jpg'
    image_data = tf.gfile.FastGFile(filename, 'rb').read()
```

ç„¶ååœ¨ 67 è¡Œå¯ä»¥ä¿®æ”¹è½¬æ¢ä¹‹åæ¯ä¸€ä¸ª tfrecoard æ–‡ä»¶å¯¹åº”å¤šå°‘å¼ å›¾ç‰‡ï¼š
```python
# TFRecords convertion parameters.
RANDOM_SEED = 4242
SAMPLES_PER_FILES = 100
```

**ä¿®æ”¹ nets/ssd_vgg_300.py  æ–‡ä»¶**

95 å’Œ 96 è¡Œçš„ç±»åˆ«ä¸ªæ•°æŒ‰ç…§è‡ªå·±æ•°æ®çš„æƒ…å†µæ¥åšä¿®æ”¹ï¼Œå¤§å°ä¸ºç±»åˆ«ï¼‹1ï¼š
```python
# before
# num_classes=21,
# no_annotation_label=21,
# change to 4
```

**ä¿®æ”¹ ./train_ssd_network.py æ–‡ä»¶**

 å¯¹åº”åœ°ä¿®æ”¹è®­ç»ƒé…ç½®ï¼ŒåŒ…æ‹¬è¿­ä»£æ¬¡æ•°(154è¡Œ)ï¼Œbatch å¤§å°ï¼ŒGPU ç”¨é‡ç­‰ã€‚

**ä¿®æ”¹ ./eval_ssd_network.py ç±»åˆ«ä¸ªæ•°**



**ä¿®æ”¹ datasets/pascalvoc_2007.py é…ç½®**

```python
TRAIN_STATISTICS = {
'none': (0, 0),
'pantograph': (1000, 1000),
}
TEST_STATISTICS = {
'none': (0, 0),
'pantograph': (1000, 1000),
}
SPLITS_TO_SIZES = {
'train': 500,
'test': 500,
}
```

4ã€è®­ç»ƒ

ä¸è¦ç”¨ checkpoint å°±å¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒäº†ã€‚

```
DATASET_DIR=./tfrecords
TRAIN_DIR=./logs/
python train_ssd_network.py \
    --train_dir=${TRAIN_DIR} \
    --dataset_dir=${DATASET_DIR} \
    --dataset_name=pascalvoc_2007 \
    --dataset_split_name=train \
    --model_name=ssd_300_vgg \
    --save_summaries_secs=60 \
    --save_interval_secs=600 \
    --weight_decay=0.0005 \
    --optimizer=adam \
    --learning_rate=0.001 \
    --batch_size=32
```


5ã€æµ‹è¯•
```shell
DataLossError: Unable to open table file ../checkpoints/model.ckpt-2938.ckpt: Failed precondition: ../checkpoints/model.ckpt-2938.ckpt: perhaps your file is in a different file format and you need to use a different restore operator?
```

è¿™ä¸ªé—®é¢˜çš„è§£å†³å¾ˆå‚»â€¦â€¦åªè¦å»æ‰æ–‡ä»¶å¤¹ç»“å°¾çš„`.ckpt`å°±å¯ä»¥äº†ï¼Œä¸éœ€è¦æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚

ç„¶è€Œç»§ç»­è¿è¡Œçš„æ—¶å€™åˆå‡ºç°äº†ä¸€ä¸ªé—®é¢˜ï¼š
```shell
Cannot feed value of shape (1080, 1920, 4) for Tensor u'Placeholder_5:0', which has shape '(?, ?, 3)'
```
å‘ç°åŸæ¥æ˜¯è¯» png å›¾ç‰‡æœ‰å››ä¸ªé€šé“ï¼Œé™¤äº† RGB è¿˜æœ‰ä¸€ä¸ª alpha é€šé“ï¼Œåœ¨ jupyter ä¸Šåšä¸‹é¢çš„ä¿®æ”¹ï¼š
```python
if len(img.shape) > 2 and img.shape[2] == 4:
    #convert the image from RGBA2RGB
    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)
```
æ­¤æ—¶è™½ç„¶æ²¡æœ‰æŠ¥é”™äº†ï¼Œä½†æ˜¯è·‘å‡ºæ¥çš„ç»“æœæ˜¯æ ¹æœ¬æ²¡æœ‰ç»“æœï¼Œä¸«æ ¹æœ¬æ²¡æœ‰æ£€æµ‹ğŸ¤·â€â™€ï¸ï¼çŒœæƒ³å¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒçš„ç¨‹åº¦ä¸å¤Ÿï¼Œæ¯•ç«Ÿåªç”¨äº†5000è½®è¿­ä»£ï¼Œäºæ˜¯æƒ³æ”¹æˆäº”ä¸‡è¯•ä¸€ä¸‹ï¼Œç»“æœå‡†å¤‡è¯•çš„æ—¶å€™å°±æŠ¥äº† OOM çš„é”™è¯¯ï¼ŒåŸæ¥æ˜¯å› ä¸ºå¼€äº†ä¸€ä¸ª jupyter æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹å°±å†…å­˜å‘Šæ€¥äº†ï¼Œä¸€å¼ å¡çœŸæ˜¯å¯æ€œï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å‡å°‘ä¸€ç‚¹ batch size çš„é‡ï¼Œè€—æ—¶ç‚¹ã€‚

ä¸€éæµ‹è¯•ä¸åˆ°ç»“æœå¯ä»¥å¢åŠ è¿­ä»£æ¬¡æ•°åŒæ—¶é™ä½ learning rate è¯•ä¸€ä¸‹ï¼Œæ˜¨æ™šè¯•äº†ä¸‹è¿­ä»£æ¬¡æ•°è°ƒåˆ°äº”ä¸‡ï¼Œéœ‡è¡å¾—æ¯”è¾ƒå‰å®³ï¼Œç»“æœä»ç„¶æ£€æµ‹ä¸åˆ°ï¼Œæ—©ä¸Šæ¥åˆè¯•äº†ä¸‹é™ä½ lr è¯•ä¸‹ï¼Œä¾ç„¶ä¸è¡Œï¼Œåæ¥ç»ˆäºæ‰¾åˆ°é—®é¢˜ä¹‹æ‰€åœ¨äº†â€¦â€¦

```python
rclasses, rscores, rbboxes = process_image(img, select_threshold=0.2)
```

è¿™é‡Œå¯ä»¥åœ¨ tensorboard ä¸ŠæŸ¥çœ‹ loss çš„å˜åŒ–ï¼š
```python
tensorboard --logdir=./logs --port=9998
```

é‡æ–°å°è¯•è®­ç»ƒè¡¨ç›˜ç±»çš„è¯†åˆ«æ—¶ï¼Œåœ¨æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ—¶åˆæœ‰æŠ¥é”™ï¼š

```python
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [8] rhs shape= [12]
```

è¿™ä¸ªè§£å†³åŠæ³•å±…ç„¶æ˜¯åœ¨è®­ç»ƒæ¨¡å‹æ—¶è®¾å®šå‚æ•°è¦åŠ ä¸Š num_classesï¼Œæ€€ç–‘æ˜¯åœ¨æŸå¤„ä¿®æ”¹æ—¶æ²¡æœ‰æ”¹å…¨ï¼š

```python
python train_ssd_network.py \
--train_dir=./logs/ \
--dataset_dir=./tfrecords \
--dataset_name=pascalvoc_2007 \
--dataset_split_name=train \
--model_name=ssd_300_vgg \
--save_summaries_secs=60 \
--save_interval_secs=600 \
--weight_decay=0.0005 \
--optimizer=adam \
--learning_rate=0.001 \
--batch_size=32 \
--ignore_missing_vars=True \
--num_classes=2 \
--checkpoint_exclude_scopes=ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/ block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box
```

ä½¿ç”¨ batch_size = 2ï¼Œå¥½åƒéœ‡è¡å¾ˆå‰å®³ï¼Ÿï¼ä½†æ˜¯ç»“æœå´å¼‚å¸¸çš„å¥½ï¼Œéœ‡è¡ä¸éœ‡è¡ï¼Œè¿˜æ˜¯è¦ç”¨ tensorboard çœ‹ã€‚æ•°æ®é‡æ¯”è¾ƒå°æ—¶è¿˜æ˜¯è¦ç”¨æ¯”è¾ƒå°çš„ batch_sizeã€‚


![image-20190315194152295](/img/media/image-20190315194152295.png)

## ä¼˜åŒ–
ç›´æ¥é€šè¿‡æ ‡æ³¨æ•°æ®è¯†åˆ«è¡¨ç›˜å’ŒæŒ‡é’ˆæ•ˆæœéå¸¸å·®ï¼Œå› ä¸ºå›¾ç‰‡æ¯”è¾ƒå°‘ï¼Œç»“æœæ˜¯ç”»å‡ºçš„å›¾ç½®ä¿¡åº¦å¾ˆå°ï¼Œè€Œä¸”æ¡†å¾ˆå°ï¼Œå¤„äºä¹±æ¡†çš„çŠ¶æ€ã€‚

äºæ˜¯é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå•ç‹¬è¯†åˆ«è¡¨ç›˜ï¼Œå‘ç°è¯†åˆ«è¡¨ç›˜çš„å‡†ç¡®ç‡æé«˜ï¼ŒåŸºæœ¬ä¸Šæ¥è¿‘äº1ã€‚äºæ˜¯ç»§ç»­é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå•ç‹¬è¯†åˆ«æŒ‡é’ˆï¼Œå‘ç°çº¯æŒ‡é’ˆçš„è¯†åˆ«ç‡å¾ˆä½ã€‚é‚£ä¹ˆæˆ‘å°±æƒ³åˆ°é€šè¿‡åœ¨è¯†åˆ«ç‡æ¥è¿‘ 1 çš„è¡¨ç›˜çš„åŸºç¡€ä¸Šæˆªä¸‹è¡¨ç›˜å±€éƒ¨çš„å›¾æ¥ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå†æ¥è®­ç»ƒè¯†åˆ«æŒ‡é’ˆçš„ç»“æœï¼Œç»“æœå‘ç°è™½ç„¶ç½®ä¿¡åº¦ä¸æ˜¯å¾ˆé«˜ï¼Œä½†æ˜¯åŸºæœ¬ä¸Šè¿˜æ˜¯èƒ½æ¡†ä½æŒ‡é’ˆçš„ã€‚

æ•ˆæœï¼šç»å¯¹è¯¯å·®ä¸è¶…è¿‡æ­£è´Ÿ4ï¼Œè¯¯å·®å‡æ–¹æ ¹å¤§æ¦‚1.87ã€‚

### Recording
import matplotlib.image as mpimg
ç”¨ mpimg.imsave æ¯”ç”¨ cv2.imwrite å­˜çš„å›¾ç‰‡æ•ˆæœå¥½å¾ˆå¤šï¼ã€‚

## References

1. [Single Shot MultiBox Detector (SSD) and Implement It in Pytorch](https://medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad)
2. [A guide to receptive field arithmetic for Convolutional Neural Networks](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)
3. [Faster R-CNN Explained](https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8)
4. [Preparing Inputs](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md)
5. [Face Detection for CCTV surveillance](https://blog.usejournal.com/face-detection-for-cctv-surveillance-6b8851ca3751)
6. [Tensorflow-SSDæµ‹è¯•åŠè®­ç»ƒè‡ªå·±çš„æ•°æ®é›†](https://blog.csdn.net/ei1990/article/details/75282855)
7. [SSD Tensorflow è®­ç»ƒæµ‹è¯•è‡ªå·±çš„æ•°æ®é›† Jupiter notebook æ˜¾ç¤ºè®­ç»ƒç»“æœ](https://blog.csdn.net/Echo_Harrington/article/details/81131441)
