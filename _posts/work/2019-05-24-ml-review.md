---
layout: post
title: Review of Machine Learning Algorithms
subtitle: 
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　按照一定的精简形式整理机器学习相关算法，为了能够快速回忆，也帮助自己再一次梳理一遍算法。

## 1. 天下没有免费的午餐
　　在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。

## 2. 偏差和方差
　　**偏差**描述的是预测值（估计值）的**期望**与真实值之间的差距。偏差越大，说明越偏离真实数值。

$$
\text{Bias}[\hat{f}(x)]=E[\hat{f}(x)]-f(x)
$$

　　**方差**描述的是预测值的变化范围和离散程度，即离其期望值的距离。

$$
\text{Var}[\hat{f}(x)]=E\left[(\hat{f}(x)-E[\hat{f}(x)])^{2}\right]
$$

　　模型的**真实误差**则是偏差和方差的和：

$$
E\left[(y-\hat{f}(x))^{2}\right]=\text{Bias}[\hat{f}(x)]^{2}+\text{Var}[\hat{f}(x)]+\sigma^{2}
$$


## 线性回归（Linear Regression）

$$
\begin{array}{l}
h(x) = x^T \hat{w} \\
\hat{w}=\text{argmin}_{w} \sum_{x, y}\left(y-x^{T} w\right)^{2} \\
\hat { w } = \left( \gamma \mathbf { I } + X ^ { T } X \right) ^ { - 1 } X ^ { T } y
\end{array}
$$



通过梯度下降的方式求解过程：
$1.~ 输入$ $X$, $y$, $初始化权重$ $w_0$
$2.~ 计算损失函数对参数$ $w$ $的偏导并迭代更新$
$3.~ 达到最大迭代次数，或者损失降低到一定程度退出$


**优点**：
* 实现简单，计算简单。

**缺点**：
* 不能拟合非线性数据。
* 对异常值非常敏感。


## 逻辑斯特回归（Logistic Regression）
$$
\begin{array}{l}
h(x) = {1\over{1+e^{-x^T w}}} = p \\
\begin{aligned}
\hat{w} &= \text{argmin}_{w} ~J(w)\\
&= \text{argmin}_{w} ~ -{1\over{m}}\sum_{i=1}^m\left( y \ln p + \left(1 - y \right)\ln \left(1 - p \right) \right)
\end{aligned} \\
{\partial J \over \partial w} = -{1\over{m}} \sum_{i=1}^m\left( y_i - h(x_i)\right) x_i
\end{array}
$$

## 技巧性问题
如何解决过拟合问题？
* 正则化

## References
1. [机器学习算法优缺点对比及选择（汇总篇）](https://zhuanlan.zhihu.com/p/46831267)
2. [机器学习面试](http://www.aomanhao.top/2018/09/04/机器学习面试精华/)