---
layout: post
title: "基于支持向量机的交叉验证与参数选择研究"
author: "Bin Li"
tags: "tags"
comments: true
published: false
---


2.1.3 常用训练算法

### 块算法(chunking algorithm)

Chunking算法 [10] 的出发点是删除矩阵中对应 Lagrange乘数为零的行和列将不会影响最终的结 果。对于给定的样本，Chunking算法的目标是通过 某种迭代方式逐步排除非支持向量，从而降低训练 过程对存储器容量的要求。具体做法是，将一个大 型QP问题分解为一系列较小规模的QP问题，然后找 到所有非零的Lagrange乘数并删除。在算法的每步 中Chunking都解决一个QP问题，其样本为上一步所剩的具有非零Lagrange乘数的样本以及M个不满足 KKT条件的最差样本。如果在某一步中，不满足KKT 条件的样本数不足M个，则这些样本全部加入到新 的QP问题中。每个QP子问题都采用上一个QP子问 题的结果作为初始值。在算法进行到最后一步时， 所有非零Lagrange乘数都被找到，从而解决了初始 的大型QP问题。

Chunking算法将矩阵规模从训练样本数的平方 减少到具有非零Lagrange乘数的样本数的平方，在 很大程度上降低了训练过程对存储容量的要求。 Chunking算法能够大大提高训练速度，尤其是当支 持向量的数目远远小于训练样本的数目时。然而， 如果支持向量个数比较多，随着算法迭代次数的增 多，所选的块也会越来越大，算法的训练速度依旧 会变得十分缓慢。

如块算法、Osuna 方法、SVMlight、SMO算法、Generalized Decomposition Algorithm算法、GSMO 算法。

### 分解算法（decomposition algorithm）

分解算法最早在文献[11]中提出，是目前有效解决大规模问题的主要方法。分解算法将二次规划问题分解成一系列规模较小的二次规划子问题，进行迭代求解。在每次迭代中，选取拉格朗日乘子分量的一个子集做为工作集，利用传统优化算法求解一个二次规划的子问题。以分类SVM为例，分解算法的主要思想是将训练样本分成工作集B和非工作集N，工作集B中的样本个数为q，q远小于训练样本总数。每次只针对工作集B中的样本进行训练，而固定N中的训练样本。该算法的关键在于选择一种最优工作集选择算法，而在工作集的选取中采用了随机的方法，因此限制了算法的收敛速度。

文献[12]在分解算法的基础上对工作集的选择做了重要改进。采用类似可行方向法的策略确定工作集B。如果存在不满足KTT条件的样本，利用最速下降法，在最速下降方向中存在q个样本，然后以这q个样本构成工作集，在该工作集上解决QP问题，直到所有样本满足KTT条件。如此改进提高了分解算法的收敛速度，并且实现了SVMlight算法。

文献[13]提出的序列最小优化(sequential minimal optimization，SMO)算法是分解算法的一个特例，工作集中只有2个样本，其优点是针对2个样本的二次规划问题可以有解析解的形式，从而避免多样本情况下的数值解不稳定及耗时问题，且不需要大的矩阵存储空间，特别适合稀疏样本。工作集的选择不是传统的最陡下降法，而是启发式。通过两个嵌套的循环寻找待优化的样本，然后在内环中选择另一个样本，完成一次优化，再循环，进行下一次优化，直到全部样本都满足最优条件。SMO算法主要耗时在最优条件的判断上，所以应寻找最合理即计算代价最低的最优条件判别式。

SMO算法提出后，许多学者对其进行了有效的改进。文献[14]提出了在内循环中每次优化3个变量，因为3个变量的优化问题同样可以解析求解，实验表明该算法比SMO的训练时间更短。文献[15-16]在迭代过程中的判优条件和循环策略上做了一定的修改，加快了算法的速度。

### 增量算法(incremental algorithm)
增量学习是机器学习系统在处理新增样本时，能够只对原学习结果中与新样本有关的部分进行增加修改或删除操作，与之无关的部分则不被触及。增量训练算法的一个突出特点是支持向量机的学习不是一次离线进行的，而是一个数据逐一加入反复优化的过程。

文献[17]最早提出了SVM增量训练算法，每次只选一小批常规二次算法能处理的数据作为增量，保留原样本中的支持向量和新增样本混合训练，直到训练样本用完。文献[18]提出了增量训练的精确解，即增加一个训练样本或减少一个样本对Lagrange系数和支持向量的影响。文献[19]提出了另一种增量式学习方法，其思想是基于高斯核的局部特性，只更新对学习机器输出影响最大的Lagrange系数，以减少计算复杂度。文献[20]提出了一种“快速增量学习算法”，该算法依据边界向量不一定是支持向量，但支持向量一定是边界向量的原理，首先选择那些可能成为支持向量的边界向量，进行SVM的增量学习，找出支持向量，最终求出最优分类面，提高训练速度。文献[21]提出了基于中心距离比值的增量运动向量机，利用中心距离比值，在保证训练和测试准确率没有改变的情况下，提高收敛速度。文献[22]提出了基于壳向量的线性SVM增量学习算法，通过初始样本集求得壳向量，进行SVM训练，得到支持向量集降低二次规划过程的复杂度，提高整个算法的训练速度和分类精度。

## 交叉验证
交叉验证是又一种模型选择方法，它与前面介绍的模型选择方法有所不同，是一种没有任何前提假定直接估计泛化误差的模型选择方法，由于没有任何假定，可以应用于各种模型选择中，因此具有应用的普遍性，又由于其操作的简便性，被人们认为是一种行之有效的模型选择方法。

### 交叉验证的产生
交叉验证的产生是一个曲折的过程。首先是人们发现用同一数据集既进行模型训练又进行泛化误差的估计会产生一个较差的结果，也就是我们常说的训练误差估计的乐观性，为了克服这个问题，交叉验证的方法被人们提了出来，它的基本思想是将数据分为两部分，一部分数据用来进行模型的训练，通常我们叫做训练集，另一部分数据用来测试训练生成模型的误差，我们叫做测试集，由于两部分数据的不同，泛化误差的估计是在新的数据上进行，这样的泛化误差的估计可以更接近真实的泛化误差，在数据足够的情况下，我们可以很好估计出真实的泛化误差，但是在实际应用中，往往只有有限的数据可用，我们必须对数据进行重用，对数据进行多次切分来得到好的估计，自从交叉验证提出以后，人们提出了不同的数据切分方式，因此产生了多种形式的交叉验证方法，下面我们对主要的交叉验证方法作一个详细的介绍。

### 交叉验证的方法


## 参考文献


