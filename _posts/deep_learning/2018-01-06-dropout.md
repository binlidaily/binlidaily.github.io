---
layout: post
title: Dropout
subtitle: 丢弃法
author: "Bin Li"
tags: [Deep Learning]
comments: true
published: true
---

　　Dropout (丢弃法) 是指在深度网络的训练中，以一定的概率随机地“临时丢弃”一部分神经元节点。具体来讲，Dropout 作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。

　　类比于 Bagging 方法，Dropout 可被认为是一种实用的大规模深度神经网络的**模型集成算法**。这是由于传统意义上的 Bagging 涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout 在小批量级别上的操作，提供了一种轻量级的 Bagging 集成近似，能够实现指数级数量神经网络的训练与评测。

　　Dropout 的具体实现中，要求某个神经元节点激活值以一定的概率 p 被“丢弃”，即该神经元暂时停止工作，如下图所示。因此，对于包含 N 个神经元节点的网络，在 Dropout 的作用下可看作为 $2^N$ 个模型的集成。这 $2^N$ 个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

<p align="center">
  <img width="500" height="" src="/img/media/15620738408794.jpg">
</p>

　　在神经网络中应用 Dropout 包括训练和预测两个阶段。在训练阶段中，每个神经元节点需要增加一个概率系数，如下图所示。**训练阶段**又分为前向传播和反向传播两个步骤。

<p align="center">
  <img width="500" height="" src="/img/media/15620741218791.jpg">
</p>

　　原始网络对应的前向传播公式为

$$
z_{i}^{(l+1)}=w_{i}^{(l+1)} y^{l}+b_{i}^{(l+1)}
$$

$$
y_{i}^{(l+1)}=f\left(z_{i}^{(l+1)}\right)
$$

　　应用 Dropout 之后，前向传播公式变为

$$
r_{j}^{(l)} \sim \text { Bernoulli }(p)
$$

$$
\tilde{y}^{(1)}=r^{(l) *} y^{(l)}
$$

$$
z_{i}^{(l+1)}=w_{i}^{(l+1)} \tilde{y}^{l}+b_{i}^{(l+1)}
$$

$$
y_{i}^{(l+1)}=f\left(z_{i}^{(l+1)}\right)
$$

　　上面的 Bernoulli 函数的作用是以概率系数 p 随机生成一个取值为 0 或 1 的向量，代表每个神经元是否需要被丢弃。如果取值为 0，则该神经元将不会计算梯度或参与后面的误差传播。

　　测试阶段是前向传播的过程。在前向传播的计算时，每个神经元的参数要预先乘以概率系数 p，以恢复在训练中该神经元只有 p 的概率被用于整个神经网络的前向传播计算。


## References
1. [理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)