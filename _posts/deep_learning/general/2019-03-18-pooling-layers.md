---
layout: post
title: Pooling Layers
subtitle: 池化层/汇聚层
author: Bin Li
tags: [Deep Learning]
image: 
comments: true
published: true
typora-root-url: ../../../binlidaily.github.io
typora-copy-images-to: ../../img/media
---

　　池化层/汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是进行特征选择，去除多余特征，降低特征数量，从而减少参数数量。池化层可以在一定程度上提高空间不变形，例如平移不变性，尺度不变形和形变不变形。

　　典型的汇聚层是将每个特征映射划分为 2 × 2 大小的不重叠区域，然后使用最大汇聚的方式进行下采样. 汇聚层也可以看作是一个特殊的卷积层，卷积核大小为 $K \tims K$，步长为 $S \tims S$，卷积核为 max 函数或 mean 函数. 过大的采样区域会 急剧减少神经元的数量，也会造成过多的信息损失.


## 1. 最大池化（Max Pooling）
　　选择区域内值最大的，认为其最能代表给定特征。我们可以把 Max Pooling 看作一种某个给定特征在图像区域中被发现的方式，其忽略特征的确切位置。直观上看，只是判断一个特征有没有，并不需要知道其确切位置，这样可以减少特征数量和参数数目。

![](/img/media/15842571760994.jpg)

　　对于一个区域 $R_{m, n}^{d}$，选择这个区域内所有神经元的最大活性值作为这个区域的表示：

$$
y_{m, n}^{d}=\max _{i \in R_{m, n}^{d}} x_{i}
$$

　　其中 $x_i$ 为区域 $R_k^d$ 内每个神经元的活性值.


## 2. 平均池化（Average Pooling）
　　一般是取区域内所有神经元活性值的平均值：

$$
y_{m, n}^{d}=\frac{1}{\left|R_{m, n}^{d}\right|} \sum_{i \in R_{m, n}^{d}} x_{i}
$$

　　对每一个输入特征映射 $X^d$ 的 $M^\prime \times N^\prime$ 个区域进行子采样，得到汇聚层的输出特征映射 $Y^d = \\{ y^d_{m,n}\\}$，$1\le m \le M^\prime$，$1\le n \le N^\prime$。

![](/img/media/15842571760993.jpg)

　　平均池化更多的保留图像的背景信息。

## 总结
　　池化层的具体作用：
1. 在一定程度上实现了特征不变性
    * 使模型更关注包含一定的自由度，能容忍特征微小的位移。（平移、伸缩、旋转）
2. 进行特征选择
    * 特征降维，下采样使后续操作的计算量得到减少
3. 一定程度防止过拟合
    * 去除多余特征，降低特征数量，从而减少参数数量，在一定程度上降低过拟合风险

　　池化层缺点：
1. 主要是会丢失信息。


　　CNN 特征提取的误差主要来自两个方面：

1. 邻域大小受限造成的估计值方差增大
    * 平均池化能有效减少该误差，更多的保留图像的背景信息；
    * 均匀采样的方差只有总体方差的 $\frac{1}{N}$；
    * 但如果模型中杂波方差较大（也即第二个误差明显），最后输出类别的概率分布将出现明显的混叠，导致分类准确率下降
2. 卷积层参数误差造成估计值均值偏移
    * 最大池化能有效减少该误差，更多的保留图像纹理信息；
    * 最大值采样的方差为总体方差的 $\frac{1}{\sqrt{\log (N)}}$（推导过程参见论文），受第一种误差影响较大；


## 问答（Q&A）
### 池化层会信息丢失，在使用中要如何注意？

TODO：了解一下池化灾难->胶囊网络。

　　在应用池化的时候会减小图像的信息，所以是否使用池化往往取决于你的目的。
1. 如果是希望进行图像分割，图像分类等等不需要关注图像细节的任务时，使用池化往往可以加速训练或者获取更深的特征，例如unet在downsamling的应用。
2. 但是如果你希望最终产生图像的细节，比如生成图像，补全信息等等任务，一定一定不要使用池化，那样会导致最终结果变糊，因为经过了池化之后要补全缺失的信息需要convolution kernel去拟合，也就是猜的过程，那样一定是不准确的。或者可以考虑unet的解决方式，加入一条横向的channel将pooling前的信息给到后面，用于恢复信息。


## References
1. [深度学习笔记(三)：激活函数和损失函数](https://blog.csdn.net/u014595019/article/details/52562159)