---
layout: post
title: Fully Connected Layer
subtitle: 全连接层
author: Bin Li
tags: [Deep Learning]
image: 
comments: true
published: true
---

## Fully Connected Layer
　　全连接层（Fully Connected Layer, FC）是由许多神经元组成的平铺结构，其根据得到的特征进行分类。

　　如何从卷积层后激活函数的输出转到全连接层？可以把它理解成用相同大小的卷积核进行卷积计算输出到一个神经元，FC 层有多少个神经元就用多少个这样的卷积核进行操作。

　　例如我们要把 3x3x5 的输出，转换成 1x4096 的形式。

<p align="center">
<img src="/img/media/15559854607020.jpg" width="">
</p>

　　可以理解成如下采用的 3x3x5 的 filter 去卷积激活函数的输出：

<p align="center">
<img src="/img/media/15559855009136.jpg" width="">
</p>

　　因为有 4096 个神经元，这里可以看成用一个 3x3x5x4096 的卷积层去卷积激活函数的输出。这一层的卷积很是重要，用来把特征的 representation 整合到一起，输出为一个值。这样的好处是能够大大减少特征位置对分类带来的影响。如下面的例子：

<p align="center">
<img src="/img/media/15559882225191.jpg" width="">
</p>

　　从上图可以看出喵喵不管在什么位置，输出的结果都是相同的，而对于机械化的算法来说特征值相同，如果特征值位置不同，那么可能分类的结果也不一样。那么通过全连接层的卷积核操作就能够把 Feature Map 整合成一个值，判断是否有喵喵存在（值大说明存在）。所以，全连接层的作用是**分类**。值得注意的是因为全连接层将空间结构特征忽略了，所以其不适合在方位上找 Pattern 的任务，比如 Detection 和 Segmentation。

　　那么这里我们会问一个问题，为什么有的网络是连续两层全连接层的平铺结果？

　　这里可以联系到泰勒公式，即用多项式函数去拟合光滑函数。我们这里用许多神经元去拟合数据分布，但是因为如果只有一层全连接层，有时候没有办法解决非线性的问题（激活函数可以在一定程度上可以实现非线性），如果多层就能解决。

　　全连接层之前的作用是提取特征，那么到了全连接层我们只要得到如下特征就能判断图片中是否有喵喵：

<p align="center">
<img src="/img/media/15559912454297.jpg" width="">
</p>

　　我们先看下输出层前面的一个全连接层的特征情况：

<p align="center">
<img src="/img/media/15559915410132.jpg" width="">
</p>

　　从图中可以看到，红色的神经元被激活了，也就是对应的特诊刚被找到了，而同一层中的其他没有被激活的神经元说明要么喵喵的特征不明显，要么没找到。当在输出层把这些特征组合到一起时，就能判断是喵喵了。至此，我们如果再继续往前看一层全连接层，我们看如何得到在当前全连接层被激活的喵喵的头部特征。

<p align="center">
<img src="/img/media/15559917593770.jpg" width="">
</p>

　　通过前面的卷积层、下采样层，可以得到喵喵头部的局部特征：

<p align="center">
<img src="/img/media/15559918008969.jpg" width="">
</p>

　　虽然不是很严谨的一个例子，但是也可以说明这样的问题，有的时候单层全连接层效果并不是很好，如果这里只拿一层全连接层直接连到输出层效果可能就不好。但是在实际操作中似乎也只好用尝试的办法来选择到底要用几层全连接层。

## References
1. [CNN 入门讲解：什么是全连接层（Fully Connected Layer）?](https://zhuanlan.zhihu.com/p/33841176)