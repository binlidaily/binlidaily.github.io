---
layout: post
title: Activate Function and Loss Function
subtitle:
author: Bin Li
tags: [Deep Learning]
image: 
comments: true
published: true
typora-root-url: ../../../binlidaily.github.io
typora-copy-images-to: ../../img/media
---

## Pooling layer

![1533523266540](/img/media/1533523266540.png)

为什么这里 $14 \times 14 \times 512$ 后就变成了 $1 \times 1 \times 4096$ ？

## Softmax layer


## References
1. [深度学习笔记(三)：激活函数和损失函数](https://blog.csdn.net/u014595019/article/details/52562159)