---
layout: post
title: "2018腾讯广告算法大赛"
author: "Bin Li"
tags: [Competitions]
category: ""
comments: true
published: false
---

## 数据预处理部分
### 广告特征文件 adFeature.csv
广告数据（aid）一共有173个，包括aid能展开成483个维度。

不能直接读进来比较大的csv文件，要一行一行来哦！

要有容灾的意识啊！我第一个版本是准备让所有的数据（八百万）跑完才写到文件里，一点意识都没有，万一跑到七百万就挂了要咋办？！设定一个五十万就输出一个版本也是很好的策略。


## 特征工程
突然想到LibSVM的数据结构为什么要那么设计？对应的alpha又是怎么存储的？

特征分成两个部分，一个是对用户特征，一个是广告特征。于是就要开始分析特征工程要怎么做？

one hot的特征，我们可以采用线性统计的办法，对于数值型的特征又需要进一步探索？！#TODO


素材⼤⼩与库存素材平均⼤⼩的⽐例

- [ ] 所有⼴告的one-hot属性的不同特征 （例如假设⼴告计划有3种，投放给该⽤户的⼴告属于这些⼴告计划中的前两种，那么⽤户增加3个特征[1, 1, 0]）

- [ ] 所有⼴告的one-hot属性的不同特征的count （例如假设⼴告计划有3种，投放给该⽤户的⼴告属于这些计划的数量分别是[2, 3, 1]，这个vector就是该⽤户的特征）

这个方面是利用同一个uid下面对应的aid进行统计

```
accept_users_features = raw_userfeature_df[raw_userfeature_df['uid'].isin(accept_users)]
```

直接用pandas的query会非常慢，

按照第二列排序：
np.sort(a.view('i8,i8'), order=['f1'], axis=0).view(np.int)
arr[arr[:, 1].argsort()]

按照行号删除：
np.delete(aid_uids, (del_idx), axis=0)

a[[0,1,3], :]            # Returns the rows you want

```
awk -F ',' '$1=="2"' uid_csMean.csv # "i want to search in a speific column for a particular string... how to do that ?"
```

---

对照一些开源的方法，主要用了一下三种特征：

* 特征工程
    * 基础特征「One-Hot, Multi-Hot, Embedding」
        * 单值类别特征
        * 多值类别特征
    * 统计特征「One-Hot，Embedding」
        * 技术特征
            * uid 出现次数
            * 多值特征长度
        * 转化率特征
            * uid 转化率
        * 曝光特征
            * uid 分组 aid 数目
            * aid 分组 uid 数目
            * campaignId 分组 aid 数目
    * 共现特征「Multi-Hot，Embedding」
        * 正负 aid
        * 正负 productId

特征穿越是一种 data leakage，比如购买行为预测，给你前三个月的数据，预测后一周的用户购买行为，你用后一周的用户行为如点击率什么的，放进前三个月的特征中，就是特征穿越了。

共现特征是指同一个用户转化过的 aid 集合和没有转化过的 aid 集合，在处理时可以作为多值特征，在深度模型中 Embeding，lgb 中 multi-hot 下。


#### Embedding
然后搜索了下文献，其实非常简洁，只是做了一层映射，one hot 之后的特征乘上 embedding matrix ($W _ { E } \in \mathcal { R } ^ { V \times D }$)，得到的就是embedding之后的特征


## Python 的使用
### is vs. == for string comparison
is 判断别名

### create dict with keys
```python
uid_name = ['LBS', 'age', 'appIdAction', 'appIdInstall', 'carrier', 'consumptionAbility', 'ct','education', 'gender', 'house', 'interest1', 'interest2', 'interest3', 'interest4','interest5', 'kw1', 'kw2', 'kw3', 'marriageStatus', 'os', 'topic1', 'topic2', 'topic3']
                 
feature_dict = dict.fromkeys(uid_name)
```

### Ternary Operators
```
is_fat = True
state = "fat" if is_fat else "not fat"
```

### Add key, value pair to dictionary
```
feature_dict[feature_key].update({-2: sum_not_none})
null_dict.update({feature_key: sum_none})
ge_thred.update({feature_key: sum_ge_thred})
```

### Numpy 排序
```
index = np.argsort(-np.array(null_dict.values()))
print np.array(null_dict.keys())[index]
```


## Linux 使用
列出第一列不重复的个数：
 ```
 awk -F ',' '{print $1}' test1.csv | sort | uniq | wc -l
 ```

list to dict：
```
dict((el,0) for el in a)
```

重定向的用一个 greater-than (>)，会替换掉之前的同名文件（如果有的话），两个 greater-than (>) 会在重定向指向的文件后面 append 对应的内容（如果存在同名文件的话）。

```
wc -l uid_stat_test.csv

tail -n 2 uid_stat_test.csv > uid_only_test.csv

head -n -4 uid_stat_test.csv > uid_stat_test.csv
```

### conda installation
```
# if command not found: conda
export PATH=~/anaconda2/bin:$PATH

# create
conda create -n env_name python=3 ipython numpy

# add package
conda install --name env_name scipy

# activate
source ~/anaconda3/bin/activate test_env
source activate test_env
conda activate test_env

# deactivate
source deactivate test_env

# clone
conda create -n test_env_2 -clone test_env

# remove
conda env remove -n test_env_2
```

### tmux
```
tmux new -s session_name
tmux ls
tmux a -t session_name
tmux kill-session -t myname
```

[Kaggle 比赛资料](https://www.kaggle.com/getting-started/39193#post219659)：

Data ScienceTutorial for Beginners: https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners

Machine Learning Tutorial for Beginners: https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners

Deep Learning Tutorial for Beginners: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners

Plotly Tutorial for Beginners: https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners

Feature Selection and Data Visualization: https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization

Seaborn Tutorial for Beginners: https://www.kaggle.com/kanncaa1/seaborn-tutorial-for-beginners

cat ct.csv | sort | uniq

## 数据记录
### 数据库
萧永乐1
> 110.64.66.198
> root
> ibm198720#@!
> /usr/local/glad/

萧永乐2
> 110.64.66.194
> root
> ibm198720#@!
> /usr/local/glad/

ssh -p 25563 ubuntu@222.201.139.197
passwd: qwe123


## Bagging, boosting and stacking in machine learning
All three are so-called "meta-algorithms": approaches to combine several machine learning techniques into one predictive model in order to decrease the variance (bagging), bias (boosting) or improving the predictive force (stacking alias ensemble).

Every algorithm consists of two steps:

Producing a distribution of simple ML models on subsets of the original data.

Combining the distribution into one "aggregated" model.



## References
[交叉统计交叉特征](https://github.com/wangle1218/Advertising-algorithm-competition)
[腾讯广告算法大赛](http://algo.qq.com/home/home/index.html)
[深度学习在CTR预估中的应用 | CTR深度模型大盘点](https://mp.weixin.qq.com/s/CMZHhxAMno2GlnQCjv0BKg)
[2018腾讯广告算法大赛Top10-特征工程](https://zhuanlan.zhihu.com/p/40479648)




