---
layout: post
title: Kaggle 入门一步一步记录
subtitle: 认真，坚持
author: Bin Li
tags: [Machine Learning, Competitions]
image: 
comments: true
published: true
---

为了不让手生，纯复现机器学习算法也蛮无聊的，想抽空也看看Kaggle上的比赛，当然，刚开始还是拿已经结束的比较经典的比赛来试试看吧！


## Titanic: Machine Learning from Disaster
先通过具体[比赛介绍](https://www.kaggle.com/c/titanic)了解一下题意，我们是要通过给定的数据预测一个人是否最后会在灾难中生存下来。

了解了题目之后，接下来就是要看数据了。

这里新列出一般的比较经典的流程：

1. Data exploration and visualization
    * Explore dataset
    * Choose important features and visualize them according to survival/not-survival
1. Data cleaning, Feature selection and Feature engineering
    * Null values
    * Encode categorical data
    * Transform features
1. Test different classifiers
    * Logistic regression
    * KNN
    * Support Vector Machines (SVM)
    * Naive Bayes
    * Radom Forest


### 数据预处理
#### 认识数据
* 看有什么类型的数据，每种数据应该用什么方式整理，数值型看最大最小值，众数，平均数等，标签型的看一共有多少类，分布是如何的，还有就是variable transformation, normalization and etc.

我们在处理数据的时候，最好将训练数据集和测试数据集放到一起来进行，然后再分开来训练和测试模型。

| Variable<span class="Apple-tab-span" style="white-space:pre"></span> | Definition | Key | Type |
| :-: | :-: | :-: | :-: |
| survival | Survival | 0 = No, 1 = Yes | L |
| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | L |
| sex | Sex | male, female | L |
| Age | Age  | in years | N |
| sibsp | # of siblings / spouses aboard the Titanic |  | N |
| parch | # of parents / children aboard the Titanic |  | N |
| ticket | Ticket number |  | L |
| fare | Passenger fare |  | N |
| cabin | Cabin number |  | L |
| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | L |
| PassengerId | PassengerId |  | L |
| name |  |  | L |


对于固定类别的属性我们用onehot编码，对于连续数值型的我们就要对其进行标准化。

训练数据集和测试数据集到底是一起操作还是分开操作这得具体看情况了。

分类的问题，分类的结果不能是浮点型，要用整型！

### Feature engineering
Feature engineering is the process of creating new features from existing data.

如果出现多个csv文件的数据的情况，首先我们要把所有数据集中到一起来，才好做特征工程。那么在整合的时候，对于连续数值型数据，一般会对`mean`, `max`, `min`, `sum`做一个group的统计。

How do we know if any of these features are helpful? One method is to calculate the Pearson correlation coefficient between the variables and the TARGET. 

除了皮尔森系数还可以用决策树跑一下特征，然后参考特征的权重值。

因为特征工程而生成数量庞大的新的特征时，注意避免会有 multiple comparisons problem 的问题。

### Feature Selection
1. Removing collinear variables
2. Removing variables with many missing values
3. Using feature importances to keep only “important” variables


### Programming
drop data
```python
X = dataset.drop(['PassengerId','Cabin','Ticket','Fare', 'Parch', 'SibSp'], axis=1)
```

**Name**

针对名字来说，具体姓氏名谁好像没有太大的利用之处，我们可以从称呼（salutations）中看到区别，于是我们多加一列，将称呼补充进去。

像Sex和Embarked是字符串类型的类标型数据，那么我们需要对其进行Encoding。

After this, it is then possible to use the get_dummies and get three new columns (Embarked_C, Embarked_Q, Embarked_S) which are called dummy variables (they assign ‘0’ and ‘1’ to indicate membership in a category). The previous "Embarked" can be dropped from X as it will not be needed anymore and we can now concatenate the X dataframe with the new "Embarked" which has the three dummy variables. Finally, as the number of dummy variables necessary to represent a single feature is equal to the number of categories in that feature minus one, we can remove one of the dummies created, lets say Embarked_S, for example. This will not remove any information because by having the values from Embarked_C and Embarked_Q the algorithm can easily understand the values from the remaining dummy variable (when Embarked_C and Embarked_Q are '0' Embarked_S will be '1', otherwise it will be '0').


**pandas.map()怎么用？**

pandas.map() is used to map values from two series having one column same. For mapping two series, the last column of the first series should be same as index column of the second series, also the values should be unique.

走合同，走材料费，一单不能超过2五万。小额零星的规矩。


## 问题总结
* 如果文件太大不能用pd.read_csv来装咋办？
* 对于NaN的数据，我们要怎么办？
    * 通过平均数来填充
    * 众数填充
* heatmap 是起什么作用的？
* t-SNE 是个啥？？

## References
1. [如何在 Kaggle 首战中进入前 10%](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)
2. [使用sklearn做单机特征工程](https://www.cnblogs.com/jasonfreak/p/5448385.html)
3. [分分钟带你杀入Kaggle Top 1%](https://zhuanlan.zhihu.com/p/27424282)
4. [Learn Machine Learning](https://www.kaggle.com/dansbecker/learn-machine-learning)
5. [Titanic Competition from Kaggle](https://www.kaggle.com/rochellesilva/simple-tutorial-for-beginners)
6. [Suggest an easy level/start level competition](https://www.kaggle.com/getting-started/12606)
7. [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)
8. [All You Need is PCA (LB: 0.11421, top 4%)](https://www.kaggle.com/massquantity/all-you-need-is-pca-lb-0-11421-top-4)
9. [Tutorial: Accessing Data with Pandas](https://www.kaggle.com/sohier/tutorial-accessing-data-with-pandas)
10. [Data Preparation & Exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration)
11. [Data preparation for building models](https://www.kaggle.com/sirpunch/flatten-de-serialize-credits-data)
12. [Data Preparation for Sberbank](https://www.kaggle.com/optidatascience/data-preparation-for-sberbank)
13. ✅ [Titanic: score over 80% - part 1 data preparation](https://www.kaggle.com/ymlai87416/titanic-score-over-80-part-1-data-preparation)
14. [Titanic: score over 80% - part 2 model selection](https://www.kaggle.com/ymlai87416/titanic-score-over-80-part-2-model-selection)
15. [Titanic, a step-by-step intro to Machine Learning](https://www.kaggle.com/ydalat/titanic-a-step-by-step-intro-to-machine-learning)
16. [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)
17. [Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic)
18. [Machine Learning Kaggle Competition Part One: Getting Started](https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426)
19. ✅ [Machine Learning Kaggle Competition Part Two: Improving](https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8)
20. ✅ [Automated Feature Engineering in Python](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219)
21. [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
22. [Introduction to Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
23. [chrisalbon blog](https://chrisalbon.com/)
24. [Open Machine Learning Course. Topic 1. Exploratory Data Analysis with Pandas](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68)
25. [Understand Your Problem and Get Better Results Using Exploratory Data Analysis](https://machinelearningmastery.com/understand-problem-get-better-results-using-exploratory-data-analysis/)
26. [Kaggle Tutorial: EDA & Machine Learning](https://www.datacamp.com/community/tutorials/kaggle-machine-learning-eda)
27. [Exploratory data analysis - Coursera](https://www.coursera.org/lecture/competitive-data-science/exploratory-data-analysis-zvOJc?authMode=login)
28. ✅ [Feature Engineering](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)
29. [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
30. [Why Automated Feature Engineering Will Change the Way You Do Machine Learning](https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96)
31. [A Hands-On Guide to Automated Feature Engineering using Featuretools in Python](https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/)