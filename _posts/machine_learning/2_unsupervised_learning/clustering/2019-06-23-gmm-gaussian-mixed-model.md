---
layout: post
title: Gaussian Mixture Model
subtitle: 高斯混合模型
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　高斯混合模型假设每个簇的数据都是符合高斯（正态）分布的，而当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。

# 1. 高斯混合模型
　　首先，我们看下图的例子，上子图是只用一个高斯分布来拟合图中的数据，其中椭圆表示高斯分布的二倍标准差所对应的椭圆，对数据分布的拟合效果差强人意。仔细看数据明显有两个簇存在，下子图是利用了两个高斯分布的叠加来拟合得到较好的结果。

<p align="center">
<img src="/img/media/15620491807297.jpg" width="400">
</p>
<p style="margin-top:-2.5%" align="center">
    <em style="color:#808080;font-style:normal;font-size:80%;">高斯混合模型示意图</em>
</p>

　　这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。理论上，高斯混合模型可以拟合出任意类型的分布。

　　高斯混合模型的**核心思想**是，假设数据可以看作由多个高斯分布中生成得到。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $\mu_i$ 和方差 $\Sigma_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi$，可以理解为权重或生成数据的概率。

　　那么为什么可以这么假设呢？🤔

　　高斯混合模型的公式为：

$$
p(x)=\sum_{i=1}^{K} \pi_{i} N\left(x | \mu_{i}, \Sigma_{i}\right)
$$

　　高斯混合模型是一个**生成式模型**，数据的生成过程可以看成：
1. 首先按照权重选择到一个高斯分布的分模型
2. 然后再根据该高斯模型的分布生成出一个数据点
3. 循环生成所有点即可

　　但是，通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列数据点，给出类簇的数量 $K$ 后，希望求得最佳的 $K$ 个高斯分模型。因此，高斯混合模型的计算，便成了最佳的均值 $\mu$，方差 $\Sigma$、权重 $\pi$ 的寻找。

　　这类问题通常通过最大似然估计来求解，然而例如如果直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数形式是和的对数，难以展开和对其求偏导优化。

# 2. 求解高斯混合模型
　　常规的优化模式对于高斯混合模型来说会得到复杂的目标函数，不方便优化，于是有前人提出使用 EM 算法框架来解决该优化问题。EM 算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。

　　具体到高斯混合模型的求解，EM 算法的迭代过程如下：

1. 初始随机选择各参数的值

2. E 步骤：根据当前的参数，计算每个点由某个分模型生成的概率

3. M 步骤：使用 E 步骤估计出的概率，来改进每个分模型的均值，方差和权重

4. 重复 E 和 M 两个步骤，直到收敛

　　我们并不知道最佳的 $K$ 个高斯分布各自的 3 个参数，也不知道每个数据点究竟是哪个高斯分布生成的，所以每次循环时，先固定当前的高斯分布不变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组合更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。

# 总结


　　高斯混合模型与 K 均值算法的相同点是：
* 它们都是可用于聚类的算法；
* 都需要指定 K 值；
* 都是使用 EM 算法来求解；
* 都往往只能收敛于局部最优。

　　而它相比于 K 均值算法的优点是：
* 可以给出一个样本属于某类的**概率**是多少；
* 不仅仅可以用于聚类，还可以用于概率密度的估计；
* 并且可以用于生成新的样本点。

# References
1. 《百面机器学习》