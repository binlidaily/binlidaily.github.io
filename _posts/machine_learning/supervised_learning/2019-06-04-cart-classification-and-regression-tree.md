---
layout: post
title: Classification and Regression Tree
subtitle: 分类回归树
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　分类与回归树（Classification And Regression Tree，CART）是目前应用广泛的决策树学习方法。CART 是给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的**条件概率分布**的学习方法。CART 假设决策树是**二叉树**，其决策结点特征的取值为“是”或者“否”，其左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于迭代的二分每个特征，将输入空间划分成有限个单元，并在这些单元上确定预测的概率分布。

⁉️ 为什么说会涉及到概率分布呢？

⁉️ 为什么想到一定要用二叉树？
* 只是二分类能保证所有类别能在叶子结点上体现？继续细分是可以的！

CART 算法一般分两个步骤:
1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大（怎么保证？）。
2. 决策树剪枝：用**验证数据集**对已生成的决策树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

## CART 生成
　　CART 的生成就是迭代地构建二叉树的过程，如果待预测结果是离散型数据，则 CART 生成分类决策树，如果待遇测数据是连续型数据，则 CART 生成回归决策树。对分类树用基尼系数最小化准则，对回归树用平方误差最小化准则，进行特征选择，生成二叉树。接下来我们从分类树和回归树分别介绍 CART。

### 分类树的生成
　　与 ID3 和 C4.5 不同的是，CART 划分的结果是二叉树。所以在生成分类树时只要对每一个属性 $a$ 的每一个取值 $a^v$ 做一个“当前属性值是否等于 $a^v$”的判断，就可以分成将样本集 $D$ 分成 $D_1$ 和 $D_2$ 两个部分。然后在选取最优划分点时，找到能够使得[基尼系数](http://gitlinux.net/2018-09-11-decision-tree/#113-基尼系数gini-index)最小化的划分对，即对应的属性 $a$ 和该属性的取值 $a^v$。

$$
\min _{a^v}~\text{Gini}(D,a), \quad  v \in [1, V]
$$

　　对于样本集 $D$，计算所有属性的最优二分方案，选择其中的最小值，作为样本集 $D$ 的最优二分方案：

$$
\min _{a \in A}~\left(\min _{a^v}~\text{Gini}\left(D,a\right)\right),  \quad  v \in [1, V]
$$

　　所得到的属性 $a$ 及其的属性值 $a^v$ 即为样本集 $D$ 的最优分裂属性以及最优分裂属性值。然后对划分的两个子结点迭代的进行该流程，直到满足停止条件。

### 回归树的生成
　　回归树的预测结果（对比属性值为连续型）是连续型数据，那么有两个问题很直接的摆在面前：
1. 划分点怎么找？
2. 因为划分后为二叉树，原本可能有很多取值的结果要怎么输出？

　　以一般机器学习模型的训练目标为参考，我们在划分点的寻找上也想设定一种有着类似训练目标的划分点寻找方式，于是就有前辈采用了**均方误差最小**的原则来寻找合适的划分点。那么对应的回归树度量目标就是：对于任意的划分属性 $a$（假设划分属性 $a$ 是训练样本的第 $j$ 个变量，记作 $x^{(j)}$），其对应有任意的划分点 $s$ 将数据集分成 $R_1$ 和 $R_2$ 两个部分，我们要找这样的划分点，其能够使得 $R_1$ 和 $R_2$ 两个集合各自的均方差最小，同时能够使得这两个均方差的加和最小。(外层最小时会不会让内层最小调整？在设计该算法的时候有什么考虑⁉️)

　　接下来对具体到公式上，首先假设 $X$ 和 $Y$ 分别为输入和输出变量，并且 $Y$ 为连续变量，给定数据集：

$$
D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( x _ { N } , y _ { N } \right) \right\}
$$

　　其中 $x _ { i } = \left( x _ { i } ^ { ( 1 ) } , x _ { i } ^ { ( 2 ) } , \ldots , x _ { i } ^ { ( n ) } \right)$ 为输入实例 (特征向量)，$i=1,2,\dots,N$，$n$ 为特征个数，$N$ 为样本总量。

　　我们选择特征向量的第 $j$ 个特征变量 $x^{(j)}$ 和取某一个属性值 $s$ 作为切分变量 (Splitting variable) 和切分点 (Splitting point)，由其划分的两个区域为：

$$R _ { 1 } ( j , s ) = \{ x | x ^ { ( j ) } \leq s \} \text{ 和 } R _ { 2 } ( j , s ) = \{ x | x ^ { ( j ) } > s \}$$

　　我们的目标是让划分之后的每个子区域的输出值跟真实值之间的差距最小，然后遍历所有可能的切分变量和切分点的组合划分得到的两个子区域，找到能够使得两个子区域输出值与真实值差距最小的划分组合。

　　那么，问题就转化成如何求解最优划分变量 $j$ 和 $s$，具体的有以下的方式：

$$
\min _ { j , s } \left[ \min _ { c _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - c _ { 1 } \right) ^ { 2 } + \min _ { c _ { 2 } } \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - c _ { 2 } \right) ^ { 2 } \right]
$$

　　对于划分之后的两个子区域，我们先求出其对应的输出值 $c_m$ (回归时子树输出的值)，这里也可以采用平方误差最小化原则分别得到 $c_1$，$c_2$:

$$
\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}, \quad \min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}
$$

<details><summary markdown="span">我们能通过此处隐藏的证明知道每个子区域的的最优输出值就是 $y_i$ 的均值。</summary>
　　一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已知将输入空间分成了 $M$ 个单元 $R_1, R_2, \dots, R_M$ (这个 $M$ 的取值得看最后回归树生成到什么程度对应的划分空间)，并且每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可以表示为：

$$
f ( x ) = \sum _ { m = 1 } ^ { M } c _ { m } I \left( x \in R _ { m } \right)
$$

　　其中 $I$ 为指示函数:

$$I=\left\{\begin{array}{ll}{1} & {\text { if }\left(x \in R_{m}\right)} \\ {0} & {\text { if }\left(x \notin R_{m}\right)}\end{array}\right.$$


　　当输入空间的划分确定时，我们用平方误差来表示回归树对于训练数据的预测误差

$$\sum _ { x _ { i } \in R _ { m } } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$$

　　利用平方误差最小化原则来求解每个单元上的最优输出值。通过以下的证明，我们知道单元 $R_m$ 上的 $c_m$ 的最优值是 $R_m$ 上所有输入实例 $x_i$ 对应的输出的 $y_i$ 的均值：

$$
\hat { c } _ { m } = \operatorname { ave } \left( y _ { i } | x _ { i } \in R _ { m } \right)
$$

　　给定一个随机的数列 $\{ x _ { 1 } , x _ { 2 } , \dots , x _ { n } \}$，假设该空间中最优的输出值为 $a$，则根据最小平方误差准则，我们能得到有关 $a$ 的函数如下：

$$
F ( a ) = \left( x _ { 1 } - a \right) ^ { 2 } + \left( x _ { 2 } - a \right) ^ { 2 } + \ldots + \left( x _ { n } - a \right) ^ { 2 }
$$

　　我们考察其单调性：

$$
F ^ { \prime } ( a ) = - 2 \left( x _ { 1 } - a \right) - 2 \left( x _ { 2 } - a \right) + \ldots - 2 \left( x _ { n } - a \right) = 2 n a - 2 \sum _ { i = 1 } ^ { n } x _ { i }
$$

$$F ^ { \prime \prime } ( a ) = 2n > 0$$

　　二阶导为正数，所以该函数为严格凸函数，那么令 $F ^ { \prime } ( a ) = 0$，得到 $a = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }$，即最小值点为：

$$
\hat { a } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }
$$

</details>

　　那么两个子区域的最优输出值分别为：

$$
\hat{c}_{1}=\frac{1}{N_{1}} \sum_{x_{i} \in R_{1}(j, s)} y_{i}, \quad \hat{c}_{2}=\frac{1}{N_{2}} \sum_{x_{j} \in R_{2}(j, s)} y_{i}
$$

　　则对应求解最优划分变量的方式可以写成:

$$
\min _ { j , s } \left[ \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - \hat { c _ { 1 } } \right) ^ { 2 } + \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - \hat { c _ { 2 } } \right) ^ { 2 } \right]
$$

　　如此我们得到了当前划分的一个最优划分对 $(j,s)$，据此将输入空间划分为两个区域，接着循环执行这个划分流程，直到满足停止条件，生成回归树。


## CART 剪枝——代价复杂性剪枝法
　　CART 剪枝算法是想从“完全生长”的决策树的底端减去一些子树，使决策树变小 (模型变简单)。这里介绍 CART 应用最广泛的代价复杂性剪枝法 (Cost Complexity Pruning, CCP)。

　　CCP 主要包含两个步骤：
1. 从原始决策树 $T_0$ 开始生成一个子树序列 $\{T_0, T_1, \dots, T_n \}$，其中，$T_{i+1}$ 从 $T_{i}$ 产生，$T_n$ 为根节点。
2. 从第 $1$ 步产生的子树序列中，根据树的真实误差估计选择最佳决策树。 

　　完整的树我们以 $T_0$ 表示，以此为基础，从决策树底端开始，考察一个内部结点，减去一个子树形成 $T_1$；接着在 $T_1$ 的基础上继续减去一个子树形成 $T_2$，循环往复直到剪枝只剩下根节点 $T_n$。接着利用独立的验证数据集对 $\{T_0, T_1, \dots, T_n \}$ 这 $n+1$ 个子树进行测试，从中选择最优的子树。

　　那么接下来的问题是，如何选择一个子树（内部结点）进行剪枝？

　　CART 剪枝时定义了计算子树的损失函数：

$$
C _ { \alpha } ( T ) = C ( T ) + \alpha | T |
$$

　　其中 $T$ 表示任意的子树，$C ( T )$ 为对训练数据的预测误差 (比如基尼系数等指标)，$\mid\mathcal {T} \mid $ 表示子树的叶节点个数。$\alpha$ 衡量和调节训练数据的拟合程度与模型复杂度的参数，$C _ { \alpha } ( T )$ 表示参数为 $\alpha$ 的子树 $T$ 的整体损失。

　　接下来从整体树 $T_0$ 开始剪枝，对于 $T_0$ 的任意内部结点 $t$，以 $t$ 为单节点树的损失函数是：

$$
C _ { \alpha } ( t ) = C ( t ) + \alpha
$$

　　以 $t$ 为根节点的待剪枝的子树的损失函数为:

$$
C _ { \alpha } \left( T _ { t } \right) = C \left( T _ { t } \right) + \alpha \left| T _ { t } \right|
$$

　　当 $\alpha=0$ 及 $\alpha$ 充分小时，有如下不等式成立：

$$
C _ { \alpha } \left( T _ { t } \right) < C _ { \alpha } ( t )
$$

　　可以想象在单个节点下，其纯度肯定更高，那么其损失函数会对应最小。

当 $\alpha$ 慢慢增大时，在某一个 $\alpha$ 上会有

$$
C _ { \alpha } \left( T _ { t } \right) = C _ { \alpha } ( t )
$$

当 $\alpha$ 再增大时，上面的不等式就会反向。也就是说当 $\alpha = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }$
，$T_t$ 与 $t$ 具有相同的损失函数，而 $t$ 的结点少，因此 $t$ 比 $T_t$ 更可取，对 $T_t$ 进行剪枝。剪枝最后保留 $t$ 这个内部结点，不过变成了叶节点？⁉️

为此，对于 $T_0$ 中每一个内部结点，我们定义

$$
g ( t ) = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }
$$

$g(t)$ 表示剪枝剪枝的阈值。我们可以这样想，当 $\alpha$ 逐渐增大时，某些内部结点有 $\alpha > g(t)$，此时就应该剪掉这些子树，因为这样剪枝的话，能够使得整体损失函数减小。当一个一个剪枝后，$\alpha$ 就变成了一个分段区间。

## CART 总结
CART **优点**：

CART **缺点**：

## References
1. [决策树之CART（分类回归树）详解](https://blog.csdn.net/zhihua_oba/article/details/72230427)