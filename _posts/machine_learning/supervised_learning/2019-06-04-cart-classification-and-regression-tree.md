---
layout: post
title: Classification and Regression Tree
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　分类与回归树（Classification And Regression Tree，CART）是目前应用广泛的决策树学习方法。CART 是给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的**条件概率分布**的学习方法。CART 假设决策树是**二叉树**，其决策结点特征的取值为“是”或者“否”，其左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于迭代的二分每个特征，将输入空间划分成有限个单元，并在这些单元上确定预测的概率分布。

⁉️ 为什么说会涉及到概率分布呢？

⁉️ 为什么想到一定要用二叉树？
* 只是二分类能保证所有类别能在叶子结点上体现？继续细分是可以的！

CART 算法一般分两个步骤:
1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大（怎么保证？）。
2. 决策树剪枝：用**验证数据集**对已生成的决策树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

CART 的生成就是迭代地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。接下来我们从回归树和分类树分别介绍 CART。

#### 回归树的生成
回归树的生成既然要迭代地构建二叉树，那么很直接地就有两个问题摆在面前：
1. 划分点怎么找？
2. 因为划分后为二叉树，原本可能有很多取值的结果要怎么输出？

以一般机器学习模型的训练目标为参考，我们在划分点的寻找上也想设定一种有着类似训练目标的划分点寻找方式，于是就有前辈采用了**均方误差最小**的原则来寻找合适的划分点。那么对应的回归树度量目标就是：对于任意的划分属性 $a$（假设划分属性 $a$ 是训练样本的第 $j$ 个变量，记作 $x_j$），其对应有任意的划分点 $s$ 将数据集分成 $R_1$ 和 $R_2$ 两个部分，我们要找这样的划分点，其能够使得 $R_1$ 和 $R_2$ 两个集合各自的均方差最小，同时能够使得这两个均方差的加和最小。(⁉️外层最小时会不会让内层最小调整？在设计该算法的时候有什么考虑？)

接下来对具体到公式上，首先假设 $X$ 和 $Y$ 分别为输入和输出变量，并且 $Y$ 为连续变量，给定数据集：

$$
D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( x _ { N } , y _ { N } \right) \right\}
$$

其中 $x _ { i } = \left( x _ { i } ^ { ( 1 ) } , x _ { i } ^ { ( 2 ) } , \ldots , x _ { i } ^ { ( n ) } \right)$ 为输入实例（特征向量），$i=1,2,\dots,N$，$n$ 为特征个数，$N$ 为样本总量。

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已知将输入空间分成了 $M$ 个单元 $R_1, R_2, \dots, R_M$（这个 $M$ 的取值得看最后回归树生成到什么程度对应的划分空间），并且每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可以表示为：

$$
f ( x ) = \sum _ { m = 1 } ^ { M } c _ { m } I \left( x \in R _ { m } \right)
$$

其中 $I$ 为指示函数，$I =  \{ \begin{array} { l l } { 1 } & { i f \left( x \in R _ { m  } \right) } \\ { 0 } & { i f \left( x \notin R _ { m } \right) } \end{array} $

可以用李宏毅视频上的截图来介绍！⁉️



当输入空间的划分确定时，我们用平方误差 $\sum _ { x _ { i } \in R _ { m } } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$ 来表示回归树对于训练数据的预测误差，用平方误差最小化原则来求解每个单元上的最优输出值。通过以下的证明，我们可以知道单元 $R_m$ 上的 $c_m$ 的最优值是 $R_m$ 上所有输入实例 $x_i$ 对应的输出的 $y_i$ 的均值：

$$
\hat { c } _ { m } = \operatorname { ave } \left( y _ { i } | x _ { i } \in R _ { m } \right)
$$

给定一个随机的数列 $\{ x _ { 1 } , x _ { 2 } , \dots , x _ { n } \}$，假设该空间中最优的输出值为 $a$，则根据最小平方误差准则，我们能得到有关 $a$ 的函数如下：
$$
F ( a ) = \left( x _ { 1 } - a \right) ^ { 2 } + \left( x _ { 2 } - a \right) ^ { 2 } + \ldots + \left( x _ { n } - a \right) ^ { 2 }
$$

我们考察其单调性：
$$
F ^ { \prime } ( a ) = - 2 \left( x _ { 1 } - a \right) - 2 \left( x _ { 2 } - a \right) + \ldots - 2 \left( x _ { n } - a \right) = 2 n a - 2 \sum _ { i = 1 } ^ { n } x _ { i }
$$

$$F ^ { \prime \prime } ( a ) = 2n > 0$$

二阶导为正数，所以该函数为严格凸函数，那么令 $F ^ { \prime } ( a ) = 0$，得到 $a = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }$，即最小值点为：

$$
\hat { a } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }
$$

那么接下来就是要讨论该如何划分了，选择第 $j$ 个变量 $x^{(j)}$ 和它的取值 $s$，作为切分变量（Splitting variable）和切分点（Splitting point），则由此划分的两个区域为：

$$R _ { 1 } ( j , s ) = \{ x | x ^ { ( j ) } \leq s \} \text{ 和 } R _ { 2 } ( j , s ) = \{ x | x ^ { ( j ) } > s \}$$

问题就转化成如何求解最优划分变量 $j$ 和 $s$，具体的有以下的方式：

$$
\min _ { j , s } \left[ \min _ { c _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - c _ { 1 } \right) ^ { 2 } + \min _ { c _ { 2 } } \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - c _ { 2 } \right) ^ { 2 } \right]
$$

对于固定输入变量 $j$，可以利用求均值的方式在内循环的 min 中找到这两个区域中的最优切分点 $s$：
$$
\hat { c _ { 1 } } = \frac { 1 } { N _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } y _ { i } \text{ 和 } \hat { c _ { 2 } } = \frac { 1 } { N _ { 2 } } \sum _ { x _ { j } \in R _ { 3 } ( j , s ) } y _ { i }
$$

其对应的求解最优划分变量的方式可以写成:

$$
\min _ { j , s } \left[ \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - \hat { c _ { 1 } } \right) ^ { 2 } + \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - \hat { c _ { 2 } } \right) ^ { 2 } \right]
$$

接着据此遍历所有的输入变量 $j$ 从而在外循环的 min 中找到最优的切分变量，如此构成了一个划分对 $(j,s)$。据此可以将输入空间划分为两个区域，接着循环执行这个划分流程，直到满足停止条件。

#### 分类树的生成
与 ID3 和 C4.5 不同的是，CART 划分的结果是二叉树。所以在生成分类树时只要对每一个属性 $a$ 的每一个取值 $a^v$ 做一个“当前属性值是否等于 $a^v$”的判断，就可以分成将样本集 $D$ 分成 $D_1$ 和 $D_2$ 两个部分。然后在选取最优划分点时，找到能够使得基尼系数最小化的划分对，即对应的属性 $a$ （相当于回归树里面的 $j$）和该属性的取值 $a^v$（相当于回归树里面的 s）。然后对划分的两个子结点迭代的进行该流程，直到满足停止条件。

#### CART 剪枝
CART 剪枝算法是想从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单）。完整的树我们以 $T_0$ 表示，以此为基础，从决策树底端开始，考察一个内部结点，减去一个子树形成 $T_1$；接着在 $T_1$ 的基础上技术减去一个子树形成 $T_2$，循环往复直到剪枝只剩下根节点 $T_n$。接着利用独立的验证数据集对 $\{T_0, T_1, \dots, T_n \}$ 这 $n+1$ 个子树进行测试，从中选择最优的子树。

那么接下来的问题是，如何选择一个子树（内部结点）进行剪枝？

CART 剪枝时定义了计算子树的损失函数：

$$
C _ { \alpha } ( T ) = C ( T ) + \alpha | T |
$$

其中 $T$ 表示任意的子树，$C ( T )$ 为对训练数据的预测误差（比如基尼系数等指标），$\mid\mathcal {T} \mid $ 表示子树的叶节点个数。$\alpha$ 衡量和调节训练数据的拟合程度与模型复杂度的参数，$C _ { \alpha } ( T )$ 表示参数为 $\alpha$ 的子树 $T$ 的整体损失。

具体地，从整体树 $T_0$ 开始剪枝，对 $T_0$ 的任意内部结点 $t$，以 $t$ 为单结点树的损失函数是：

$$
C _ { \alpha } ( t ) = C ( t ) + \alpha
$$

以 $t$ 为根节点的待剪枝的子树的损失函数为:

$$
C _ { \alpha } \left( T _ { t } \right) = C \left( T _ { t } \right) + \alpha \left| T _ { t } \right|
$$

当 $\alpha=0$ 及 $\alpha$ 充分小时，有如下不等式成立：

$$
C _ { \alpha } \left( T _ { t } \right) < C _ { \alpha } ( t )
$$

可以想象在单个结点下，其纯度肯定更高，那么其损失函数会对应最小。

当 $\alpha$ 慢慢增大时，在某一个 $\alpha$ 上会有

$$
C _ { \alpha } \left( T _ { t } \right) = C _ { \alpha } ( t )
$$

当 $\alpha$ 再增大时，上面的不等式就会反向。也就是说当 $\alpha = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }$
，$T_t$ 与 $t$ 具有相同的损失函数，而 $t$ 的结点少，因此 $t$ 比 $T_t$ 更可取，对 $T_t$ 进行剪枝。剪枝最后保留 $t$ 这个内部结点，不过变成了叶节点？⁉️

为此，对于 $T_0$ 中每一个内部结点，我们定义

$$
g ( t ) = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }
$$

$g(t)$ 表示剪枝剪枝的阈值。我们可以这样想，当 $\alpha$ 逐渐增大时，某些内部结点有 $\alpha > g(t)$，此时就应该剪掉这些子树，因为这样剪枝的话，能够使得整体损失函数减小。当一个一个剪枝后，$\alpha$ 就变成了一个分段区间。