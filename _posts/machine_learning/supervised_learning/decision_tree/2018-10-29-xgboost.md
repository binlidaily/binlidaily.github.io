---
layout: post
title: XGBoost
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　XGBoost (eXtreme Gradient Boosting) 算法是在 CART 基础上对 Boosting 算法的一个改进，CART 内部决策树选定回归树。由于 Boosting 算法在损失函数选择时有较大区别，例如如果选择平方损失函数，就是 Boosting Tree 的方式，那么需要拟合残差。而对于使用一般损失函数而言，则可以采用 Gradient Boosting 的方式，根据梯度下降来拟合残差的近似值。

## 1. Prerequisites
　　回顾一下，当我们谈及决策树的时候，其实它是启发式的，但是这些启发式的操作却有对应的意义：
* 根据信息增益划分属性 ➡️ 降低训练误差
* 剪枝 ➡️ 可以看成通过节点数定义的归一化方式
* 最大深度 ➡️ 在函数空间上的约束条件
* 平滑叶节点取值平滑 ➡️ 实际是叶节点权重的 L2 归一化

　　回归树不光可以做回归，主要看你如何利用预测结果，实际上可以用来做：分类、回归和排名等。主要依赖于你怎么定义目标函数，像之前用平方误差就构成了最基本的 Gradient Boosting 方法，我们可以总结如下：

<p align="center">
  <img width="" height="" src="/img/media/15456354000624.jpg">
</p>


　　我们在考虑一个模型的目标函数时，强调有两个部分：

$$
{ Obj } ( \Theta ) = L ( \Theta ) + \Omega ( \Theta )
$$

　　$L ( \Theta )$ 代表损失函数的部分，衡量模型在训练集上拟合的好坏程度；$\Omega ( \Theta )$ 代表正则化部分，衡量模型的复杂度。


## 2. XGBoost 算法
　　在前面我们可以看到，损失函数的选取对 Gradient Boosting 方法来说有较大的影响。而对于除了平方误差的损失函数，其他损失函数处理起来依然很是复杂，于是陈天奇就想出了利用**泰勒展开来拟合目标函数**的方法。

　　对于给定的数据集，$n$ 个实例，$m$ 个特征，

$$
\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)
$$

　　树集成模型（下图所示）使用 $K$ 个函数的累加和来预测输出。

$$
\hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F}
$$

<p align="center">
  <img width="" height="" src="/img/media/15627434677953.jpg">
</p>
<p align="center">
  <img width="" height="" src="/img/media/15627434741436.jpg">
</p>


　　其中

$$
\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q : \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)
$$

　　是所有回归树（CART）的函数空间，里面的
* $q$ 表示每颗树的结构，它将实例映射到相应的叶子节点的索引；
* $T$ 表示一棵树的叶子节点个数；
* 每一个 $f_k$ 对应于一个独立的树结构 $q$ 和叶子节点权重 $w$。

　　与决策树不同的是，每一个回归树在每一个叶子节点上包含了连续的分值，$w_i$ 表示第 $i$ 个叶子节点的得分。在上图给定的例子中，我们使用树的决策规则来将小男孩分类到对应的叶子节点，然后将所有分到的叶子节点所对应的分值相加，最后得到小男孩的最终得分2.9，因此，相较于老爷爷，小男孩玩电脑游戏的可能性更大。

　　在上述例子中，我们学习到了两棵树，并且每颗树的决策变量不同，且他们具有决策先后顺序，那么这些决策（函数）是如何学习到的呢？我们通过最小化下面的目标函数来学习的：

$$
\begin{array}{l}{\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)} \\ {\text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}}\end{array}
$$

　　这里 $l$ 是一个可微的凸损失函数，测量预测值和目标值的不同程度。$\Omega$ 是 $k$ 颗树的正则项，用于抑制模型的复杂度防止过拟合。当正则项的参数是0时，上述目标函数就是一个传统的梯度树提升模型。

　　至此，我们会有几个问题需要注意：
1. 每一颗树的分支依据是如何确定的？（比如先分age，再分性别）
2. 每一颗树的分支分到哪里为止？（比如is male？下面还要不要分支？）
3. 每棵树的叶子节点的权重分值是如何确定的？（为什么有的是2，有的是0.9，有的是-1）
4. 树的颗数是如何确定的，多少颗树是最合适的？

　　模型的训练是采用加法的形式来训练的，形式上，使用 $\hat{y}^{(t)}_i$ 作为第 $i$ 个实例在第 $t$ 次迭代中的预测值，我们需要添加 $f_t$ 来最小化下面的目标函数：

$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)
$$

　　对于上式，XGBoost 使用二阶近似解来快速的优化目标：

$$
\begin{array}{c}{\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)} \\ {\text { where } g_{i}=\partial_{\hat{y}(t-1)} l\left(y_{i}, \hat{y}^{(t-1)}\right) \text { and } h_{i}=\partial_{\hat{y}^{(t-1)}}^{2} l\left(y_{i}, \hat{y}^{(t-1)}\right)}\end{array}
$$

　　其中 $l\left(y_{i}, \hat{y}^{(t-1)}\right)$ 是一个常数，最小化时可以不考虑，那么优化目标简化为：

$$
\tilde{\mathcal{L}}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)
$$

　　对于固定的树结构 $q(x)$，我们可以叶子节点 $j$ 的最优权重 $w_{j}^{*}$:

$$
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
$$

　　计算这棵树的最优值:

$$
\tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
$$

　　这个公式也可以作为得分函数来测量树结构 $q$ 的质量（也就是学习的这棵树对总体结果有没有提升），下图展示了如何计算一棵树的分值

<p align="center">
  <img width="" height="" src="/img/media/15627471733865.jpg">
</p>


　　树的计算分值越低，说明这个树的结构越好。但由于训练数据可能有很多特征，构建一棵树可能有许多种不同的构建形式，我们不可能枚举所有可能的树结构q来一一计算它的分值。故而采用贪心算法来解决这个问题，贪心算法从一个单独树叶开始迭代的增加分支。

　　假设 $I_L$ 和 $I_R$ 是分割之后节点的左侧和右侧实例集合，令 $I=I_{L} \cup I_{R}$，那么在节点分割前-后的损失减少量为：

$$
\mathcal{L}_{s p l i t}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
$$

　　这个公式通常用于在实践中评估候选分裂节点是不是应该分裂。

## 3. 分支节点分裂算法
### 3.1 Basic Exact Greedy Algorithm
　　在树学习中，一个关键问题是**如何找到每一个特征上的分裂点**，比如在本文的例子中，age 的分裂节点是 15 岁，而不是其他年纪。为了找到最佳分裂节点，分裂算法枚举特征上所有可能的分裂点，然后计算得分，这种算法称为 Exact Greedy Algorithm，单机版本的XGBoost 支持这种 Exact Greedy Algorithm，算法如下所示：

<p align="center">
  <img width="" height="" src="/img/media/15627477589527.jpg">
</p>

　　为了有效率的找到最佳分裂节点，算法必须先将该特征的所有取值进行排序，之后按顺序取分裂节点计算 $L_{s p l i t}$。

### 3.2 近似算法
　　Exact Greedy Algorithm 使用贪婪算法非常有效的找到分裂节点，但是当数据量很大时，数据不可能一次性的全部读入到内存中，或者在分布式计算中，这样不可能事先对所有值进行排序，且无法使用所有数据来计算分裂节点之后的树结构得分。为解决这个问题，近似算法被应用进来。近似算法首先按照特征取值的统计分布的一些百分位点确定一些候选分裂点，然后算法将连续的值映射到 buckets 中，然后汇总统计数据，并根据聚合统计数据在候选节点中找到最佳节点。

　　近似算法有两个变体， global variant 和 local variant。

### 3.3 加权分位数（Weighted Quantile Sketch）
　　在近似算法中的重要步骤中是如何确定候选分裂点。通常特征取值的百分位被用于生成候选分裂点。更进一步的我们可以令多集合

$$
\mathcal{D}_{k}=\left\{\left(x_{1 k}, h_{1}\right),\left(x_{2 k}, h_{2}\right) \cdots\left(x_{n k}, h_{n}\right)\right\}
$$

　　表示第 $k$ 个特征的每个训练样本的二阶梯度统计。我们可以定义一个排序函数 $r_{k}$ : $\mathbb{R} \rightarrow[0,+\infty)$：

$$
r_{k}(z)=\frac{1}{\sum_{(x, h) \in \mathcal{D}_{k}} h} \sum_{(x, h) \in \mathcal{D}_{k}, x<z} h
$$

　　表示特征值 $k$ 小于 $z$ 的实例比例。目标就是寻找候选分裂点集 $\\{s_{k 1}, s_{k 2}, \dots s_{k l}\\}$，因而：

$$
\left|r_{k}\left(s_{k, j}\right)-r_{k}\left(s_{k, j+1}\right)\right|<\epsilon, \quad s_{k 1}=\min _{i} \mathbf{x}_{i k}, s_{k l}=\max _{i} \mathbf{x}_{i k}
$$

　　这里 $\epsilon$ 是近似因子，这意味着有大概 $\frac{1}{\epsilon}$ 个候选点。这里每个数据点的权重为 $h_i$。

### 3.4 稀疏感知分割
　　在很多现实业务数据中，训练数据x可能很稀疏。造成这个问题得原因可能是：存在大量缺失值，太多0统计值，one-hot-encoding所致。算法能够处理稀疏模式数据非常重要，XGB在树节点中添加默认划分方向的方法来解决这个问题，如下图所示：

<p align="center">
  <img width="500" height="" src="/img/media/15627482356053.jpg">
</p>


　　当有缺失值时，系统将实例分到默认方向的叶子节点。每个分支都有两个默认方向，最佳的默认方向可以从训练数据中学习到。算法如下：

<p align="center">
  <img width="" height="" src="/img/media/15627482553328.jpg">
</p>


## 4. 总结
XGBoost **优点**：
1. 利用了二阶梯度来对节点进行划分，相对其他 GBM 来说，精度更加高。
2. 利用局部近似算法对分裂节点的贪心算法优化，取适当的 eps 时，可以保持算法的性能且提高算法的运算速度。
3. 在损失函数中加入了 L1/L2 项，控制模型的复杂度，提高模型的鲁棒性。
4. 提供并行计算能力，主要是在树节点求不同的候选的分裂点的Gain Infomation（分裂后，损失函数的差值）。

XGBoost **缺点**：
* 每次迭代训练时需要读取整个数据集，耗时耗内存；每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。
* 使用 Basic Exact Greedy Algorithm 计算最佳分裂节点时需要预先将特征的取值进行排序，排序之后为了保存排序的结果，费时又费内存；需要pre-sorted，这个会耗掉很多的内存空间（2 * #data * # features）。
* 计算分裂节点时需要遍历每一个候选节点，然后计算分裂之后的信息增益，费时；数据分割点上，由于 XGBoost 对不同的数据特征使用 pre-sorted 算法而不同特征其排序顺序是不同的，所以分裂时需要对每个特征单独做依次分割，遍历次数为 (#data * #features) 来将数据分裂到左右子节点上。
* 由于 pre-sorted 处理数据，在寻找特征分裂点时（level-wise），会产生大量的cache随机访问。生成决策树是 level-wise 级别的，也就是预先设置好树的深度之后，每一颗树都需要生长到设置的那个深度，这样有些树在某一次分裂之后效果甚至没有提升但仍然会继续划分树枝，然后再次划分....之后就是无用功了，耗时。
* 尽管使用了局部近似计算，但是处理粒度还是太细了。



优点：
* 可以找到精确的划分条件

缺点：
* 计算量巨大
* 内存占用巨大
* 易产生过拟合

## References
1. [Boosted Tree by Tianqi Chen](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
2. [Loss function Approximation With Taylor Expansion](https://stats.stackexchange.com/questions/202858/loss-function-approximation-with-taylor-expansion)
3. [Tree Boosting With XGBoost](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf)
4. [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
5. [Newton Boosting Tree 算法原理：详解 XGBoost](https://zhuanlan.zhihu.com/p/38297689)
6. [机器学习竞赛大杀器XGBoost--原理篇](https://zhuanlan.zhihu.com/p/31654000)
7. [Gradient Boosting梯度提升-GBDT与XGBoost解析及应用](http://www.tensorinfinity.com/paper_151.html)