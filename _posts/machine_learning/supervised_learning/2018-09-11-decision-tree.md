---
layout: post
title: Decision Tree
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

整理决策树的原理，优缺点和应用情况，并以Python实现。

决策树的三大部分，首先是特征选择，然后是决策树的生成，接着是决策树的剪枝。


## 算法过程
决策树的构造过程秉承着分而治之（divide-and-conquer）的原则，最初版本的决策树算法还是比较好容易实现的。

算法流程：
1. 遍历特征集合，找到能够使得某一个衡量标准达到最大（这里可以将其做成一个虚函数，然后面继承的子类去实现具体的衡量方法，像信息增益，基尼系数等）。
2. 按照选定的特征将样本划分样本，迭代调用构造子树的函数，接着返回该节点的信息，如果是决策结点返回结点信息，如果是叶子结点返回对应的 label 值。

注意点：
* 多子类的处理方法，非二叉树类型
* 决策树预测时也要迭代地从根节点访问到叶节点
* 判断叶节点的类型时如果仍有多个类型的数据在，那么就采用投票的方法。
* 所有的样本最后的归属地都是某一个节点中，决策结点中不包含样本。
* 知道如何计算impurity，计算impurity是有关y的事情，而在决策节点进行选择特征时测涉及到X每列的取值。


### 划分子树
在遍历特征选择最合适划分样本的特征时，有几个划分选择，分别是信息增益、增益率和基尼系数。

#### 信息增益 （Information Gain）
[信息熵](https://binlidaily.github.io/2018-10-23-information-theory/)是度量一个样本集合不纯度最常用的一个指标，假设当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$（$k=1, 2, \dots, |\mathcal { Y }|$），则当前样本集合 $D$ 的信息熵定义为：

$$Ent(D) = - \Sigma_{k=1}^{|\mathcal { Y }|} p_k \log p_k$$

信息熵 $Ent(D)$ 越大，说明不纯度越高，或者说不确定性越大。

如果此时我们用有 $v$ 个可能取值 ${a^1, a^2, \dots, a^V}$ 的离散属性 $a$ 来划分当前样本集合 $D$，能够得到 $V$ 个分支子树，每个分支子树 $D^v$ 都包含各自在属性 $a$ 上的同一取值 $a^v$（这里只是在 $X$ 方面的一致，label 的取值是不一定一样的）。根据上面信息熵的定义，我们可以计算出划分之后的每一个子树的信息熵 $Ent(D^v)$，然后通过对子树信息熵的加权求和得到划分后所有子树的整体信息熵（即所谓的条件熵）：

$$Ent(D, a)=\Sigma_{v=1}^V { {|D^v|}\over {|D|}}Ent(D^v)$$

然后我们比较划分前后的信息熵变化：

$$Gain(D, a)=Ent(D)-Ent(D, a)=Ent(D)-\Sigma_{v=1}^V { {|D^v|}\over {|D|}}Ent(D^v)$$

我们将划分前后的信息熵变化定义为信息增益（Information Gain），信息增益越大，说明当前的划分越能够降低信息的不纯度，亦即越能够降低系统的不确定性，如此得到的决策树决策效果越好。于是我们就选择能够使得信息增益最大的属性进行划分，著名的ID3就是根据信息增益来进行划分属性的。

**Drawbacks**

信息增益有一个比较严重的缺点是会非常倾向取值多的那一类特征，因为属性取值越多，$Ent(D, a)$的结果明显就会越小。当特征中有类似 ID 的属性，这个问题就比较棘手！

#### 增益率（Gain Ratio）
为了克服信息增益的劣势，后人就开始想是不是有这样一种抑制朝着多离散值的属性方向划分呢？那么我们可以设计一种衡量标准，当我们选怎离散值较多的属性的时候信息增益虽然最多，我们设计的这个衡量标准的值也会相应较大，这两个在一定程度上抵消。我们衡量划分的子树的信息量（Split Information，也称 Intrinsic Value）：

$$Split\_info(D, a)=-\Sigma_{v=1}^{V}{ {|D^v|}\over{|D|}}\log_2{ {|D^v|}\over{|D|}}$$

划分信息量可以衡量划分后数据的广度和均匀程度，划分后每个子树的数目越均匀，那么划分信息量就越大。因为信息增益在划分时就有找寻这种类似ID型特征的趋势，所以此时我们取划分信息量的倒数，亦即将信息增益除以划分信息量就能起到抑制效果，这就是所谓的增益率（Gain Ratio）：

$$Gain\_ratio(D, a)={Gain(D, a)\over{Split\_info(D, a)}}$$

于是，现在划分属性的依据是最大化增益率了。

**Drawbacks**
要想增益率尽量大，那么划分信息量就得尽量小，亦即尽量选择分裂子树更少的方式。例如，划分方式A是将数据均匀划分成 $n$ 分，可计算得到划分信息量为 $\log_2 n$ ；划分方式B是将数据均匀分成两个部分，则划分信息量为 $1$；那么划分信息量会倾向选择能得到较小值的方案 $B$，即子树数目小的。我们也可以通过对照对数函数在 $(0,1)$ 区间的趋势图，很明显距离 $1$ 的部分（即平均子树样本数较多的部分，也就是说划分子树较少时）函数结果较小，也就是更倾向选划分子树数目较少的方式！所以，增益率的**弱点**是有一点过分强调了找取值数目较少的属性。

为了避免仅仅因为划分信息量非常小就选择一个特征来进行划分，C4.5在根据增益率划分属性的时候一般采用一种启发式方法，即先计算每个属性的信息增益，然后仅对增益高过平均值的属性应用增益率进行测试。

#### 基尼系数（Gini Index）
之前两个衡量不纯度（不确定性）的标准是依赖信息熵，但是信息熵的计算有 $\log$ 运算，相对来说计算量较大，那么有没有一个没有那么复杂计算的衡量不纯度的方式呢？这就促生了基尼系数（Gini Index）。我们说一个东西的纯度（purity）高，一般指的是其中尽量只包含一种物质，例如纯金的纯度；相对的不纯度（impurity）高说明里面的杂质多，在类别中的说法就是不同类的数目越多。那么，我们同样假设当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$（$k=1, 2, \dots, |\mathcal { Y }|$），那么我们连续抽取两个样本，这两个样本不属同一个类的概率如下：

$$Gini(D)=\Sigma_{k=1}^{|\mathcal { Y }|}p_k(1-p_k)=\Sigma_{k=1}^{|\mathcal { Y }|}p_k-\Sigma_{k=1}^{|\mathcal { Y }|}p_k^2=1-\Sigma_{k=1}^{|\mathcal { Y }|}p_k^2$$

这就是所谓的基尼系数，比较好理解的是，连续选取的两个样本不属于同一个类的概率越大，那么此批样本的不纯度就越高。所以，我们在划分属性的时候，选择能够使得基尼系数最小的属性。

如果样本结合 $D$ 根据属性 $a$ 是否取某一可能值 $c$ 被划分成 $D_1$ 和 $D_2$ 两部分，即：

$$D_1={(x, y)\in D|A(x)=a}, D_2=D-D_1$$

则在特征 $a$ 的条件下，集合 $D$ 的基尼系数定义为：

$$Gini(D,a)={|D_1|\over{|D|}}Gini(D_1)+{|D_2|\over{|D|}}Gini(D_2)$$

**Advantages**
基尼系数在计算上会快一些，因为没有求 $\log$ 运算。

### 递归退出条件
决策树采用递归方式生成，在递归时有如下几个递归结束的条件：
1. 当前结点包含的样本全都属于同一个类别，无法继续划分；（同一类了还分啥？）
2. 当前的属性集为空，没有属性用来划分了；或者即使有属性可划分但是所有样本在所有属性上的衡量结果相同，无法进行划分了；
3. 除了属性为空，当前结点包含的样本结合为空时，无法进一步划分。

注意递归的时候，写完子递归还要确保本层递归的返回！

### 代码实现
```python
# Check if expansion of y is needed
        if len(np.shape(y)) == 1:
            y = np.expand_dims(y, axis=1)
```
为什么要拓展y的维度？

设定 min_samples_split（最小的划分样本），即如果还剩下大于等于这个数字的样本数，那么继续划分。类似的，设定max_depth，防止决策树划分太深；还要设定min_impurity（最小的不纯度），要超过这个最小不纯度，就继续划分。

**代码实现问题**
1、按照开源的代码实现的结果是，准确率一直不太稳定，偏差太大。

#### 预测的做法
也是要遍历地从根节点一直往下延伸到具体的叶节点上，返回的值是叶节点的value。

⁉️ 预测样本出现了，有的类在训练数据集的特征中不存在的情况要怎么处理?

## 决策树应用注意
#### 剪枝处理（Pruning）
决策树比较大的问题是容易过拟合，为了在一定程度上抑制这样的现象就提出了剪枝（Pruning）的操作，剪枝分为预剪枝和后剪枝两种。值得注意的是，不管是预剪枝还是后剪枝，都是在考虑**泛化性能**的提升与否，而不是信息熵。

⁉️ 这里的泛化性能怎么验证？在一定的验证集上做验证？
⁉️ 如果在损失函数那儿加上正则化呢？

**预剪枝**
在决策树的生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来*泛化性能*上的提升，则停止划分并将当前结点标记为叶节点。

**后剪枝**
先从训练集生成完整的决策树，然后自底向上地对每一个非叶子结点进行考察，如果将该结点的子树改成叶结点能够提高决策树的性能，那么就将当前结点的子树改成对应的叶结点。

### 连续值处理
对于离散的标称型数据来说，特征值是离散有限的，但是对于连续特征来说，其连续值就不一定是有限的了，于是我们需要进行连续属性离散化。最简单的而且也正是C4.5中采用的方法叫做二分法（bi-partition）。

假设给定样本集 $D$ 和连续的属性 $a$，假设 $a$ 在样本 $D$ 上有 $n$ 个不同的取值，将这 $n$ 个不同的值按照从小到大的顺序排好，记为 ${a^1, a^2, \dots, a^n}$。那么，基于一个划分点 $t$，我们可以将样本分成 $D^-$ 和 $D^+$ 的两个部分。在相邻的两个取值 ${a^i, a^{i+1}}$ 上，我们知道 $t$ 无论取区间 $[a^i, a^{i+1})$ 的任何一个值，划分结果都不变，因为虽然属性是连续型属性，但是其在训练样本中所有的取值目前是已知的了。可能在测试集中会有取值有不一样的情况，但是也是始终逃不出大小关系的，所以不用担心像标称型属性那样测试集出现训练集中没有的属性取值的苦恼了。

对于连续属性 $a$，我们采取两个相邻取值的中位数 ${a^i+a^{i+1}}\over 2$ 来做一个划分点，于是可以得到 $n-1$ 个候选划分点集合：

$$T_a = \{ { {a^i+a^{i+1}}\over 2} | 1 \leq i \leq n-1 \} $$

然后遍历 $T_a$ 找到能够使得在评判标准下得到最好效果的那个划分点。

$$
\begin{align}
Gain(D, a) &= max_{t \in T_a} Gain(D, a, t) \\
&=max_{t \in T_a} Ent(D) - \Sigma_{\lambda \in\{-,+\}}{|D_t^{\lambda}|\over{D}}Ent(D_t^{\lambda})
\end{align}
$$

### 缺失值处理
对于缺失值处理来说有两个问题需要考虑:
1. 如何在划分属性值缺失的情况下进行划分属性选择？（比如“色泽”这个属性作为划分属性时，有的样本在其上面取值缺失时，要该如何计算信息增益？）
2. 已经选好划分属性，但某样本在该属性上值缺失，该如何对样本进行划分？（该将这个样本划分到哪个类？）

首先，我们要明确一点，缺失值是针对样本的某个属性值来说的。当然也可简单粗暴的进行处理：

* 舍弃缺失值
* 填充缺失值

#### 问题一：有属性值缺失，如何选哪个做划分属性？
这个是在选择划分属性时需要面临的问题，我们要判断是否将这个属性作为划分属性，按照之前的流程我们需要计算其信息增益，但是有的样本在这个属性上取值是缺失的，那么我在计算信息增益时该如何处理这些样本的缺失值呢？

像 CART 算法就采用一种基于无缺失值样本来计算信息增益的方式，首先给定训练集 $D$ 和属性 $a$，然后定义在属性 $a$ 上无缺失值的样本子集为 $\tilde { D }$。$\tilde { D } ^ { v }$ 表示 $\tilde { D }$ 中在属性 $a$ 上取值为 $a ^ { v }$ 的样本子集；$\tilde { D } _ { k }$ 表示 $\tilde { D }$ 中属于第 $k$ 类（$ k=1, 2, \dots,\mid\mathcal{Y} \mid $）的样本子集。

我们需要知道信息增益，那么将其分成两个部分，第一个是对已知值（即无缺失值的部分）所带来的信息增益；第二部分是对未知值（即缺失值）带来的信息增益，通过加权平均的方式综合起来。而一个明显的事实是，对于未知值而言我们的能获得的信息量是零，于是我们有下面的式子：

$$
\begin{aligned} \operatorname { gain } ( D, a ) & =  \text { probability of } a \text { is known } \times  \operatorname { gain } (\tilde { D }, a) \\ & + \text { probability of } a \text { is not known } \times 0 \\ 
& = \rho \times  \operatorname { gain } (\tilde { D }, a)  \\
& = \rho \times  \left(\operatorname { Ent } ( \tilde { D } ) - \operatorname { Ent } ( \tilde { D }, a) \right)\\
& = \rho \times \left( \operatorname { Ent } ( \tilde { D } ) - \sum _ { v = 1 } ^ { V } \tilde { r } _ { v } \operatorname { Ent } \left( \tilde { D } ^ { v } \right) \right)
\end{aligned}
$$

假定我们为每个样本 $x$ 赋予一个权重 $w_x$，那么上面的式子中 $\rho$ 表示无缺失值样本占所有样本的比例：

$$\rho = \frac { \sum _ { x \in \tilde { D } } w _ { x } } { \sum _ { x \in D } w _ { x } }$$

$\tilde { r } _ { v }$ 表示无缺失值的样本在属性 $a$ 上取值为 $a^v$ 的样本占无缺失值样本子集的比例：

$$
\tilde { r } _ { v } = \frac { \sum _ { x \in \tilde { D } ^ { v } } w _ { x } } { \sum _ { x \in \tilde { D } } w _ { x } } \quad ( 1 \leq v \leq V )
$$

可以计算出无缺失值样本子集的信息熵为:

$$
\operatorname { Ent } ( \tilde { D } ) = - \sum _ { k = 1 } ^ { | \mathcal { Y | } } \tilde { p } _ { k } \log _ { 2 } \tilde { p } _ { k }
$$

其中 $\tilde { p } _ { k }$ 表示无缺失值样本中第 $k$ 类样本所占的百分比：

$$
\tilde { p } _ { k } = \frac { \sum _ { x \in \tilde { D } _ { k } } w _ { x } } { \sum _ { x \in \overline { D } } w _ { x } } ( 1 \leq k \leq | \mathcal { Y } | )
$$

所以，当样本在划分属性上缺失时，我们就用上面的 $\operatorname { gain } ( D, a )$ 来计算信息增益，进而选择对应的划分属性。值得注意的是，这里在处理缺失值的时候假设了每个样本都有一个权值，在学习开始初期这些权重初始化为 1。设置每个样本都有权重的好处或许是使得算法更灵活一些，对 imbalance 问题处理得更好，但是一个问题是要如何去更新每个参数的权重？⁉️

#### 问题二：样本值划分属性上缺失，如何划分该样本？
若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子结点，且样本的权值在属性值为 $a^v$ 对应的子结点上调整为 $\tilde{r} \cdot w_x$。我们知道 $\sum _ { k = 1 } ^ { | \mathcal { Y } | } \tilde { p } _ { k } = 1$，$\sum _ { v = 1 } ^ { V } \tilde { r } _ { v } = 1$，可见这里是将原来这单个样本的权值按照不同的概率分拆成了多份划分到对应的子结点中去了。这样很好理解，因为没有足够多的信息，我们不知道要如何划分该样本，每个子结点都有可能接收这个样本，但是子结点接收的概率是不同的，那么我们就将该单个样本的信息按照相应的概率分给各个子结点。


#### 另外的问题：模型训练好，但测试时出现缺失值
这时候，就不能按比例分配了，因为你必须给该样本一个确定的label，而不是薛定谔的label。这时候根据投票来确定，或者填充缺失值。

## 实际应用
### ID3
### C4.5
### CART
只是二分类能保证所有类别能在叶子结点上体现？继续细分是可以的！

分类与回归树（Classification And Regression Tree，CART）是目前应用广泛的决策树学习方法。CART 是给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的**条件概率分布**的学习方法。CART 假设决策树是二叉树，其决策结点特征的取值为“是”或者“否”，其左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于迭代的二分每个特征，将输入空间划分成有限个单元，并在这些单元上确定预测的概率分布。⁉️ 为什么说会涉及到概率分布呢？

⁉️ 为什么想到一定要用二叉树？

CART 算法一般分两个步骤:
1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大（怎么保证？）。
2. 决策树剪枝：用**验证数据集**对已生成的决策树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

CART 的生成就是迭代地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。接下来我们从回归树和分类树分别介绍 CART。

#### 回归树的生成
回归树的生成既然要迭代地构建二叉树，那么很直接地就有两个问题摆在面前：
1. 划分点怎么找？
2. 因为划分后为二叉树，原本可能有很多取值的结果要怎么输出？

以一般机器学习模型的训练目标为参考，我们在划分点的寻找上也想设定一种有着类似训练目标的划分点寻找方式，于是就有前辈采用了**均方误差最小**的原则来寻找合适的划分点。那么对应的回归树度量目标就是：对于任意的划分属性 $a$（假设划分属性 $a$ 是训练样本的第 $j$ 个变量，记作 $x_j$），其对应有任意的划分点 $s$ 将数据集分成 $R_1$ 和 $R_2$ 两个部分，我们要找这样的划分点，其能够使得 $R_1$ 和 $R_2$ 两个集合各自的均方差最小，同时能够使得这两个均方差的加和最小。(⁉️外层最小时会不会让内层最小调整？在设计该算法的时候有什么考虑？)

接下来对具体到公式上，首先假设 $X$ 和 $Y$ 分别为输入和输出变量，并且 $Y$ 为连续变量，给定数据集：

$$
D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( x _ { N } , y _ { N } \right) \right\}
$$

其中 $x _ { i } = \left( x _ { i } ^ { ( 1 ) } , x _ { i } ^ { ( 2 ) } , \ldots , x _ { i } ^ { ( n ) } \right)$ 为输入实例（特征向量），$i=1,2,\dots,N$，$n$ 为特征个数，$N$ 为样本总量。

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已知将输入空间分成了 $M$ 个单元 $R_1, R_2, \dots, R_M$（这个 $M$ 的取值得看最后回归树生成到什么程度对应的划分空间），并且每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可以表示为：

$$
f ( x ) = \sum _ { m = 1 } ^ { M } c _ { m } I \left( x \in R _ { m } \right)
$$

其中 $I$ 为指示函数，$I = \left \{ \begin{array} { l l } { 1 } & { i f \left( x \in R _ { m  } \right) } \\ { 0 } & { i f \left( x \notin R _ { m } \right) } \end{array} \right.$

可以用李宏毅视频上的截图来介绍！⁉️

当输入空间的划分确定时，我们用平方误差 $\sum _ { x _ { i } \in R _ { m } } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$ 来表示回归树对于训练数据的预测误差，用平方误差最小化原则来求解每个单元上的最优输出值。通过以下的证明，我们可以知道单元 $R_m$ 上的 $c_m$ 的最优值是 $R_m$ 上所有输入实例 $x_i$ 对应的输出的 $y_i$ 的均值：

$$
\hat { c } _ { m } = \operatorname { ave } \left( y _ { i } | x _ { i } \in R _ { m } \right)
$$

给定一个随机的数列 $\{ x _ { 1 } , x _ { 2 } , \dots , x _ { n } \}$，假设该空间中最优的输出值为 $a$，则根据最小平方误差准则，我们能得到有关 $a$ 的函数如下：
$$
F ( a ) = \left( x _ { 1 } - a \right) ^ { 2 } + \left( x _ { 2 } - a \right) ^ { 2 } + \ldots + \left( x _ { n } - a \right) ^ { 2 }
$$

我们考察其单调性：
$$
F ^ { \prime } ( a ) = - 2 \left( x _ { 1 } - a \right) - 2 \left( x _ { 2 } - a \right) + \ldots - 2 \left( x _ { n } - a \right) = 2 n a - 2 \sum _ { i = 1 } ^ { n } x _ { i }
$$

$$F ^ { \prime \prime } ( a ) = 2n > 0$$

二阶导为正数，所以该函数为严格凸函数，那么令 $F ^ { \prime } ( a ) = 0$，得到 $a = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }$，即最小值点为：

$$
\hat { a } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }
$$

那么接下来就是要讨论该如何划分了，选择第 $j$ 个变量 $x^{(j)}$ 和它的取值 $s$，作为切分变量（Splitting variable）和切分点（Splitting point），则由此划分的两个区域为：

$$R _ { 1 } ( j , s ) = \{ x | x ^ { ( j ) } \leqslant s \} \text{ 和 } R _ { 2 } ( j , s ) = \{ x | x ^ { ( j ) } > s \}$$

问题就转化成如何求解最优划分变量 $j$ 和 $s$，具体的有以下的方式：

$$
\min _ { j , s } \left[ \min _ { c _ { 1 } } \sum _ { x _ { i } \in R _ { \perp } ( j , s ) } \left( y _ { i } - c _ { 1 } \right) ^ { 2 } + \min _ { c _ { 2 } } \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - c _ { 2 } \right) ^ { 2 } \right]
$$

对于固定输入变量 $j$，可以利用求均值的方式在内循环的 min 中找到这两个区域中的最优切分点 $s$：
$$
\hat { c _ { 1 } } = \frac { 1 } { N _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } y _ { i } \text{ 和 } \hat { c _ { 2 } } = \frac { 1 } { N _ { 2 } } \sum _ { x _ { j } \in R _ { 3 } ( j , s ) } y _ { i }
$$

其对应的求解最优划分变量的方式可以写成:

$$
\min _ { j , s } \left[ \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - \hat { c _ { 1 } } \right) ^ { 2 } + \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - \hat { c _ { 2 } } \right) ^ { 2 } \right]
$$

接着据此遍历所有的输入变量 $j$ 从而在外循环的 min 中找到最优的切分变量，如此构成了一个划分对 $(j,s)$。据此可以将输入空间划分为两个区域，接着循环执行这个划分流程，直到满足停止条件。

#### 分类树的生成
与 ID3 和 C4.5 不同的是，CART 划分的结果是二叉树。所以在生成分类树时只要对每一个属性 $a$ 的每一个取值 $a^v$ 做一个“当前属性值是否等于 $a^v$”的判断，就可以分成将样本集 $D$ 分成 $D_1$ 和 $D_2$ 两个部分。然后在选取最优划分点时，找到能够使得基尼系数最小化的划分对，即对应的属性 $a$ （相当于回归树里面的 $j$）和该属性的取值 $a^v$（相当于回归树里面的 s）。然后对划分的两个子结点迭代的进行该流程，直到满足停止条件。

#### CART 剪枝

## 总结
代码实现算法的时候，首先第一步需要将每一步要做什么细节的用伪代码的形式写下来才能比较好的进行 Code。
1. 模型的起点，如何更新，结束条件有哪些
2. 如何预测
    * 如何计算
3. 返回什么东西？？

⁉️ 自己实现的 Decision Tree 为什么不稳定？
* 可以对照着参考的算法，一步一步调试找问题……

零星待整理的几点：
1. 决策树的主要优势在于数据形式非常容易理解。
2. 决策树是后续很多重要的集成方法的基础。
3. 树构造算法只适用于标称型数据，数值型数据必须进行离散化。

**优点**：
1. 计算复杂度不高
2. 输出结果易于理解
3. 对中间值的缺失不敏感
4. 可以处理不相关特征数据

**缺点**：
1. 可能会产生过度匹配问题，即 Overfitting。


## References
1. [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
2. [Page 11 - BASIC CONCEPTS IN INFORMATION THEORY](http://www-public.imtbs-tsp.eu/~uro/cours-pdf/poly.pdf)
3. [Intuitive explanation of entropy](https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy)
4. [决策树算法原理](http://www.cnblogs.com/pinard/p/6050306.html)
5. [A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
6. [决策树—回归](https://zhuanlan.zhihu.com/p/42505644)
