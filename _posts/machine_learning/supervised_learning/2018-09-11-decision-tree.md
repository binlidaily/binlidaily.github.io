---
layout: post
title: Decision Tree
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

整理决策树的原理，优缺点和应用情况，并以Python实现。

零星待整理的几点：
1. 决策树的主要优势在于数据形式非常容易理解。
2. 决策树是后续很多重要的集成方法的基础。
3. 树构造算法只适用于标称型数据，数值型数据必须进行离散化。

**优点**：
1. 计算复杂度不高
2. 输出结果易于理解
3. 对中间值的缺失不敏感
4. 可以处理不相关特征数据

**缺点**：
1. 可能会产生过度匹配问题，即 Overfitting。

决策树实现算法类型：
1. Classification and Regression Tree (CART)
2. Iterative Dichotomiser 3 (ID3)
3. C4.5 and C5.0 (different versions of a powerful approach

## 信息论预备知识
### 信息量（Information Content）
信息量是一个对信息的度量，就好比时分秒对时间的度量一样，信息是相对于信息源对接收者接收到的信息而言的。那么如何衡量当一个事件发生时，我们能够接受到多少信息量呢？我们可以从随机事件发生概率的角度来看待这个衡量方式。

一个比较容易接受的是，如果一个事件发生的概率越小，其发生后我们获得信息是越多的。比如像太阳明天会从东方升起这样的大概率（为真，概率为1）事件对我们来说基本上没有什么信息量，但是如果有一天太阳从西边出来了这种可以说概率极低的事件发生了信息量就巨大了。如此，我们可以将衡量随机事件$E$发生所包含信息量多少的方式看成是一个针对事件发生概率的函数$h(E)$，且事件发生的概率$P(E)$越小，其所包含的信息量$h(E)$越大。那么很明显的，这就**要求此函数要是连续的，且是递减的。**不难理解的是，**对于相互独立的两个事件，我们观察到他们同时发生所包含的信息量和分别观察到他们单独发生时所包含的信息量应该是一样的。**

那么，对于衡量随机事件$E$发生所包含信息量函数$h(E)$必须要满足以下三个条件：
1. 信息量函数$h(E)$必须是连续且递减的。
2. 当信息量$h(E)$为0时，事件发生的概率$P(E)$为1。
3. 当事件$E$和事件$F$相互独立时，$P(E\cap F)=P(E)+P(F)$。

而满足以上三条公理的只有如下的对数函数，则信息量的表达式是：

$$h(E)=\log{1\over{P(E)}}=-\log P(E)$$

对于这样的信息量公式，其实底数可以取很多（比如，2、e、3、10等），一般按照香农的取法采用底数2。

### 信息熵 （Information Entropy）
信息量所度量的是一个事件发生后所包含的信息量，那么我们拓展一下，当信息接收者面对所有可能发生的独立事件时该如何衡量接收者接收到的信息量呢？于是可以想到，用**所有可能发生的事件所包含的信息量的期望值**，即所谓的信息熵来衡量。

具体定义信息熵$H(X)$前我们先做一些设定，一个离散的随机变量$X$的取值为$\{x_1, x_2, \dots ,x_n\}$，且取到每个值的概率对应为$p_i=\{X=x_i\}$；那么对所有可能取值$\{X=x_1\}, \{X=x_2\}, \dots, \{X=x_n\}$的信息量期望，即信息熵$H(X)$为：

$$H(X)=-\Sigma_{i=1}^n p_i \log p_i$$

对于信息熵而言，值得注意的是：
1. $n$可以是无穷大。
2. 信息熵$H(X)$只跟$X$的分布有关，跟其具体的取值无关。
3. 如果$n$是有限的，那么当$X$时均匀分布时，即$p_i={1\over n}$ $\forall i \in \{1,2,\dots,n\}$，$H(X)$取最大值$\log n$。这里就可以从对于熵的不确定性解释上理解了（信息熵描述的就是对不确定性的一种度量，信息熵越大，不确定性就越大），当所有可能取值的取值概率一样时，观察者就根本没有办法有猜哪种取值最有可能的依据，那么这个时候取值结果当然是最不确定的，也可以说此时整体的信息量是最大的。所有取值之间的差距越大，取值结果就越有偏向，那么不确定性就越低。
4. $H(X)=0$等价于$p_i = 0 and p_j = 1, \forall i \neq j$。

## 算法过程
决策树的构造过程秉承着分而治之（divide-and-conquer）的原则，最初版本的决策树算法还是比较好容易实现的。

算法流程：
1. 遍历特征集合，找到能够使得某一个衡量标准达到最大（这里可以将其做成一个虚函数，然后面继承的子类去实现具体的衡量方法，像信息增益，基尼系数等）。
2. 按照选定的特征将样本划分样本，迭代调用构造子树的函数，接着返回该节点的信息，如果是决策结点返回结点信息，如果是叶子结点返回对应的label值。

注意点：
* 多子类的处理方法，非二叉树类型
* 决策树预测时也要迭代地从根节点访问到叶节点
* 判断叶节点的类型时如果仍有多个类型的数据在，那么就采用投票的方法。
* 所有的样本最后的归属地都是某一个节点中，决策结点中不包含样本。
* 知道如何计算impurity，计算impurity是有关y的事情，而在决策节点进行选择特征时测涉及到X每列的取值。


### 划分子树
在遍历特征选择最合适划分样本的特征时，有几个划分选择，分别是信息增益、增益率和基尼系数。

#### 信息增益 （Information Gain）
信息熵是度量一个样本集合不纯度最常用的一个指标，假设当前样本集合$D$中第$k$类样本所占比例为$p_k$（$k=1, 2, \dots, |y|$），则当前样本集合$D$的信息熵定义为：

$$Ent(D) = - \Sigma_{k=1}^{|y|} p_k \log p_k$$

信息熵$Ent(D)$越大，说明不纯度越高，或者说不确定性越大。

如果此时我们用有$v$个可能取值${a^1, a^2, \dots, a^V}$的离散属性$a$来划分当前样本集合$D$，能够得到$V$个分支子树，每个分支子树$D^v$都包含各自在属性$a$上的同一取值$a^v$（这里只是$X$方面的一致，label的取值是不一定一样的）。根据上面信息熵的定义，我们可以计算出划分之后的每一个子树的信息熵$Ent(D^v)$，然后通过对子树信息熵的加权求和得到划分后所有子树的整体信息熵（这个其实是条件熵）：

$$Ent(D, a)=\Sigma_{v=1}^V { {|D^v|}\over {|D|}}Ent(D^v)$$

然后我们比较划分前后的信息熵变化：

$$Gain(D, a)=Ent(D)-Ent(D, a)=Ent(D)-\Sigma_{v=1}^V { {|D^v|}\over {|D|}}Ent(D^v)$$

我们将划分前后的信息熵变化定义为信息增益（Information Gain），信息增益越大，说明当前的划分越能够降低信息的不纯度，亦即越能够降低系统的不确定性，如此得到的决策树决策效果越好。于是我们就选择能够使得信息增益最大的属性进行划分，著名的ID3就是根据信息增益来进行划分属性的。

**Drawbacks**
信息增益有一个比较严重的缺点是会非常倾向取值多的那一类特征，因为属性取值越多，$Ent(a\_braches)$的结果明显就会越小。当特征中有类似ID的属性，这个问题就比较棘手！

#### 增益率（Gain Ratio）
为了克服信息增益的劣势，后人就开始想是不是有这样一种抑制朝着多离散值的属性方向划分呢？那么我们可以设计一种衡量标准，当我们选怎离散值较多的属性的时候信息增益虽然最多，我们设计的这个衡量标准的值也会相应较大，这两个在一定程度上抵消。我们衡量划分的子树的信息量（Split Information，也称 Intrinsic Value）：

$$Split\_info(D, a)=-\Sigma_{v=1}^{V}{{|D^v|}\over{|D|}}\log_2{{|D^v|}\over{|D|}}$$

划分信息量可以衡量划分后数据的广度和均匀程度，划分后每个子树的数目越均匀，那么划分信息量就越大。因为信息增益在划分时就有找寻这种类似ID型特征的趋势，所以此时我们取划分信息量的倒数，亦即将信息增益除以划分信息量就能起到抑制效果，这就是所谓的增益率（Gain Ratio）：

$$Gain\_ratio(D, a)={Gain(D, a)\over{Split\_info(D, a)}}$$

于是，现在划分属性的依据是最大化增益率了。

**Drawbacks**
要想增益率尽量大，那么划分信息量就得尽量小，亦即尽量选择分裂子树更少的方式。例如，划分方式A是将数据均匀划分成n分，可计算得到划分信息量为$\log_2 n$；划分方式B是将数据均匀分成两个部分，则划分信息量为$1$；那么划分信息量会倾向选择能得到较小值的方案B，即子树数目小的。我们也可以通过对照对数函数在(0,1)区间的趋势图，很明显距离1的部分（即平均子树样本数较多的部分，也就是说划分子树较少时）函数结果较小，也就是更倾向选划分子树数目较少的方式！所以，增益率的**弱点**是有一点过分强调了找取值数目较少的属性。

为了避免仅仅因为划分信息量非常小就选择一个特征来进行划分，C4.5在根据增益率划分属性的时候一般采用一种启发式方法，即先计算每个属性的信息增益，然后仅对增益高过平均值的属性应用增益率进行测试。

#### 基尼系数（Gini Index）
之前两个衡量不纯度（不确定性）的标准是依赖信息熵，但是信息熵的计算有$\log$运算，那么有没有一个没有那么复杂计算的衡量不纯度的方式呢？这就促生了基尼系数（Gini Index）。我们说一个东西的纯度（purity）高，一般指的是其中只包含一种物质，例如纯金的纯度；相对的不纯度（impurity）高说明里面的杂质多，在类别中的说法就是不同类的数目越多。那么，我们同样假设当前样本集合$D$中第$k$类样本所占比例为$p_k$（$k=1, 2, \dots, |y|$），那么我们连续抽取两个样本，这两个样本不属同一个类的概率如下：

$$Gini(D)=\Sigma_{k=1}^{|y|}p_k(1-p_k)=\Sigma_{k=1}^{|y|}p_k-\Sigma_{k=1}^{|y|}p_k^2=1-\Sigma_{k=1}^{|y|}p_k^2$$

这就是所谓的基尼系数，比较好理解的是，连续选取的两个样本不属于同一个类的概率越大，那么此批样本的不纯度就越高。所以，我们在划分属性的时候，选择能够使得基尼系数最小的属性。

如果样本结合$D$根据属性$a$是否取某一可能值$c$被划分成$D_1$和$D_2$两部分，即：

$$D_1={(x,y)\in D|A(x)=a}, D_2=D-D_1$$

则在特征$a$的条件下，集合$D$的基尼系数定义为：

$$Gini(D,a)={|D_1|\over{|D|}}Gini(D_1)+{|D_2|\over{|D|}}Gini(D_2)$$

**Advantages**
基尼系数在计算上会快一些，因为没有求$\log$运算。

### 递归退出条件
决策树采用递归方式生成，在递归时有如下几个递归结束的条件：
1. 当前结点包含的样本全都属于同一个类别，无法继续划分；（同一类了还分啥？）
2. 当前的属性集为空，没有属性用来划分了；或者即使有属性可划分但是所有样本在所有属性上的衡量结果相同，无法进行划分了；
3. 除了属性为空，当前结点包含的样本结合为空时，无法进一步划分。

注意递归的时候，写完子递归还要确保本层递归的返回！

### 代码实现
```python
# Check if expansion of y is needed
        if len(np.shape(y)) == 1:
            y = np.expand_dims(y, axis=1)
```
为什么要拓展y的维度？

设定min_samples_split（最小的划分样本），即如果还剩下大于等于这个数字的样本数，那么继续划分。类似的，设定max_depth，防止决策树划分太深；还要设定min_impurity（最小的不纯度），要超过这个最小不纯度，就继续划分。

**代码实现问题**
1、按照开源的代码实现的结果是，准确率一直不太稳定，偏差太大。

#### 预测的做法
也是要遍历地从根节点一直往下延伸到具体的叶节点上，返回的值是叶节点的value。


## 决策树应用注意
#### 剪枝处理（Pruning）
决策树比较大的问题是容易过拟合，为了在一定程度上抑制这样的现象就提出了剪枝（Pruning）的操作，剪枝分为预剪枝和后剪枝两种。

**预剪枝**
在决策树的生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来*泛化性能*上的提升，则停止划分并将当前结点标记为叶节点。

**后剪枝**
先从训练集生成完整的决策树，然后自底向上地对每一个非叶子结点进行考察，如果将该结点的子树改成叶结点能够提高决策树的性能，那么就将当前结点的子树改成对应的叶结点。

### 连续值处理


### 缺失值处理


## 实际应用
### ID3
### C4.5
### CART
只是二分类能保证所有类别能在叶子结点上体现？

## 回归树




## 总结
代码实现算法的时候，首先第一步需要将每一步要做什么细节的用伪代码的形式写下来才能比较好的进行Code。
1. 模型的起点，如何更新，结束条件有哪些
2. 如何预测
    * 如何计算
3. 返回什么东西？？

## References
1. [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
2. [Page 11 - BASIC CONCEPTS IN INFORMATION THEORY](http://www-public.imtbs-tsp.eu/~uro/cours-pdf/poly.pdf)
3. [Intuitive explanation of entropy](https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy)
4. [决策树算法原理](http://www.cnblogs.com/pinard/p/6050306.html)
5. [A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
