---
layout: post
title: Decision Tree
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

整理决策树的原理，优缺点和应用情况，并以Python实现。

零星待整理的几点：
1. 决策树的主要优势在于数据形式非常容易理解。
2. 决策树是后续很多重要的集成方法的基础。
3. 树构造算法只适用于标称型数据，数值型数据必须进行离散化。

**优点**：
1. 计算复杂度不高
2. 输出结果易于理解
3. 对中间值的缺失不敏感
4. 可以处理不相关特征数据

**缺点**：
1. 可能会产生过度匹配问题，即 Overfitting。

决策树实现算法类型：
1. Classification and Regression Tree (CART)
2. Iterative Dichotomiser 3 (ID3)
3. C4.5 and C5.0 (different versions of a powerful approach

## 信息论预备知识
### 信息量（Information Content）
信息量是一个对信息的度量，就好比时分秒对时间的度量一样，信息是相对于信息源对接收者接收到的信息而言的。那么如何衡量当一个事件发生时，我们能够接受到多少信息量呢？我们可以从随机事件发生概率的角度来看待这个衡量方式。

一个比较容易接受的是，如果一个事件发生的概率越小，其发生后我们获得信息是越多的。比如像太阳明天会从东方升起这样的大概率（为真，概率为1）事件对我们来说基本上没有什么信息量，但是如果有一天太阳从西边出来了这种可以说概率极低的事件发生了信息量就巨大了。如此，我们可以将衡量随机事件$E$发生所包含信息量多少的方式看成是一个针对事件发生概率的函数$h(E)$，且事件发生的概率$P(E)$越小，其所包含的信息量$h(E)$越大。那么很明显的，这就**要求此函数要是连续的，且是递减的。**不难理解的是，**对于相互独立的两个事件，我们观察到他们同时发生所包含的信息量和分别观察到他们单独发生时所包含的信息量应该是一样的。**

那么，对于衡量随机事件$E$发生所包含信息量函数$h(E)$必须要满足以下三个条件：
1. 信息量函数$h(E)$必须是连续且递减的。
2. 当信息量$h(E)$为0时，事件发生的概率$P(E)$为1。
3. 当事件$E$和事件$F$相互独立时，$P(E\cap F)=P(E)+P(F)$。

而满足以上三条公理的只有如下的对数函数，则信息量的表达式是：

$$h(E)=\log{1\over{P(E)}}=-\log P(E)$$

对于这样的信息量公式，其实底数可以取很多（比如，2、e、3、10等），一般按照香农的取法采用底数2。

### 信息熵 （Information Entropy）
信息量所度量的是一个事件发生后所包含的信息量，那么我们拓展一下，当信息接收者面对所有可能发生的独立事件时该如何衡量接收者接收到的信息量呢？于是可以想到，用**所有可能发生的事件所包含的信息量的期望值**，即所谓的信息熵来衡量。

具体定义信息熵$H(X)$前我们先做一些设定，一个离散的随机变量$X$的取值为$\{x_1, x_2, \dots ,x_n\}$，且取到每个值的概率对应为$p_i=\{X=x_i\}$；那么对所有可能取值$\{X=x_1\}, \{X=x_2\}, \dots, \{X=x_n\}$的信息量期望，即信息熵$H(X)$为：

$$H(X)=-\Sigma_{i=1}^n p_i \log p_i$$

对于信息熵而言，值得注意的是：
1. $n$可以是无穷大。
2. 信息熵$H(X)$只跟$X$的分布有关，跟其具体的取值无关。
3. 如果$n$是有限的，那么当$X$时均匀分布时，即$p_i={1\over n}$ $\forall i \in \{1,2,\dots,n\}$，$H(X)$取最大值$\log n$。这里就可以从对于熵的不确定性解释上理解了（信息熵描述的就是对不确定性的一种度量，信息熵越大，不确定性就越大），当所有可能取值的取值概率一样时，观察者就根本没有办法有猜哪种取值最有可能的依据，那么这个时候取值结果当然是最不确定的，也可以说此时整体的信息量是最大的。所有取值之间的差距越大，取值结果就越有偏向，那么不确定性就越低。
4. $H(X)=0$等价于$p_i = 0 and p_j = 1, \forall i \neq j$。

## 算法过程
决策树的构造过程秉承着分而治之（divide-and-conquer）的原则。

划分的目的是将属性集全部用完，并且得到的决策树在某种衡量标准下（信息增益，基尼系数等）达到最好的状态。

TODO
* 多类的处理方法，非二叉树类型

算法流程：
1. 生成一个结点
2. 判断是否有划分必要
    * 有必要：根据某个特征，对应这个特征下的所有取值分成对应的子节点，继续对子节点进行步骤1、2的过程。
    * 无必要：返回
3. 判断最终结束条件

### 划分子树

判断叶节点的类型时如果仍有多个类型的数据在，那么就采用投票的方法。

所有的样本最后的归属地都是某一个节点中，决策结点中不包含样本。

知道如何计算impurity，计算impurity是有关y的事情，而在决策节点进行选择特征时测涉及到X每列的取值。


### 递归退出条件
决策树采用递归方式生成，在递归时有如下几个递归结束的条件：
1. 当前结点包含的样本全都属于同一个类别，无法继续划分；（同一类了还分啥？）
2. 当前的属性集为空，没有属性用来划分了；或者即使有属性可划分但是所有样本在所有属性上的衡量结果相同，无法进行划分了；
3. 除了属性为空，当前结点包含的样本结合为空时，无法进一步划分。

注意递归的时候，写完子递归还要确保本层递归的返回！

### 代码实现
```python
# Check if expansion of y is needed
        if len(np.shape(y)) == 1:
            y = np.expand_dims(y, axis=1)
```
为什么要拓展y的维度？

设定min_samples_split（最小的划分样本），即如果还剩下大于等于这个数字的样本数，那么继续划分。类似的，设定max_depth，防止决策树划分太深；还要设定min_impurity（最小的不纯度），要超过这个最小不纯度，就继续划分。

#### 预测的做法

## 回归树


## 决策树应用注意
### 连续值处理


### 缺失值处理


## 实际应用

## 总结
代码实现算法的时候，首先第一步需要将每一步要做什么细节的用伪代码的形式写下来才能比较好的进行Code。
1. 模型的起点，如何更新，结束条件有哪些
2. 如何预测
    * 如何计算
3. 返回什么东西？？

## References
1. [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
2. [Page 11 - BASIC CONCEPTS IN INFORMATION THEORY](http://www-public.imtbs-tsp.eu/~uro/cours-pdf/poly.pdf)
3. [Intuitive explanation of entropy](https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy)
