---
layout: post
title: Decision Tree
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

整理决策树的原理，优缺点和应用情况，并以Python实现。

零星待整理的几点：
1. 决策树的主要优势在于数据形式非常容易理解。
2. 决策树是后续很多重要的集成方法的基础。
3. 树构造算法只适用于标称型数据，数值型数据必须进行离散化。

**优点**：
1. 计算复杂度不高
2. 输出结果易于理解
3. 对中间值的缺失不敏感
4. 可以处理不相关特征数据

**缺点**：
1. 可能会产生过度匹配问题，即 Overfitting。

决策树实现算法类型：
1. Classification and Regression Tree (CART)
2. Iterative Dichotomiser 3 (ID3)
3. C4.5 and C5.0 (different versions of a powerful approach

## 信息论预备知识
### 信息量（Information Content）
信息量是一个对信息的度量，就好比时分秒对时间的度量一样，信息是相对于信息源对接收者接收到的信息而言的。那么如何衡量当一个事件发生时，我们能够接受到多少信息量呢？我们可以从随机事件发生概率的角度来看待这个衡量方式。

一个比较容易接受的是，如果一个事件发生的概率越小，其发生后我们获得信息是越多的。比如像太阳明天会从东方升起这样的大概率（为真，概率为1）事件对我们来说基本上没有什么信息量，但是如果有一天太阳从西边出来了这种可以说概率极低的事件发生了信息量就巨大了。如此，我们可以将衡量随机事件$E$发生所包含信息量多少的方式看成是一个针对事件发生概率的函数$h(E)$，且事件发生的概率$P(E)$越小，其所包含的信息量$h(E)$越大。那么很明显的，这就**要求此函数要是连续的，且是递减的。**不难理解的是，**对于相互独立的两个事件，我们观察到他们同时发生所包含的信息量和分别观察到他们单独发生时所包含的信息量应该是一样的。**

那么，对于衡量随机事件$E$发生所包含信息量函数$h(E)$必须要满足以下三个条件：
1. 信息量函数$h(E)$必须是连续且递减的。
2. 当信息量$h(E)$为0时，事件发生的概率$P(E)$为1。
3. 当事件$E$和事件$F$相互独立时，$P(E\cap F)=P(E)+P(F)$。

而满足以上三条公理的只有如下的对数函数，则信息量的表达式是：

$$h(E)=\log{1\over{P(E)}}=-\log P(E)$$

对于这样的信息量公式，其实底数可以取很多（比如，2、e、3、10等），一般按照香农的取法采用底数2。

### 信息熵 （Information Entropy）
信息量所度量的是一个事件发生后所包含的信息量，那么我们拓展一下，当信息接收者面对所有可能发生的独立事件时该如何衡量接收者接收到的信息量呢？于是可以想到，用**所有可能发生的事件所包含的信息量的期望值**，即所谓的信息熵来衡量。

具体定义信息熵$H(X)$前我们先做一些设定，一个离散的随机变量$X$的取值为$\{x_1, x_2, \dots ,x_n\}$，且取到每个值的概率对应为$p_i=\{X=x_i\}$；那么对所有可能取值$\{X=x_1\}, \{X=x_2\}, \dots, \{X=x_n\}$的信息量期望，即信息熵$H(X)$为：

$$H(X)=-\Sigma_{i=1}^n p_i \log p_i$$

对于信息熵而言，值得注意的是：
1. $n$可以是无穷大。
2. 信息熵$H(X)$只跟$X$的分布有关，跟其具体的取值无关。
3. 如果$n$是有限的，那么当$X$时均匀分布时，即$p_i={1\over n}$ $\forall i \in \{1,2,\dots,n\}$，$H(X)$取最大值$\log n$。这里就可以从对于熵的不确定性解释上理解了（信息熵描述的就是对不确定性的一种度量，信息熵越大，不确定性就越大），当所有可能取值的取值概率一样时，观察者就根本没有办法有猜哪种取值最有可能的依据，那么这个时候取值结果当然是最不确定的，也可以说此时整体的信息量是最大的。所有取值之间的差距越大，取值结果就越有偏向，那么不确定性就越低。
4. $H(X)=0$等价于$p_i = 0 and p_j = 1, \forall i \neq j$。

## 算法过程
决策树的构造过程秉承着分而治之（divide-and-conquer）的原则，最初版本的决策树算法还是比较好容易实现的。

算法流程：
1. 遍历特征集合，找到能够使得某一个衡量标准达到最大（这里可以将其做成一个虚函数，然后面继承的子类去实现具体的衡量方法，像信息增益，基尼系数等）。
2. 按照选定的特征将样本划分样本，迭代调用构造子树的函数，接着返回该节点的信息，如果是决策结点返回结点信息，如果是叶子结点返回对应的label值。

注意点：
* 多子类的处理方法，非二叉树类型
* 决策树预测时也要迭代地从根节点访问到叶节点
* 判断叶节点的类型时如果仍有多个类型的数据在，那么就采用投票的方法。
* 所有的样本最后的归属地都是某一个节点中，决策结点中不包含样本。
* 知道如何计算impurity，计算impurity是有关y的事情，而在决策节点进行选择特征时测涉及到X每列的取值。


### 划分子树
在遍历特征选择最合适划分样本的特征时，有几个划分选择，分别是信息增益、增益率和基尼系数。

#### 信息增益 （Information Gain）
信息熵是度量一个样本集合不纯度最常用的一个指标，假设当前样本集合$D$中第$k$类样本所占比例为$p_k$（$k=1, 2, \dots, |y|$），则当前样本集合$D$的信息熵定义为：

$$Ent(D) = - \Sigma_{k=1}^{|y|} p_k \log p_k$$

信息熵$Ent(D)$越大，说明不纯度越高，或者说不确定性越大。

如果此时我们用有$v$个可能取值${a^1, a^2, \dots, a^V}$的离散属性$a$来划分当前样本集合$D$，能够得到$V$个分支子树，每个分支子树$D^v$都包含各自在属性$a$上的同一取值$a^v$（这里只是$X$方面的一致，label的取值是不一定一样的）。根据上面信息熵的定义，我们可以计算出划分之后的每一个子树的信息熵$Ent(D^v)$，然后通过对子树信息熵的加权求和得到划分后所有子树的整体信息熵（这个其实是条件熵）：

$$Ent(a\_braches)=\Sigma_{v=1}^V {{|D^v|}\over {|D|}}Ent(D^v)$$

然后我们比较划分前后的信息熵变化：

$$Gain(D, a)=Ent(D)-Ent(a\_branches)=Ent(D)-\Sigma_{v=1}^V {{|D^v|}\over {|D|}}Ent(D^v)$$

我们将划分前后的信息熵变化定义为信息增益（Information Gain），信息增益越大，说明当前的划分越能够降低信息的不纯度，亦即越能够降低系统的不确定性，如此得到的决策树决策效果越好。于是我们就选择能够使得信息增益最大的属性进行划分，著名的ID3就是根据信息增益来进行划分属性的。

**Drawbacks**
信息增益有一个比较严重的缺点是会非常倾向取值多的那一类特征，因为属性取值越多，$Ent(a\_braches)$的结果明显就会越小。当特征中有类似ID的属性，这个问题就比较棘手！

#### 增益率


#### 基尼系数

### 递归退出条件
决策树采用递归方式生成，在递归时有如下几个递归结束的条件：
1. 当前结点包含的样本全都属于同一个类别，无法继续划分；（同一类了还分啥？）
2. 当前的属性集为空，没有属性用来划分了；或者即使有属性可划分但是所有样本在所有属性上的衡量结果相同，无法进行划分了；
3. 除了属性为空，当前结点包含的样本结合为空时，无法进一步划分。

注意递归的时候，写完子递归还要确保本层递归的返回！

### 代码实现
```python
# Check if expansion of y is needed
        if len(np.shape(y)) == 1:
            y = np.expand_dims(y, axis=1)
```
为什么要拓展y的维度？

设定min_samples_split（最小的划分样本），即如果还剩下大于等于这个数字的样本数，那么继续划分。类似的，设定max_depth，防止决策树划分太深；还要设定min_impurity（最小的不纯度），要超过这个最小不纯度，就继续划分。

#### 代码实现问题
1、按照开源的代码实现的结果是，准确率一直不太稳定，偏差太大。

#### 预测的做法
也是要遍历地从根节点一直往下延伸到具体的叶节点上，返回的值是叶节点的value。


## 决策树应用注意
### 连续值处理


### 缺失值处理


## 实际应用
### ID3
### C4.5
### CART


## 回归树




## 总结
代码实现算法的时候，首先第一步需要将每一步要做什么细节的用伪代码的形式写下来才能比较好的进行Code。
1. 模型的起点，如何更新，结束条件有哪些
2. 如何预测
    * 如何计算
3. 返回什么东西？？

## References
1. [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
2. [Page 11 - BASIC CONCEPTS IN INFORMATION THEORY](http://www-public.imtbs-tsp.eu/~uro/cours-pdf/poly.pdf)
3. [Intuitive explanation of entropy](https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy)
