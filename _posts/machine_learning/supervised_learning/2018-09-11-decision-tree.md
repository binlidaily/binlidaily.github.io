---
layout: post
title: Decision Tree
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　决策树是一个树结构（二叉树或非二叉树），每一个非叶子结点表示一个对特征属性的测试，每一个分支代表这个特征属性在某个值域上的输出，每个叶节点存放一个特定的类别。使用决策树进行决策的过程就是从根节点开始，测试待分类样本中对应的特征属性，并按照其值选择输出分支继续往下，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

## 1. 决策树模型
　　决策树模型的构建主要分三大部分，首先我们位于根节点处，需要做的是**特征选择**，选择出一个特征属性来创造分支，特征选择的依据一般会涉及几类指标（如信息增益，增益率和基尼系数），据此指标选择合适的特征，进而划分子树，迭代此过程完成**决策树生成**，然后对生成的决策树进行**剪枝**以提高其泛化能力。决策树的构造过程秉承着分而治之（divide-and-conquer）的原则，决定好划分属性后，将原树分成多个子树，再子树中继续同样的过程，直到遇到结束条件，即可生成决策树。

### 1.1 特征选择
　　首先我们要解决的问题是如何选择哪一个特征的哪一个属性值在一个叶子节点处划分，常规做法就是遍历所有特征以特征中的所有取值，衡量按照这个特征的这个属性值划分子树是否是最佳的，衡量是否最佳的指标有三个：分别是信息增益、增益率和基尼系数。

### 1.1.1 信息增益 （Information Gain）
　　[信息熵](https://binlidaily.github.io/2018-10-23-information-theory/)是度量一个样本集合不纯度最常用的一个指标，假设当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$（$k=1, 2, \dots, |\mathcal { Y }|$），则当前样本集合 $D$ 的信息熵定义为：

$$Ent(D) = - \sum_{k=1}^{|\mathcal { Y }|} p_k \log p_k$$

信息熵 $Ent(D)$ 越大，说明不纯度越高，或者说不确定性越大。

　　如果此时我们用有 $V$ 个可能取值 ${a^1, a^2, \dots, a^V}$ 的离散属性 $a$ 来划分当前样本集合 $D$，能够得到 $V$ 个分支子树，每个分支子树 $D^v$ 都包含各自在属性 $a$ 上的同一取值 $a^v$（这里只是在 $X$ 方面的一致，label 的取值是不一定一样的）。根据上面信息熵的定义，我们可以计算出划分之后的每一个子树的信息熵 $Ent(D^v)$，然后通过对子树信息熵的加权求和得到划分后所有子树的整体信息熵（即所谓的条件熵）：

$$Ent(D, a)=\sum_{v=1}^V { {|D^v|}\over {|D|}}Ent(D^v)$$

　　然后我们比较划分前后的信息熵变化：

$$Gain(D, a)=Ent(D)-Ent(D, a)=Ent(D)-\sum_{v=1}^V { {|D^v|}\over {|D|}}Ent(D^v)$$

　　我们将划分前后的信息熵变化定义为信息增益 (Information Gain)，信息增益越大，说明当前的划分越能够降低信息的不纯度，亦即越能够降低系统的不确定性，如此得到的决策树决策效果越好。于是我们就选择能够使得信息增益最大的属性进行划分，著名的 ID3 就是根据信息增益来进行划分属性的。

信息增益的**优缺点**:
* 优点
    * 信息增益看起来比较直观而且计算量相对较少
* 缺点
    * 信息增益有一个比较严重的缺点是会非常**倾向取值多的那一类特征**
        * 原因：因为属性取值越多，$Ent(D, a)$ 的结果明显就会越小。当特征中有类似 ID 的属性，这个问题就比较棘手！
        * 举例：如果一个特征是消费者的信用卡号，在 IG 选择特征划分时肯定会选这个特征，因为此时 $Ent(D, a)$ 已经小到零了，即分完的结果纯度非常高（每一个分支只有一个样本），信息增益达到最大。

### 1.1.2 增益率（Gain Ratio）
　　为了克服信息增益的劣势，后人就开始想是不是有这样一种抑制朝着多离散值的属性方向划分呢？我们可以设计一种衡量标准，当我们选怎离散值较多的属性的时候信息增益虽然最多，但是我们设计的这个衡量标准的值也会相应较大，这两个在一定程度上抵消。首先衡量一下划分之后子树划分信息量 (Split Information，也称 Intrinsic Value) 的概念：

$$\text{Split_info}(D, a)=-\sum_{v=1}^{V}{ {|D^v|}\over{|D|}}\log_2{ {|D^v|}\over{|D|}}$$

　　划分信息量可以衡量划分后数据的广度和均匀程度，划分后每个子树的数目越均匀，那么划分信息量就越大。因为信息增益在划分时会偏向类似 ID 型特征的趋势，所以我们如果取划分信息量的倒数，亦即将信息增益除以划分信息量就能起到抑制效果，这就是所谓的增益率 (Gain Ratio)：

$$\text{Gain_ratio}(D, a)={\text{Gain}(D, a)\over{\text{Split_info}(D, a)}}$$

　　那么，现在划分属性的依据是最大化增益率了，这也是 C4.5 采用的方式。

　　 信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。

　　 要想增益率尽量大，那么划分信息量就得尽量小，亦即尽量选择分裂子树更少的方式。例如，划分方式 A 是将数据均匀划分成 $n$ 分，可计算得到划分信息量为 $\log_2 n$ ；划分方式B是将数据均匀分成两个部分，则划分信息量为 $1$；那么划分信息量会倾向选择能得到较小值的方案 $B$，即子树数目小的。我们也可以通过对照对数函数在 $(0,1)$ 区间的趋势图，很明显距离 $1$ 的部分（即平均子树样本数较多的部分，也就是说划分子树较少时）函数结果较小，也就是更倾向选划分子树数目较少的方式！所以，增益率的**弱点**是有一点过分强调了找取值数目较少的属性。

增益率的优缺点：
* 优点
    * 解决了信息增益偏向类别多的特征做划分的问题
* 缺点
    * 信息增益率偏向取值比较少的特征
        * 为了避免仅仅因为划分信息量非常小就选择一个特征来进行划分，C4.5在根据增益率划分属性的时候一般采用一种启发式方法，即先计算每个属性的信息增益，然后仅对增益高过平均值的属性应用增益率进行测试。


### 1.1.3 基尼系数（Gini Index）
　　之前两个衡量不纯度（不确定性）的标准是依赖信息熵，但是信息熵的计算有 $\log$ 运算，相对来说计算量较大，那么有没有一个没有那么复杂计算的衡量不纯度的方式呢？这就促生了基尼系数 (Gini Index)。我们说一个东西的纯度 (Purity) 高，一般指的是其中尽量只包含一种物质，例如纯金的纯度；相对的不纯度 (Impurity) 高说明里面的杂质多，在类别中的说法就是不同类的数目越多。那么，我们假设当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$（$k=1, 2, \dots, \vert\mathcal { Y }\vert$），那么我们连续抽取两个样本，这两个样本不属同一个类的概率如下：

$$Gini(D)=\sum_{k=1}^{|\mathcal { Y }|}p_k(1-p_k)=\sum_{k=1}^{|\mathcal { Y }|}p_k-\sum_{k=1}^{|\mathcal { Y }|}p_k^2=1-\sum_{k=1}^{|\mathcal { Y }|}p_k^2$$

　　这就是基尼系数 (Gini Index, Gini Impurity)，比较好理解的是，连续选取的两个样本不属于同一个类的概率越大，那么此批样本的不纯度就越高。所以，我们在划分属性的时候，选择能够使得基尼系数**最小**的属性。[这里](https://blog.csdn.net/lanchunhui/article/details/65441891)用数学推导说明了基尼系数和熵的关系。

　　如果样本集合 $D$ 根据属性 $a$ 是否取某一可能值 $c$ 被划分成 $D_1$ 和 $D_2$ 两部分，即：

$$D_1={(x, y)\in D|A(x)=a}, D_2=D-D_1$$

　　则在特征 $a$ 的条件下，集合 $D$ 的基尼系数定义为：

$$\text{Gini}(D,a)={|D_1|\over{|D|}}\text{Gini}(D_1)+{|D_2|\over{|D|}}\text{Gini}(D_2)$$


基尼系数优缺点:
* 优点
    * 基尼系数在计算上会快一些，因为没有求 $\log$ 运算。

### 属性划分递归结束条件
　　知道如何选择合适的划分特征来划分树结构后，我们便可以一直递归地调用这个选择划分属性和划分树的过程，以此来生成一棵决策树。但是我们得明确这个递归过程的结束条件，以下做了个整理：
1. 当前结点包含的样本全都属于同一个类别，无法继续划分；（同一类了还分啥？）
2. 当前的属性集为空，没有属性用来划分了；或者即使有属性可划分但是所有样本在所有属性上的衡量结果相同，无法进行划分了；
3. 除了属性为空，当前结点包含的样本结合为空时，无法进一步划分。

#### 代码实现
```python
# Check if expansion of y is needed
        if len(np.shape(y)) == 1:
            y = np.expand_dims(y, axis=1)
```
为什么要拓展y的维度？因为有的 y 是 onehot 编码过的。

设定 min_samples_split（最小的划分样本），即如果还剩下大于等于这个数字的样本数，那么继续划分。类似的，设定max_depth，防止决策树划分太深；还要设定min_impurity（最小的不纯度），要超过这个最小不纯度，就继续划分。

**代码实现问题**
1、按照开源的代码实现的结果是，准确率一直不太稳定，偏差太大。

#### 预测的做法
　　也是要遍历地从根节点一直往下延伸到具体的叶节点上，返回的值是叶节点的value。

⁉️ 预测样本出现了，有的类在训练数据集的特征中不存在的情况要怎么处理?
* 那就预测不出来啊

## 决策树应用注意
#### 剪枝处理（Pruning）
决策树比较大的问题是容易过拟合，为了在一定程度上抑制这样的现象就提出了剪枝（Pruning）的操作，剪枝分为预剪枝和后剪枝两种。值得注意的是，不管是预剪枝还是后剪枝，都是在考虑**泛化性能**的提升与否，而不是信息熵。

⁉️ 这里的泛化性能怎么验证？在一定的验证集上做验证？
⁉️ 如果在损失函数那儿加上正则化呢？

**预剪枝**

在决策树的生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来*泛化性能*上的提升，则停止划分并将当前结点标记为叶节点。

**后剪枝**

先从训练集生成完整的决策树，然后自底向上地对每一个非叶子结点进行考察，如果将该结点的子树改成叶结点能够提高决策树的性能，那么就将当前结点的子树改成对应的叶结点。

### 连续值处理
对于离散的标称型数据来说，特征值是离散有限的，但是对于连续特征来说，其连续值就不一定是有限的了，于是我们需要进行连续属性离散化。最简单的而且也正是C4.5中采用的方法叫做二分法（bi-partition）。

假设给定样本集 $D$ 和连续的属性 $a$，假设 $a$ 在样本 $D$ 上有 $n$ 个不同的取值，将这 $n$ 个不同的值按照从小到大的顺序排好，记为 ${a^1, a^2, \dots, a^n}$。那么，基于一个划分点 $t$，我们可以将样本分成 $D^-$ 和 $D^+$ 的两个部分。在相邻的两个取值 ${a^i, a^{i+1}}$ 上，我们知道 $t$ 无论取区间 $[a^i, a^{i+1})$ 的任何一个值，划分结果都不变，因为虽然属性是连续型属性，但是其在训练样本中所有的取值目前是已知的了。可能在测试集中会有取值有不一样的情况，但是也是始终逃不出大小关系的，所以不用担心像标称型属性那样测试集出现训练集中没有的属性取值的苦恼了。

对于连续属性 $a$，我们采取两个相邻取值的中位数 ${a^i+a^{i+1}}\over 2$ 来做一个划分点，于是可以得到 $n-1$ 个候选划分点集合：

$$T_a = \{ { {a^i+a^{i+1}}\over 2} | 1 \leq i \leq n-1 \} $$

然后遍历 $T_a$ 找到能够使得在评判标准下得到最好效果的那个划分点。

$$
\begin{align}
Gain(D, a) &= max_{t \in T_a} Gain(D, a, t) \\
&=max_{t \in T_a} Ent(D) - \sum_{\lambda \in\{-,+\}}{|D_t^{\lambda}|\over{D}}Ent(D_t^{\lambda})
\end{align}
$$

### 缺失值处理
对于缺失值处理来说有两个问题需要考虑:
1. 如何在划分属性值缺失的情况下进行划分属性选择？（比如“色泽”这个属性作为划分属性时，有的样本在其上面取值缺失时，要该如何计算信息增益？）
2. 已经选好划分属性，但某样本在该属性上值缺失，该如何对样本进行划分？（该将这个样本划分到哪个类？）

首先，我们要明确一点，缺失值是针对样本的某个属性值来说的。当然也可简单粗暴的进行处理：

* 舍弃缺失值
* 填充缺失值

#### 问题一：有属性值缺失，如何选哪个做划分属性？
这个是在选择划分属性时需要面临的问题，我们要判断是否将这个属性作为划分属性，按照之前的流程我们需要计算其信息增益，但是有的样本在这个属性上取值是缺失的，那么我在计算信息增益时该如何处理这些样本的缺失值呢？

像 CART 算法就采用一种基于无缺失值样本来计算信息增益的方式，首先给定训练集 $D$ 和属性 $a$，然后定义在属性 $a$ 上无缺失值的样本子集为 $\tilde { D }$。$\tilde { D } ^ { v }$ 表示 $\tilde { D }$ 中在属性 $a$ 上取值为 $a ^ { v }$ 的样本子集；$\tilde { D } _ { k }$ 表示 $\tilde { D }$ 中属于第 $k$ 类（$ k=1, 2, \dots,\mid\mathcal{Y} \mid $）的样本子集。

我们需要知道信息增益，那么将其分成两个部分，第一个是对已知值（即无缺失值的部分）所带来的信息增益；第二部分是对未知值（即缺失值）带来的信息增益，通过加权平均的方式综合起来。而一个明显的事实是，对于未知值而言我们的能获得的信息量是零，于是我们有下面的式子：

$$
\begin{aligned} \operatorname { gain } ( D, a ) & =  \text { probability of } a \text { is known } \times  \operatorname { gain } (\tilde { D }, a) \\ & + \text { probability of } a \text { is not known } \times 0 \\ 
& = \rho \times  \operatorname { gain } (\tilde { D }, a)  \\
& = \rho \times  \left(\operatorname { Ent } ( \tilde { D } ) - \operatorname { Ent } ( \tilde { D }, a) \right)\\
& = \rho \times \left( \operatorname { Ent } ( \tilde { D } ) - \sum _ { v = 1 } ^ { V } \tilde { r } _ { v } \operatorname { Ent } \left( \tilde { D } ^ { v } \right) \right)
\end{aligned}
$$

假定我们为每个样本 $x$ 赋予一个权重 $w_x$，那么上面的式子中 $\rho$ 表示无缺失值样本占所有样本的比例：

$$\rho = \frac { \sum _ { x \in \tilde { D } } w _ { x } } { \sum _ { x \in D } w _ { x } }$$

$\tilde { r } _ { v }$ 表示无缺失值的样本在属性 $a$ 上取值为 $a^v$ 的样本占无缺失值样本子集的比例：

$$
\tilde { r } _ { v } = \frac { \sum _ { x \in \tilde { D } ^ { v } } w _ { x } } { \sum _ { x \in \tilde { D } } w _ { x } } \quad ( 1 \leq v \leq V )
$$

可以计算出无缺失值样本子集的信息熵为:

$$
\operatorname { Ent } ( \tilde { D } ) = - \sum _ { k = 1 } ^ { | \mathcal { Y | } } \tilde { p } _ { k } \log _ { 2 } \tilde { p } _ { k }
$$

其中 $\tilde { p } _ { k }$ 表示无缺失值样本中第 $k$ 类样本所占的百分比：

$$
\tilde { p } _ { k } = \frac { \sum _ { x \in \tilde { D } _ { k } } w _ { x } } { \sum _ { x \in \overline { D } } w _ { x } } ( 1 \leq k \leq | \mathcal { Y } | )
$$

所以，当样本在划分属性上缺失时，我们就用上面的 $\operatorname { gain } ( D, a )$ 来计算信息增益，进而选择对应的划分属性。值得注意的是，这里在处理缺失值的时候假设了每个样本都有一个权值，在学习开始初期这些权重初始化为 1。设置每个样本都有权重的好处或许是使得算法更灵活一些，对 imbalance 问题处理得更好，但是一个问题是要如何去更新每个参数的权重？⁉️

#### 问题二：样本值划分属性上缺失，如何划分该样本？
若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子结点，且样本的权值在属性值为 $a^v$ 对应的子结点上调整为 $\tilde{r} \cdot w_x$。我们知道 $\sum _ { k = 1 } ^ { | \mathcal { Y } | } \tilde { p } _ { k } = 1$，$\sum _ { v = 1 } ^ { V } \tilde { r } _ { v } = 1$，可见这里是将原来这单个样本的权值按照不同的概率分拆成了多份划分到对应的子结点中去了。这样很好理解，因为没有足够多的信息，我们不知道要如何划分该样本，每个子结点都有可能接收这个样本，但是子结点接收的概率是不同的，那么我们就将该单个样本的信息按照相应的概率分给各个子结点。


#### 另外的问题：模型训练好，但测试时出现缺失值
这时候，就不能按比例分配了，因为你必须给该样本一个确定的label，而不是薛定谔的label。这时候根据投票来确定，或者填充缺失值。

## 实际应用
### ID3
熵表示的是数据中包含的信息量的大小。熵越小，数据的纯度越高，亦即数据越趋于一致，这是我们希望划分自后每个子结点的样子。信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性a来进行划分所获得的 “纯度提升” 越大 。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。

ID3 优点：
* 理论清晰，方法简单，学习能力较强

缺点：
* 只能处理分类属性的数据，不能处理连续的数据
* 划分过程会由于子集规模过小而造成统计特征不充分而停止
* ID3 采用信息增益来划分子树，所以导致倾向于选择取值较多的属性

### C4.5
C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵。选择信息增益比最大的作为最优特征。C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益比最大的分裂点作为该属性的分裂点。

### CART
只是二分类能保证所有类别能在叶子结点上体现？继续细分是可以的！

分类与回归树（Classification And Regression Tree，CART）是目前应用广泛的决策树学习方法。CART 是给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的**条件概率分布**的学习方法。CART 假设决策树是二叉树，其决策结点特征的取值为“是”或者“否”，其左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于迭代的二分每个特征，将输入空间划分成有限个单元，并在这些单元上确定预测的概率分布。⁉️ 为什么说会涉及到概率分布呢？

⁉️ 为什么想到一定要用二叉树？

CART 算法一般分两个步骤:
1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大（怎么保证？）。
2. 决策树剪枝：用**验证数据集**对已生成的决策树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

CART 的生成就是迭代地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。接下来我们从回归树和分类树分别介绍 CART。

#### 回归树的生成
回归树的生成既然要迭代地构建二叉树，那么很直接地就有两个问题摆在面前：
1. 划分点怎么找？
2. 因为划分后为二叉树，原本可能有很多取值的结果要怎么输出？

以一般机器学习模型的训练目标为参考，我们在划分点的寻找上也想设定一种有着类似训练目标的划分点寻找方式，于是就有前辈采用了**均方误差最小**的原则来寻找合适的划分点。那么对应的回归树度量目标就是：对于任意的划分属性 $a$（假设划分属性 $a$ 是训练样本的第 $j$ 个变量，记作 $x_j$），其对应有任意的划分点 $s$ 将数据集分成 $R_1$ 和 $R_2$ 两个部分，我们要找这样的划分点，其能够使得 $R_1$ 和 $R_2$ 两个集合各自的均方差最小，同时能够使得这两个均方差的加和最小。(⁉️外层最小时会不会让内层最小调整？在设计该算法的时候有什么考虑？)

接下来对具体到公式上，首先假设 $X$ 和 $Y$ 分别为输入和输出变量，并且 $Y$ 为连续变量，给定数据集：

$$
D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( x _ { N } , y _ { N } \right) \right\}
$$

其中 $x _ { i } = \left( x _ { i } ^ { ( 1 ) } , x _ { i } ^ { ( 2 ) } , \ldots , x _ { i } ^ { ( n ) } \right)$ 为输入实例（特征向量），$i=1,2,\dots,N$，$n$ 为特征个数，$N$ 为样本总量。

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已知将输入空间分成了 $M$ 个单元 $R_1, R_2, \dots, R_M$（这个 $M$ 的取值得看最后回归树生成到什么程度对应的划分空间），并且每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可以表示为：

$$
f ( x ) = \sum _ { m = 1 } ^ { M } c _ { m } I \left( x \in R _ { m } \right)
$$

其中 $I$ 为指示函数，$I =  \{ \begin{array} { l l } { 1 } & { i f \left( x \in R _ { m  } \right) } \\ { 0 } & { i f \left( x \notin R _ { m } \right) } \end{array} $

可以用李宏毅视频上的截图来介绍！⁉️

当输入空间的划分确定时，我们用平方误差 $\sum _ { x _ { i } \in R _ { m } } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$ 来表示回归树对于训练数据的预测误差，用平方误差最小化原则来求解每个单元上的最优输出值。通过以下的证明，我们可以知道单元 $R_m$ 上的 $c_m$ 的最优值是 $R_m$ 上所有输入实例 $x_i$ 对应的输出的 $y_i$ 的均值：

$$
\hat { c } _ { m } = \operatorname { ave } \left( y _ { i } | x _ { i } \in R _ { m } \right)
$$

给定一个随机的数列 $\{ x _ { 1 } , x _ { 2 } , \dots , x _ { n } \}$，假设该空间中最优的输出值为 $a$，则根据最小平方误差准则，我们能得到有关 $a$ 的函数如下：
$$
F ( a ) = \left( x _ { 1 } - a \right) ^ { 2 } + \left( x _ { 2 } - a \right) ^ { 2 } + \ldots + \left( x _ { n } - a \right) ^ { 2 }
$$

我们考察其单调性：
$$
F ^ { \prime } ( a ) = - 2 \left( x _ { 1 } - a \right) - 2 \left( x _ { 2 } - a \right) + \ldots - 2 \left( x _ { n } - a \right) = 2 n a - 2 \sum _ { i = 1 } ^ { n } x _ { i }
$$

$$F ^ { \prime \prime } ( a ) = 2n > 0$$

二阶导为正数，所以该函数为严格凸函数，那么令 $F ^ { \prime } ( a ) = 0$，得到 $a = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }$，即最小值点为：

$$
\hat { a } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } x _ { i }
$$

那么接下来就是要讨论该如何划分了，选择第 $j$ 个变量 $x^{(j)}$ 和它的取值 $s$，作为切分变量（Splitting variable）和切分点（Splitting point），则由此划分的两个区域为：

$$R _ { 1 } ( j , s ) = \{ x | x ^ { ( j ) } \leq s \} \text{ 和 } R _ { 2 } ( j , s ) = \{ x | x ^ { ( j ) } > s \}$$

问题就转化成如何求解最优划分变量 $j$ 和 $s$，具体的有以下的方式：

$$
\min _ { j , s } \left[ \min _ { c _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - c _ { 1 } \right) ^ { 2 } + \min _ { c _ { 2 } } \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - c _ { 2 } \right) ^ { 2 } \right]
$$

对于固定输入变量 $j$，可以利用求均值的方式在内循环的 min 中找到这两个区域中的最优切分点 $s$：
$$
\hat { c _ { 1 } } = \frac { 1 } { N _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } y _ { i } \text{ 和 } \hat { c _ { 2 } } = \frac { 1 } { N _ { 2 } } \sum _ { x _ { j } \in R _ { 3 } ( j , s ) } y _ { i }
$$

其对应的求解最优划分变量的方式可以写成:

$$
\min _ { j , s } \left[ \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - \hat { c _ { 1 } } \right) ^ { 2 } + \sum _ { x _ { i } \in R _ { 2 } ( j , s ) } \left( y _ { i } - \hat { c _ { 2 } } \right) ^ { 2 } \right]
$$

接着据此遍历所有的输入变量 $j$ 从而在外循环的 min 中找到最优的切分变量，如此构成了一个划分对 $(j,s)$。据此可以将输入空间划分为两个区域，接着循环执行这个划分流程，直到满足停止条件。

#### 分类树的生成
与 ID3 和 C4.5 不同的是，CART 划分的结果是二叉树。所以在生成分类树时只要对每一个属性 $a$ 的每一个取值 $a^v$ 做一个“当前属性值是否等于 $a^v$”的判断，就可以分成将样本集 $D$ 分成 $D_1$ 和 $D_2$ 两个部分。然后在选取最优划分点时，找到能够使得基尼系数最小化的划分对，即对应的属性 $a$ （相当于回归树里面的 $j$）和该属性的取值 $a^v$（相当于回归树里面的 s）。然后对划分的两个子结点迭代的进行该流程，直到满足停止条件。

#### CART 剪枝
CART 剪枝算法是想从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单）。完整的树我们以 $T_0$ 表示，以此为基础，从决策树底端开始，考察一个内部结点，减去一个子树形成 $T_1$；接着在 $T_1$ 的基础上技术减去一个子树形成 $T_2$，循环往复直到剪枝只剩下根节点 $T_n$。接着利用独立的验证数据集对 $\{T_0, T_1, \dots, T_n \}$ 这 $n+1$ 个子树进行测试，从中选择最优的子树。

那么接下来的问题是，如何选择一个子树（内部结点）进行剪枝？

CART 剪枝时定义了计算子树的损失函数：

$$
C _ { \alpha } ( T ) = C ( T ) + \alpha | T |
$$

其中 $T$ 表示任意的子树，$C ( T )$ 为对训练数据的预测误差（比如基尼系数等指标），$\mid\mathcal {T} \mid $ 表示子树的叶节点个数。$\alpha$ 衡量和调节训练数据的拟合程度与模型复杂度的参数，$C _ { \alpha } ( T )$ 表示参数为 $\alpha$ 的子树 $T$ 的整体损失。

具体地，从整体树 $T_0$ 开始剪枝，对 $T_0$ 的任意内部结点 $t$，以 $t$ 为单结点树的损失函数是：

$$
C _ { \alpha } ( t ) = C ( t ) + \alpha
$$

以 $t$ 为根节点的待剪枝的子树的损失函数为:

$$
C _ { \alpha } \left( T _ { t } \right) = C \left( T _ { t } \right) + \alpha \left| T _ { t } \right|
$$

当 $\alpha=0$ 及 $\alpha$ 充分小时，有如下不等式成立：

$$
C _ { \alpha } \left( T _ { t } \right) < C _ { \alpha } ( t )
$$

可以想象在单个结点下，其纯度肯定更高，那么其损失函数会对应最小。

当 $\alpha$ 慢慢增大时，在某一个 $\alpha$ 上会有

$$
C _ { \alpha } \left( T _ { t } \right) = C _ { \alpha } ( t )
$$

当 $\alpha$ 再增大时，上面的不等式就会反向。也就是说当 $\alpha = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }$
，$T_t$ 与 $t$ 具有相同的损失函数，而 $t$ 的结点少，因此 $t$ 比 $T_t$ 更可取，对 $T_t$ 进行剪枝。剪枝最后保留 $t$ 这个内部结点，不过变成了叶节点？⁉️

为此，对于 $T_0$ 中每一个内部结点，我们定义

$$
g ( t ) = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }
$$

$g(t)$ 表示剪枝剪枝的阈值。我们可以这样想，当 $\alpha$ 逐渐增大时，某些内部结点有 $\alpha > g(t)$，此时就应该剪掉这些子树，因为这样剪枝的话，能够使得整体损失函数减小。当一个一个剪枝后，$\alpha$ 就变成了一个分段区间。

## 总结
代码实现算法的时候，首先第一步需要将每一步要做什么细节的用伪代码的形式写下来才能比较好的进行 Code。
1. 模型的起点，如何更新，结束条件有哪些
2. 如何预测
    * 如何计算
3. 返回什么东西？？

⁉️ 自己实现的 Decision Tree 为什么不稳定？
* 可以对照着参考的算法，一步一步调试找问题……

零星待整理的几点：
1. 决策树的主要优势在于数据形式非常容易理解。
2. 决策树是后续很多重要的集成方法的基础。
3. 树构造算法只适用于标称型数据，数值型数据必须进行离散化。

**优点**：
1. 计算复杂度不高
2. 输出结果易于理解
3. 对中间值的缺失不敏感
4. 可以处理不相关特征数据

**缺点**：
1. 可能会产生过度匹配问题，即 Overfitting。在实践时，为了防止过拟合可以限制每个叶子节点要有多少个样本。

首先我们看看决策树算法的优点：

　　　　1）简单直观，生成的决策树很直观。

　　　　2）基本不需要预处理，不需要提前归一化，处理缺失值。

　　　　3）使用决策树预测的代价是O(log2m)。 m为样本数。

　　　　4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。

　　　　5）可以处理多维度输出的分类问题。

　　　　6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释

　　　　7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。

　　　　8） 对于异常点的容错能力好，健壮性高。

　　　　我们再看看决策树算法的缺点:

　　　　1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。

　　　　2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。

　　　　3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。

　　　　4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。

　　　　5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。
## References
1. [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
2. [Page 11 - BASIC CONCEPTS IN INFORMATION THEORY](http://www-public.imtbs-tsp.eu/~uro/cours-pdf/poly.pdf)
3. [Intuitive explanation of entropy](https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy)
4. [决策树算法原理](http://www.cnblogs.com/pinard/p/6050306.html)
5. [A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
6. [决策树—回归](https://zhuanlan.zhihu.com/p/42505644)
7. [SKlearn Decision Tree](https://scikit-learn.org/stable/modules/tree.html#classification)
8. [efficient-decision-tree-notes](https://github.com/wepe/efficient-decision-tree-notes)
