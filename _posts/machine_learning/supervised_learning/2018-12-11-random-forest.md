---
layout: post
title: Random Forest
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

随机森林（Random Forest，RF）基于 Bagging 的想法，做了一定的改进，其基学习器是 CART，然而在选择划分特征的时候，RF 采用了随机的策略。不像之前的决策树在当前结点上遍历所有 $n$ 个样本特征中找到最优的划分属性，RF 会固定一个特征子集大小 $n_{sub}$，在当前结点样本的所有特征中随机选择 $n_{sub}$ 个特征，从其中选出最优的一个划分特征来进行特征划分。如此能够提高模型的泛化能力。显然，当 $n_{sub}=n$ 时，RF 的 CART 跟传统的保持一致。

一般来说，$n_{sub}=n$ 越小，模型就越健壮，当然此时对数据的拟合程度就会变差。即 $n_{sub}=n$ 越小，variance 越小，bias 越大。

## Random Forest 算法流程
假设输入为样本集为

$$
D = \left\{ \left( x , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots \left( x _ { m } , y _ { m } \right) \right\}
$$

弱学习器记作 $G(x)$，迭代次数为 $T$：

1）对于 $t = 1,2 \ldots , T$：

* a）对训练集进行第 $t$ 次随机采样，共采集 $m$ 次，得到包含 $m$ 个样本的采样集 $D_t$

* b）用采样集 $D_t$ 训练第 $t$ 个弱学习器 $G_t(x)$，在训练决策树划分结点的时候，在结点上所有的样本特征中随机选出一部分样本特征，在这些随机选择出来的样本特征中找出最优的划分特征来划分当前结点

2) 如果是分类算法预测，则 $T$ 个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$ 个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

## 总结
由于 Bagging 的思想可以分布式地实现若干个弱分类器的学习，Random Forest 的一大优势是能够高度并行化，在大数据时可大有作为。

Random Forest 的主要优点总结如下：
1. 训练可以高度并行化，在大数据时代的大样本训练环境下极具优势

3. 由于可以随机选择决策树划分结点的特征，这样在样本特征维度较高时，仍能保持高效的训练
4. 在训练后，可以给出各个特征对于输出结果的重要性
5. 由于采用了随机采样，训练出的模型方差小，泛化能力强
6. 相对于 Boosting 的 Adaboost 和 GBDT，RF 比较好实现
7. 对部分特征确实不敏感
8. 实践中，可以对输入数据不用做太多处理（能够处理 binary features, categorical features, numerical features）

Random Forest 的主要缺点如下：
1. 在某些噪声比较大的样本集上，RF 模型容易陷入过拟合

3. 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响（偏向取值较多的特征，随机选的时候选中的概率比较高！），所以随机森林在这种数据上产出的属性权值是不可信的
4. 训练好的模型比较大，预测时较慢
5. 相比于单一决策树，它的随机性让我们难以对模型进行解释

## References
1. [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)