---
layout: post
title: Linear Regression
tags: [Machine Learning]
comments: true
published: true
typora-root-url: ../../../../binlidaily.github.io
typora-copy-images-to: ../../../img/media
---

çº¿æ€§å›å½’ä½œä¸ºä¸€ç§éå¸¸å¸¸è§çš„ç¦»æ•£å€¼é¢„æµ‹æ¨¡å‹ï¼Œå´æ˜¯éå¸¸åŸºç¡€ä¸”é‡è¦çš„ç®—æ³•ï¼Œåç»­çš„å¾ˆå¤šæœºå™¨å­¦ä¹ çš„ç®—æ³•éƒ½æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šåšçš„ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ã€‚

## çº¿æ€§å›å½’ç®—æ³•
æœ€åŸºæœ¬çš„çº¿æ€§å›å½’ç®—æ³•æƒ³æ³•æ¯”è¾ƒç®€å•ï¼Œå°±æ˜¯æ‹Ÿåˆä¸€ä¸ªå‡½æ•°ï¼Œé¢„æµ‹å¯¹åº”çš„å€¼ã€‚æˆ‘ä»¬å°†æ ·æœ¬ plot å‡ºæ¥å‘ç°ï¼Œæ•°æ®å¯èƒ½å‘ˆç°æ­£ç›¸å…³çš„åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªæœ€ç®€å•çš„çº¿æ€§æ¨¡å‹æ¥æ‹Ÿåˆã€‚

é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•ä»ä¸€å †æ•°æ®é‡Œæ±‚å‡ºå›å½’æ–¹ç¨‹å‘¢ï¼Œå³æ‰¾åˆ°å›å½’ç³»æ•°å‘¢ï¼Ÿç›´è§‚çš„æƒ³æ³•æ˜¯æˆ‘ä»¬èƒ½å¤Ÿæ‹Ÿåˆå‡ºä¸€æ¡ç›´çº¿ï¼Œä½¿å¾—æ‰€æœ‰ç‚¹åœ¨å…¶ $x$ çš„å–å€¼ä¸‹ï¼Œå¯¹åº”çš„ $y$ å€¼è·Ÿæ‹Ÿåˆå‡ºæ¥çš„ç›´çº¿åœ¨ç›¸åŒ $x$ ä¸‹çš„å–å€¼å°½å¯èƒ½æ¥è¿‘ã€‚è¿™é‡Œçš„ç‚¹åˆ°çº¿æ®µçš„è·ç¦»å’Œ SVM çš„è¿˜ä¸ä¸€æ ·ï¼ŒSVM ä¸ºäº†è¡¡é‡ margin å’Œè¶…å¹³é¢çš„è·ç¦»ï¼Œå°†è¿™ç§ç‚¹åˆ°é¢çš„è·ç¦»å°±é‡‡ç”¨å®é™…çš„å‚ç›´è·ç¦»ã€‚

<p align="center">
  <img width="220" height="" src="/img/media/15470889554836.jpg">
</p>
æˆ‘ä»¬å°±éœ€è¦é€šè¿‡ä¸€ç§è¿­ä»£è®­ç»ƒçš„æ–¹å¼ï¼Œä»ä¸€ç»„éšæœºçš„ç³»æ•°å¼€å§‹ï¼Œæ…¢æ…¢æ‰¾åˆ°æœ€ä¼˜çš„ç³»æ•°ã€‚è¿™é‡Œè¯´çš„æœ€ä¼˜çš„ç³»æ•°ï¼Œå°±ä»èƒ½å¤Ÿæ˜¯æˆ‘ä»¬é¢„æµ‹å‡ºæ¥çš„å€¼ä¸çœŸå®å€¼çš„å·®è·æœ€å°çš„ç³»æ•°ï¼›è¿™ç§è¯¯å·®æˆ‘ä»¬ä¸ºäº†é¿å…ç®€å•çš„åŠ å‡è¯¯å·®ä¼šå› ä¸ºæ­£è¯¯å·®å’Œè´Ÿè¯¯å·®ä¼šç›¸äº’æŠµæ¶ˆï¼Œæˆ‘ä»¬ç”¨å¹³æ–¹è¯¯å·®æ¥è¡¡é‡ã€‚
$$
{1\over{2}}\sum _{i=1}^m (y_i-x_iw)^2
$$
å‰é¢çš„ $1\over2$ æ˜¯ä¸ºäº†æ±‚å¯¼æ—¶èƒ½å¤Ÿä¸ç³»æ•°æ¶ˆæ‰ï¼Œæˆ‘ä»¬ç”¨çŸ©é˜µçš„å½¢å¼è¡¨ç¤ºèƒ½å¤Ÿå¾—åˆ°ï¼š

$$
{1\over 2}(y-Xw)^T(y-Xw)
$$


è¿™é‡Œå¯ä»¥é€šè¿‡çŸ©é˜µçš„å¤§å°æ¥åˆ¤æ–­ï¼Œ$wâ€‹$ è¿˜çœŸå¾—æ”¾åˆ° $Xâ€‹$ çš„å³è¾¹ï¼Œloss æœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå®æ•°ï¼Œé€šè¿‡ç»“æœåæ¨å„ä¸ªéƒ¨åˆ†çŸ©é˜µçš„å¤§å°å³å¯ã€‚

$$
y: (m,1)\\
X: (m, n)\\
w: (m, 1)\\
y-Xw: (m, 1)
$$


å¯¹åº”çš„æˆ‘ä»¬å¼€å§‹æ±‚åå¯¼ï¼Œé¦–å…ˆæˆ‘ä»¬æ–°å®šä¹‰ä¸€ä¸ªå‘é‡ $vâ€‹$ ä½œä¸ºä¸­é—´å˜é‡ï¼š

$$
v=y-Xw
$$
ç„¶åæˆ‘ä»¬åˆ©ç”¨ [Frobenius inner product](https://en.wikipedia.org/wiki/Frobenius_inner_product)ï¼ˆå³å…¬å¼ä¸­çš„ â€œ:â€ï¼‰è¡¨ç¤ºä¸Šé¢çš„å¼å­ï¼š

$$
f={1\over2}||v||_F^2={1\over 2}v:v
$$
æ¥ç€æ±‚è§£å‡½æ•° $fâ€‹$ çš„å¾®åˆ†ï¼ˆdifferentialï¼‰$dfâ€‹$ï¼š

$$
\begin{aligned}
df &= {1\over2}\times2v:dv \\
&= -v:Xdw \\
&= -X^Tv:dw
\end{aligned}
$$

å…·ä½“åˆ¤æ–­åŠ ä¸åŠ è½¬ç½®æˆ–è€…ä½ç½®é—®é¢˜ï¼Œéƒ½å¯ä»¥é€šè¿‡æ ‡è¯†å‡ºçŸ©é˜µå¤§å°çš„æ–¹å¼æ¸…æ™°åœ°åˆ†è¾¨å‡ºæ¥ã€‚æ¥ç€å†æ±‚å¯¹åº”çš„æ¢¯åº¦ï¼ˆgradientï¼‰ï¼š

$$
\begin{aligned}
{\partial f}\over {\partial w} &=  -X^Tv \\
&= -X^T(y-Xw)
\end{aligned}
$$

äºæ˜¯ï¼Œæˆ‘ä»¬å¯¹ $w$ æ±‚å¯¼æœ‰ $-X^T(y-Xw)$ï¼Œä»¤å…¶ä¸ºé›¶ï¼Œè§£å‡º $w$ çš„æœ€ä¼˜è§£ï¼š
$$
-X^T(y-Xw) = -X^Ty+X^TXw = 0
$$
åˆ™æœ‰å½“ $X^TXâ€‹$ æ»¡ç§©çŠ¶æ€ä¸‹çš„è§£æè§£ï¼š

$$
\hat{w} = (X^TX)^{-1}X^Ty
$$
å½“ç„¶è¿˜ä¼šå‡ºç° $X^TX$ **ä¸æ»¡ç§©**çš„æƒ…å†µï¼Œæ­¤æ—¶å¯ä»¥è§£å‡ºå¤šä¸ª $\hat{w}$ éƒ½èƒ½æ˜¯å¾—å‡æ–¹è¯¯å·®æœ€å°åŒ–ï¼Œé‚£å…·ä½“é€‰æ‹©å“ªä¸€ä¸ªä½œä¸ºè¾“å‡ºï¼Œå¯èƒ½å°±ä¾èµ–å­¦ä¹ ç®—æ³•çš„åå¥½å†³å®šäº†ã€‚å¯¹äº $X^TX$ ä¸æ»¡ç§©çš„æƒ…å†µï¼Œæ¯”è¾ƒå¸¸è§çš„åšæ³•æ˜¯å¼•å…¥æ­£åˆ™åŒ–ï¼ˆregularizationï¼‰é¡¹ï¼Œæ¯”å¦‚åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥å¯¹å‚æ•°çš„ L2-normï¼š

$$
E = \frac { 1 } { 2 } \sum _ { i = 1 } ^ { m } (y_i-x_iw)^2 + \frac { \gamma } { 2 } \| \mathbf { w } \| ^ { 2 }
$$

å¼•å…¥æ­£åˆ™åŒ–é¡¹åæ±‚è§£çš„å‚æ•°å…¬å¼å°±å˜æˆäº†ï¼š

$$
\hat { w }^* = \left( \gamma \mathbf { I } + X ^ { T } X \right) ^ { - 1 } X ^ { T } y
$$

å½“ç„¶å¯¹äºæ¨å¯¼è¿˜æœ‰ä¸€ç§[å…¨å±•å¼€çš„å½¢å¼](https://towardsdatascience.com/analytical-solution-of-linear-regression-a0e870b038d5)ï¼Œè¿™é‡Œå°±ä¸å†ç»†ç©¶äº†ã€‚ç„¶è€Œå…¶ä¸­è¿˜æœ‰ä¸€ä¸ªç°æ²¡æœ‰ææ¸…æ¥šçš„é—®é¢˜ï¼Œå³å¦‚æœå¼•å…¥äº†æ­£åˆ™åŒ–å‚æ•°ä¹‹åï¼Œç›´æ¥æ±‚é—­è¯•è§£æ˜¯å¦ä¸€å®šèƒ½å¾—åˆ°é—­è¯•è§£ï¼Ÿç›®å‰çš„ç†è§£æ˜¯ï¼Œæ¯•ç«Ÿ $X^TX$ æ­¤æ—¶å·²ç»æ»¡ç§©äº†ï¼Œé‚£ä¹ˆæ­¤æ—¶éœ€è¦è€ƒè™‘çš„å°±æ˜¯çŸ©é˜µç›¸ä¹˜è®¡ç®—çš„é—®é¢˜äº†ã€‚


## Implementation

å¯¹äºçº¿æ€§å›å½’çš„æ±‚è§£åŠæ³•ä¹Ÿæ˜¯è§„è§„çŸ©çŸ©çš„å‡ ä¸ªæ­¥éª¤ï¼š
```
1. åˆå§‹åŒ–æƒé‡
2. è®¡ç®—å¹¶æ›´æ–°æƒé‡
    * é€šè¿‡ Gradient Descent
    * é€šè¿‡ Projection Matrix
3. æ²¡æœ‰è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°æˆ–è€… SSE å˜åŒ–å¾ˆå°æ—¶åˆ™ç»§ç»­è¿­ä»£ï¼Œå¦åˆ™é€€å‡º
```

å‚è€ƒäº† Github çš„å®ç°è‡ªå·±[æ‰‹å†™äº†ä¸€é](https://github.com/binlidaily/machine-learning-from-scratch/blob/master/supervised_learning/regression.py)ï¼Œå…¶ä¸­ç”¨ SVD çš„éƒ¨åˆ†è¿˜éœ€è¦é‡æ–°æ•´ç†ç†è§£ä¸‹ï¼é™¤äº†é€šè¿‡ SVD çš„æ–¹å¼ï¼Œè¿˜å¯ä»¥ç”¨ QR åˆ†è§£æ¥è®¡ç®—ï¼Œè€Œä¸”åœ¨å°è§„æ¨¡çš„æ•°æ®é‡ä¸‹æ›´å»ºè®®ç”¨è¿™ä¸¤ç§åˆ†è§£æ–¹å¼æ¥æ±‚ã€‚ğŸ¤”

```python
class LinearRegression(Regression):
    """
    Linear model.

    Parameters:
    -----------
    n_iterations: float
        The number of training iterations the algorithm will tune the weights for.
    learning_rate: float
        The step length that will be used when updating the weights.
    gradient_descent: boolean
        True or false depending if gradient descent should be used when training. If
        false then we use batch optimization by least squares.

    """

    def __init__(self, n_iterations=100, learning_rate=0.001, gradient_descent=True):
        self.gradient_desent = gradient_descent
        # regularization is a class
        self.regularization = lambda x: 0
        self.regularization.grad = lambda x: 0
        super(LinearRegression, self).__init__(n_iterations=n_iterations,
                                               learning_rate=learning_rate)

    def fit(self, X, y):
        # If not gradient descent => Least squares approximation of w
        if not self.gradient_desent:
            # add bias weights (set 1 default) to training data x
            X = np.insert(X, 0, 1, axis=1)
            # Calculate weights by least squares (using Moore-Penrose pseudoinverse)
            # Use Projection Matrix calculate weights
            U, S, V = np.linalg.svd(X.T.dot(X))
            S = np.diag(S)
            X_sq_reg_inv = V.dot(np.linalg.pinv(S)).dot(U.T)
            self.w = X_sq_reg_inv.dot(X.T).fit(X, y)
        else:
            super(LinearRegression, self).fit(X, y)
```


### æ³¨æ„ç‚¹
* æ³¨æ„åœ¨çŸ©é˜µå½¢å¼ä¸‹å’Œåœ¨å±•å¼€å†™çš„å½¢åŠ¿ä¸‹å¦‚ä½•æ±‚åå¯¼ï¼ˆå¤–å±‚æ˜¯è¿­ä»£ï¼Œå†…å±‚æ˜¯æ‰€æœ‰æ ·æœ¬çš„å¾ªç¯ï¼‰
* è¿­ä»£æ›´æ–°å‚æ•°æ—¶ï¼Œå…·ä½“å¦‚ä½•åœ¨æ‰€æ‰€æœ‰æ ·æœ¬è¿›è¡Œæ¯ä¸€è½®æ›´æ–°ï¼ˆi é’ˆå¯¹çš„æ˜¯æ›´æ–°è½®æ•°ï¼Œä¸æ˜¯ç¬¬ i ä¸ªæ ·æœ¬ï¼‰

å½“æ•°æ®é‡ä¸å¤ªå¤§è€Œä¸”é—­è¯•è§£å­˜åœ¨æ—¶å¯ä»¥ä½¿ç”¨å…¬å¼ç›´æ¥è®¡ç®—ï¼ˆå¦‚æœæ˜¯ç¨€ç–çš„ä¼šæ›´å¥½ï¼Œå¯ä»¥å‡å°‘å­˜å‚¨é‡ï¼‰ï¼Œä½†æ˜¯å½“æ•°æ®é‡æ¯”è¾ƒå¤§è€Œä¸”æ˜¯ Dense çš„ï¼Œåœ¨å­˜å‚¨ä¸Šéƒ½ä¼šæˆé—®é¢˜ï¼Œæ›´ä¸ç”¨è¯´å¿«é€Ÿè®¡ç®—äº†ã€‚æ‰€ä»¥å¯ä»¥ Gradient Descent æ¥å®ç°ï¼Œä¸éœ€è¦éå¸¸å¤§çš„å­˜å‚¨é‡ã€‚å½“æ—¶æ›´å»ºè®®ä½¿ç”¨ QR å’Œ SVD çš„æ–¹å¼æ¥è®¡ç®—ã€‚

## References
1. [Matrix calculus in multiple linear regression OLS estimate derivation](https://math.stackexchange.com/questions/1968478/matrix-calculus-in-multiple-linear-regression-ols-estimate-derivation)
2. [How to Solve Linear Regression Using Linear Algebra](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)
3. [Why is SVD applied on Linear Regression](https://stackoverflow.com/questions/37072067/why-is-svd-applied-on-linear-regression)
4. [Do we need gradient descent to find the coefficients of a linear regression model?](https://stats.stackexchange.com/questions/160179/do-we-need-gradient-descent-to-find-the-coefficients-of-a-linear-regression-mode/164164#164164)
5. [Lecture 9. Linear Least Squares. Using SVD Decomposition.](https://www2.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_9_Linear_least_squares_SVD.pdf)
6. [ä½ åº”è¯¥æŒæ¡çš„ 7 ç§å›å½’æ¨¡å‹](https://zhuanlan.zhihu.com/p/40141010)






