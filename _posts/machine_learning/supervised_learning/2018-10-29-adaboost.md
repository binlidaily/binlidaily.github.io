---
layout: post
title: Adaboost
subtitle:
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published:  true
---

Adaboost 是一个对“三个臭皮匠顶个诸葛亮”的最好诠释，其利用多个弱分类器的线性组合来达到一个强分类器的效果。在算法中会对每一个样本赋予一个权重，在后续的训练中会提高前一轮被误分类的样本权重，而降低正确分类的样本权重。对于弱分类器的线性组合利用加权多数表决的方法。接下来就具体介绍 Adaboost 算法。

## Adaboost 算法
首先假设一些基本，数据集记作：
$$
T = \left\{ \left( x , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots . \left( x _ { m } , y _ { m } \right) \right\}
$$

接着假设有 $T$ 个弱学习器，且对于每一个学习器 $G_k(x)$ 来说，样本的权重（初始化时）为：

$$
D ( k ) = \left( w _ { k 1 } , w _ { k 2 } , \ldots w _ { k m } \right) ; \quad w _ { 1 i } = \frac { 1 } { m } ; \quad i = 1,2 \ldots m
$$

那么当所有的弱学习器训练好后，我们能得到一个集成的最终分类器，可以看出 Adaboost 是一个加法模型，最终的强分类器是由若干个弱学习加权平均得到：

$$
f ( \mathrm { x } ) = \sum _ { t = 1 } ^ { T } \alpha _ { t } G _ { t } ( \mathrm { x } )
$$

其中 $\alpha _ { t } $ 是每一个弱学习器的权重，那么最终的决策函数为：

$$
sign \left(\sum _ { t = 1 } ^ { T } \alpha _ { t } G _ { t } ( \mathrm { x }) \right)
$$

如果是二分类的话，每一个弱分类器的结果是 $\{+1, -1\}$，最终的决策函数通过综合所有弱分类器结果得出合适的 $\{+1, -1\}$ 结果。

知道所有的弱学习器是如何合作决策的，那么我们现在来搞清楚在训练时这些弱学习器的权重，以及每一个弱学习器下该如何更新如上面介绍的样本权重 $D(k)$。

在想办法更新权重前，我们必须定义每个弱学习器 $G(x)$ 在训练集上的分类误差率：

$$
e _ { k } = P \left( G _ { k } \left( x _ { i } \right) \neq y _ { i } \right) = \sum _ { i = 1 } ^ { m } w _ { k i } I \left( G _ { k } \left( x _ { i } \right) \neq y _ { i } \right)
$$

分类错误率 $e _ { k }$ 只记录那些分错的样本的权重并累加。

Adaboost 的这种残差学习方法是采用前向分步学习算法的具体步骤，利用前一个学习器的当一个弱学习器训练好后，如何去推动下一个弱学习器呢？我们记第 $k-1$ 轮的强学习器为：

$$
f _ { k - 1 } ( x ) = \sum _ { i = 1 } ^ { k - 1 } \alpha _ { i } G _ { i } ( x )
$$

而第 $k$ 轮的强学习器为：

$$
f _ { k } ( x ) = \sum _ { i = 1 } ^ { k } \alpha _ { i } G _ { i } ( x )
$$

则有:

$$
f _ { k } ( x ) = f _ { k - 1 } ( x ) + \alpha _ { k } G _ { k } ( x )
$$

至此，我们知道了最终决策函数的表达方式，以及每一轮强学习器的更新方式，那么对于一个完备的模型来讲还差一个衡量训练后的函数好坏的指标——损失函数，Adaboost 用的是指数损失函数！

$$
\underbrace { \arg \min } _ { \alpha , G } \sum _ { i = 1 } ^ { m } \exp \left( - y _ { i } f _ { k } ( x ) \right)
$$

至于为什么 Adaboost 用指数损失函数，ESL 上介绍说有两个原因：1）其有良好的可计算性；2）在更新权重时有比较简单的形式。

利用上面介绍的前向分布学习算法的关系，我们可以将指数损失函数改写成：

$$
\left( \alpha _ { k } , G _ { k } ( x ) \right) = \underbrace { \arg \min } _ { \alpha , G } \sum _ { i = 1 } ^ { m } \exp \left[ \left( - y _ { i } \right) \left( f _ { k - 1 } ( x ) + \alpha G ( x ) \right) \right]
$$

我们可以尝试将指数部分的加法部分提前面并拆开分成已有的强学习器和当前弱学习器对训练样本的指数损失函数，我们可以令：

$$
w _ { k i } ^ { \prime } = \exp \left( - y _ { i } f _ { k - 1 } ( x ) \right)
$$

其表示第 $i$ 个样本在前一个已有的强学习器 $f _ { k - 1 } ( x )$ 上的指数损失函数值，这个部分只依赖于前面训练得到的强学习器 $f _ { k - 1 } ( x )$，跟当前的弱学习器 $G$ 和弱学习器权重 $\alpha$ 没有关系，不影响最小化，**于是我们可以将其看成第 $i$ 个样本的权重**。那么可以进一步改写：

$$
\left( \alpha _ { k } , G _ { k } ( x ) \right) = \underbrace { \arg \min } _ { \alpha , G } \sum _ { i = 1 } ^ { m } w _ { k i } ^ { \prime } \exp \left[ - y _ { i } \alpha G ( x ) \right]
$$

首先，我们求 $G_k(x)$，可以把 $\alpha$ 看成常数，损失函数可理解为求弱分类器 $G_k(x)$ 使得其误分类样本的数量尽可能的少。等价于下面这个式子：

$$
G _ { k } ( x ) = \underbrace { \arg \min } _ { G } \sum _ { i = 1 } ^ { m } w _ { k i } ^ { \prime } I \left( y _ { i } \neq G \left( x _ { i } \right) \right)
$$

在上述指数损失函数的基础上，我们可以按照 $y_i \neq G(x_i)$ 成立与否将指数损失函数继续改写：

$$
\begin{aligned} \sum _ { i = 1 } ^ { m }  { w }^\prime _ { k  i } \cdot \exp \left[ - y _ { i  } \alpha G \left( x _ {  i  } \right) \right] = & \sum _ { y _ {  i  } = G _ { k } \left( x _ {  i  } \right) } { w }^\prime _ { k  i } \cdot e ^ { - \alpha } + \sum _ { y _ {  i  } \neq G _ { k } \left( x _ {  i  } \right) } { w }^\prime _ { k  i } \cdot e ^ { \alpha } \\ & = \left( e ^ { \alpha } - e ^ { - \alpha } \right) \sum _ { i = 1 } ^ { m } { w }^\prime _ { k  i } I \left( y _ {  i  } \neq G \left( x _ {  i  } \right) \right) + e ^ { - \alpha } \sum _ { i = 1 } ^ { m } { w }^\prime _ { k  i } \end{aligned}
$$

求该式的极值，对 $\alpha$ 求导，使其等于 $0$，得到：

$$
\left( e ^ { \alpha } + e ^ { - \alpha } \right) \sum _ { i = 1 } ^ { m } w _ { k i } ^ { \prime } I \left( y _ { i } \neq G \left( x _ { i } \right) \right) - e ^ { - \alpha } \sum _ { i = 1 } ^ { m } w _ { k i } ^ { \prime } = 0
$$

$$
\left( e ^ { \alpha } + e ^ { - \alpha } \right) \frac { \sum _ { i = 1 } ^ { m } w _ { k i } ^ { \prime } I \left( y _ { i } \neq G \left( x _ { i } \right) \right) } { \sum _ { i = 1 } ^ { m } w _ { k i } ^ { \prime } } - e ^ { - \alpha } = 0
$$


---


整理一下几个概念：
* 加法模型
* 前向分布算法

* 每一个样本都有权重，每一个弱学习器也有权重！

前向分布算法（Forward Stage-wise Algorithm）vs 前向分步加法模型？

提升方法没有正则化，所以会用 early stop 的方式达到正则化的效果。

![](/img/media/15409873907699.jpg)




1. [ML Lecture 22: Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&t=0s&index=33)
2. [第06章：深入浅出ML之Boosting家族](http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/)
3. [集成模型之Adaboost算法(三)](https://zhuanlan.zhihu.com/p/38507561)