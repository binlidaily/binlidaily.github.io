---
layout: post
title:  Metrics
subtitle: 性能度量
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---


## 准确率、查准率与查全率
　　准确率（Accuracy）是指**分类**正确（真的正类和真的负类）的样本占总样本个数的比例：

$$
\text{Accuracy}=\frac{n_\text{correct}}{n_\text{total}}
$$

　　准确率是分类问题中最简单也最直观的评价指标，但是也有明显的缺陷。比如，当负样本当负样本占 99% 时，分类器把所有样本都预测为负样本也可以获得 99% 的准确率。所以，当不同类别的**样本比例非常不均衡**时，占比大的类别往往成为影响准确率的最主要因素。

　　为了解决这个问题，可采用更为有效的平均准确率（每个类别下的样本准确率的算术平均）作为模型评估的指标。

<p align="center">
  <img width="600" height="" src="/img/media/15614281343142.jpg">
</p>

　　矩阵中的每一行报告一个类别的不同分类。你可以想象从左边进入，按不同的列进行分类，再从顶部退出的情况。准确率定义为正确答案占总体的比例，精确率定义为正确答案占标记为正类的比例，召回率定义为正确答案占本身是正类的比例。图中依据浅灰色区域的案例，对暗灰色区域的案例进行了划分。

　　查准率（Precision）是指真的正类数（正确地被标记为属于正类的实例数）占被标记为属于正类的实例数（真的正类数和假的正类数之和，假的正类是指错误地被标记为属于正类的实例）的比例，相较于对预测结果而言。查全率（Recall）是指真的正类数占本身属于正类的实例数（即真的正类数和假的负类数之和，假的负类是本应该标记为正类，却错误地被标记为负类的实例）的比例，相较于真实情况而言。所以采摘蘑菇时，你是希望精确率更高，还是召回率更高？😆

简言之：
* 查准率：
    * 表示**预测**有多准，结果就是预测中正确正例在预测为正例中的占比。
    * 回答这个问题：“有多少被标记为正类的案例是正确的?”
* 查全率：
    * 表示找到所有真实正例的本领有多大，结果就是看预测正确正例在所有真实正例中的占比。真实正例有一部分是，本来是正例但是在预测时被预测成负例了。
    * 回答这个问题：“有多少正类的案例被正确地检索为正类了?”

　　Precision 和 Recall 是既矛盾又统一的两个指标，为了综合评估一个模型的好坏，可以绘制出 P-R（Precision-Recall）曲线，其横轴是 Recall，纵轴是 Precision，上面的点表示什么呢？对于排序模型来讲，其上每一个点代表着，在某一个阈值下，模型将大于该阈值的结果判定为正样本，小于阈值的判断为负样本，此时返回的结果对应的查准率和查全率。如图：

<p align="center">
  <img width="400" height="" src="/img/media/15610870815236.jpg">
</p>

### F1 score
F1 score 是查准率和查全率的调和平均值，即综合考虑查准率和查全率的指标，也能用来反映一个排序模型的性能，越大越好：

$$
\text{F1} = \frac{2\times \text{precision} \times \text{recall}}{ \text{precision} + \text{recall}}
$$

基本上呢，问题就是如果你的两个模型，一个precision特别高，recall特别低，另一个recall特别高，precision特别低的时候，f1-score可能是差不多的，你也不能基于此来作出选择。

## ROC 曲线
　　受试者工作特征曲线（Receiver Operating Characteristic Curve, ROC）起源于军事，后广泛用于医学，是二分类的一种衡量指标。ROC 曲线的横坐标为假阳性率（False Positive Rate, FPR），纵坐标为真阳性率（True Positive Rate，TPR）。

$$
\begin{aligned} F P R &=\frac{F P}{N} \\ T P R &=\frac{T P}{P} \end{aligned}
$$

　　其中 $P$ 是真实的正样本的数量，N 是真实的负样本的数量，TP 是 P 个正样本中被分类器预测为正样本的个数，FP 是 N 个负样本中被分类器预测为正样本的个 数。

　　为了更直观地说明这个问题，我们举一个医院诊断病 人的例子。假设有10位疑似癌症患者，其中有3位很不幸确实患了癌症（P=3），另外7位不是癌症患者（N=7）。医院对这10位疑似患者做了诊断，诊断出3位癌症 患者，其中有2位确实是真正的患者（TP=2）。那么真阳性率TPR=TP/P=2/3。对 于7位非癌症患者来说，有一位很不幸被误诊为癌症患者（FP=1），那么假阳性率 FPR=FP/N=1/7。对于“该医院”这个分类器来说，这组分类结果就对应ROC曲线上的一个点（1/7，2/3）。

　　ROC 曲线是通过不断移动分类器的截断点（cut point）来生成曲线上的一组关键点，截断点就是区分正负预测结果的阈值。

　　ROC 曲线跟 P-R 曲线相比，随着正负样本的分布发生变化时，ROC 曲线的形状能基本保持不变，而 P-R 曲线一般会发生剧烈变化。这个特点让 ROC 曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。

### AUC
　　AUC 值得是 ROC 曲线下的面积大小，该值能够量化地反映基于 ROC 曲线衡量出的模型性能。AUC 越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。


## KL 散度（Kullback-Leibler Divergence）
　　KL 散度（相对熵），是一种**量化两种概率分布 P 和 Q 之间差异的方式**，又叫相对熵。在概率学和统计学上，我们经常会使用一种更简单的、近似的分布来替代观察数据或太复杂的分布。K-L散度能帮助我们度量使用一个分布来近似另一个分布时所损失的信息。

$$
D_{K L}(p(x) \| q(x))=\sum_{x \in X} p(x) \ln \frac{p(x)}{q(x)}
$$

## AP (Average Precision)
![](/img/media/15689788066970.jpg){:.center-image}

　　常用来做目标检测的指标衡量，越大越好，mAP 是对多个类的 AP 取了平均。

## TODO
目标检测
* mAP
* 协方差


## References
1. [如何理解K-L散度（相对熵）](https://www.jianshu.com/p/43318a3dc715)
2. [精确率、召回率、F1 值、ROC/AUC 、PRC各自的优缺点是什么？](http://frankchen.xyz/2016/09/21/metric/)
3. [mAP (mean Average Precision) for Object Detection](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)
4. [Understanding the mAP Evaluation Metric for Object Detection](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3)