---
layout: post
title: Types of Model
subtitle: 模型分类
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

## 1. 生成模型和判别模型

　　监督学习方法可以分为生成方法 (Generative Approach) 和判别方法 (Discriminative Approach)，对应生成的模型就是生成模型和判别模型。这个模型的一般形式为一个决策函数或一个条件概率分布（后验概率）：

$$
Y=f(X) \text { or } P(Y | X)
$$

* 决策函数：输入 $X$ 返回 $Y$；其中 $Y$ 与一个阈值比较，然后根据比较结果判定 $X$ 的类别
* 条件概率分布：输入 $X$ 返回 $X$ 属于每个类别的概率；将其中概率最大的作为 $X$ 所属的类别

　　监督学习模型可分为生成模型与判别模型:
* 生成模型
    * 由数据学习联合概率分布 $P(X,Y)$，然后根据条件概率公式计算 $P(Y|X)$作为预测
    * 模型表示了给定输入 $X$ 产生输出 $Y$ 的生成关系。
$$
P(Y | X)=\frac{P(X, Y)}{P(X)}
$$

    * 优点
        * 可以还原出联合概率分布 $P(X,Y)$，判别方法不能
        * 学习收敛速度更快；即当样本容量增加时，学到的模型可以更快地收敛到真实模型
        * 当存在“隐变量”时，只能使用生成模型
    * 缺点
        * 学习和计算过程比较复杂
    * 举例
        * 朴素贝叶斯
        * 隐马尔科夫模型
        * 混合高斯模型
        * 贝叶斯网络
        * 马尔可夫随机场
* 判别模型
    * 直接学习决策函数或者条件概率分布；直观来说，判别模型学习的是类别之间的最优分隔面，反映的是不同类数据之间的差异
    * 判别模型只关心对给定的输入 $X$，应该预测什么样的输出 $Y$。
    * 优点
        * 因为直接从数据中学习，直面预测，所以准确率更高
        * 由于直接学习 $P(Y|X)$ 或 $f(X)$，可以对数据进行各种程度的抽象，定义特征并使用特征，以简化学习过程
    * 缺点
        * 不能反映训练数据本身的特性
    * 举例
        * K 近邻、感知机（神经网络）、决策树、逻辑斯蒂回归、最大熵模型、SVM、提升方法、条件随机场


　　生成模型和判别模型的联系：
* 由生成模型可以得到判别模型，但由判别模型得不到生成模型。
* 当存在“隐变量”时，只能使用生成模型
    * 隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量”


![](/img/media/15607792534960.jpg)

　　其实机器学习的任务是从属性X预测标记 $Y$，即求概率 $P(Y|X)$；

　　对于判别式模型来说求得 $P(Y|X)$，对未见示例 $X$，根据 $P(Y|X)$ 可以求得标记$Y$，即可以直接判别出来，如上图的左边所示，实际是就是直接得到了判别边界，所以传统的、耳熟能详的机器学习算法如线性回归模型、支持向量机SVM等都是判别式模型，这些模型的特点都是输入属性 $X$ 可以直接得到 $Y$（对于二分类任务来说，实际得到一个 score，当 score 大于 threshold 时则为正类，否则为反类）。

　　而生成式模型求得P(Y,X)，对于未见示例X，你要求出X与不同标记之间的联合概率分布，然后大的获胜，如上图右边所示，并没有什么边界存在，对于未见示例（红三角），求两个联合概率分布（有两个类），比较一下，取那个大的。机器学习中朴素贝叶斯模型、隐马尔可夫模型HMM等都是生成式模型，熟悉Naive Bayes的都知道，对于输入X，需要求出好几个联合概率，然后较大的那个就是预测结果~根本原因个人认为是对于某示例X_1，对正例和反例的标记的联合概率不等于1，即P(Y_1,X_1)+P(Y_2,X_1)<1，要遍历所有的X和Y的联合概率求和，即sum(P(X,Y))=1，具体可参见楼上woodyhui提到的维基百科Generative model里的例子）



无论是生成式模型还是判别式模型，都可作为分类器使用，分类器的数学表达即为：给定输入 $X$ 以及分类变量 $Y$，求 $P(Y\vert X)$。

判别式模型直接估算 $P(Y\vert X)$，或者也可像 SVM 那样，估算出输入和输出之间的映射，与概率无关；

判别式模型的典型代表是：logistic 回归；
产生式模型的思想是先估计联合概率密度 $P(X,Y)$，再通过贝叶斯公式求出 $P(Y\vert X)$；

生成式模型的典型代表则是：朴素贝叶斯模型；

一般认为判别式模型更受欢迎，“人们更应该直接去解决问题，永远不要把求解更复杂的问题作为中间阶段”（Vapnik），Andrew Ng 的论文[1]对此作了较为全面的分析，产生式模型（朴素贝叶斯）在少量样本的情况下，可以取得更好的精确率，判别式模型（logistics 回归）在样本增加的情况下，逐渐逼近前者的概率；

这里有人会问，那为什么它是一个生成模型呢？简而言之，我们首先有一个类，也有这个类的y的先验概率分布，并且知道这个类的分布类型是伯努利分布。那么生成过程就是（1）从伯努利分布的类中抽样。 （2）基于类标签，我们从相应的分布中抽取x。这便是一个生成过程。

## 参数方法（parameter）与非参数方法（nonparameter）
参数方法表示参数固定，不随数据点的变化而变化； 
非参数方法并不意味着没有参数，而是说，参数的数目随数据点而变化，

**1. 参数方法举例**
logistic regression：p(y=1|x,α)=11+exp(−xTα)p(y=1|x,α)=11+exp⁡(−xTα)，显然参数，αα 的维数会随着数据集属性列个数的变化而变化，而不会随着数据规模的变化而变化；

**2. 非参数方法举例**
Nearest-Neighbor：比如一个二分类问题，新来一个测试点，当要计算其所属类别时，需要与全部训练集计算距离；

## References
1. [生成式模型（generative） vs 判别式模型（discriminative）](https://blog.csdn.net/lanchunhui/article/details/60321358)
2. [机器学习“判定模型”和“生成模型”有什么区别？](https://www.zhihu.com/question/20446337/answer/256466823)
3. [参数方法（parameter）与非参数方法（nonparameter）](https://blog.csdn.net/lanchunhui/article/details/53574727)
4. 《统计学习方法》（李航）