---
layout: post
title: K-Means
subtitle: K 均值聚类
author: Bin Li
tags: [Machine Learning]
image: 
comments: true
published: true
---

　　K-Means 算法是无监督聚类算法，实现简单，聚类效果也不错，所以应用广泛。

## 1. K-Means 算法
　　K-Means 的**思想**非常简单：
* 对于给定的数据集，按照样本之间的距离，将数据集划分成 $K$ 个簇。
* 尽量让簇内的样本点尽可能的靠近，而簇间的样本则距离尽量疏远。

　　看到这个思想我们首先要搞清楚的问题是，如何衡量样本间的距离？K-Means 采用的是常见的欧式距离平方（Squared Euclidean Distance）作为样本之间的距离度量:

$$
\begin{aligned}
d(x_i, x_j) &= \sum _ {k=1} ^ n (x_{ki} - x_{kj})^2 \\
&=\| x_i - x_j\|^2
\end{aligned}
$$

　　其中 $n$ 为特征维度。基于此我们就可以考虑猜想的第二个部分，让簇内的样本尽可能的靠近，假设簇被划分为 $\left(C_{1}, C_{2}, \ldots C_{k}\right)$，我们通过衡量样本与其所属簇中心的距离总和作为损失函数，目标是最小化簇内距离：

$$
E=\sum_{i=1}^{k} \sum_{x \in C_{i}}\left\|x-\mu_{i}\right\|_{2}^{2}
$$

　　其中 $\mu_i$ 是簇 $C_i$ 的均值向量，也被称为质心，表达式为：

$$
\mu_{i}=\frac{1}{\left|C_{i}\right|} \sum_{x \in C_{i}} x
$$

　　然而，这是一个组合问题，$m$ 个样本分到 $k$ 个簇，那么可能的分法有：

$$
S(m, k)=\frac{1}{k !} \sum_{l=1}^{k}(-1)^{k-l}\left(\begin{array}{c}{k} \\ {l}\end{array}\right) k^{m}
$$

　　这个明显是指数级别的问题，NP 难，现实中我们用启发式的迭代方式求解。

<p align="center">
  <img width="" height="" src="/img/media/15602575855752.jpg">
</p>
<p style="margin-top:-2.5%" align="center">
    <em style="color:#808080;font-style:normal;font-size:80%;">K-Means 启发式执行过程</em>
</p>

　　图解如下：
* (a) 初始化数据集，假设 $k=2$；
* (b) 随机选择两个簇对应的质心；
* (c) 计算所有的样本离着这两个质心的距离，选择就近的质心归属该簇；
* (d) 对于两个簇的新样本，计算各自的样本均值，作为其新的质心；
* (e) 在新的质心下，重复 (c)，得到新的两个簇；
* (f) 对于两个簇的新样本，重复 (d)。

　　在实际运行 K-Means 算法时，会多次重复步骤 (c)(d)，才能达到较好的结果。

　　了解了 K-Means 的启发式流程后，可以发现几个要点：
1. 对于 K-Means 算法，$K$ 的选取是非常重要的。
    * 一般可以利用数据先验经验选择一个合适的 $K$ 值；
    * 如果没有先验知识，可以通过交叉验证选择。
2. $K$ 确定后，$K$ 个质心的初始化对结果和算法运行时间有较大影响。
    * 需要选择合适的质心，最好这些质心不能相距太近。

### 1.1 K-Means 算法流程

传统的 K-Means 算法流程如下：
* 输入：样本集 $D=\left\{x_{1}, x_{2}, \ldots x_{m}\right\}$，聚类的簇数 $k$，最大迭代次数 $N$。
* 输出：划分 $C=\left\{C_{1}, C_{2}, \ldots C_{k}\right\}$

1. 从样本集 $D$ 中随机选择 $k$ 个样本作为初始的 $k$ 个质心向量：$C=\left\{C_{1}, C_{2}, \ldots C_{k}\right\}$
2. 对于 $n=1,2, \dots, N$

　　a）将簇划分 $C$ 初始化为 $C_{t}=\varnothing$ ($t=1,2 \ldots k$)

　　b) 对于 $\mathrm{i}=1,2 \ldots \mathrm{m}$，计算样本 $x_i$ 和各个质心向量 $\mu_j$ ($j=1,2, \ldots k$) 的距离 $d_{i j}=\left\|x_{i}-\mu_{j}\right\|_{2}^{2}$，将 $x_i$ 标记为最小的 $d_{ij}$ 所对应的类别 $\lambda_i$。此时更新 $C_{\lambda_{i}}=C_{\lambda_{i}} \cup\left\{x_{i}\right\}$。

　　c) 对于 $j=1,2, \dots, k$，对 $C_j$ 中所有的样本点重新计算新的质心 $\mu_{j}=\frac{1}{\left|C_{j}\right|} \sum_{x \in C_{j}} x$。

　　d) 如果所有的 $k$ 个质心向量都没有发生变化，则退出迭代，继续步骤 3。

{start="3"}

3. 输出簇划分 $C=\left\{C_{1}, C_{2}, \ldots C_{k}\right\}$。

## 2. K-Means 的优化
　　我们从 K-Means 的几个要点出发优化 K-Means，以期达到更好的使用效果。

### 2.1 K-Means 初始化优化 K-Means++
　　$K$ 个初始化的质心位置选择对最后的聚类结果和运行时间都有很大的影响，如果仅是完全采用随机的选择，有可能导致算法收敛很慢。K-Means++ 算法就是对 K-Means 随机初始化质心方法的优化。

　　K-Means++ 对初始化质心的优化策略也很简单：
1. 从输入的数据集合中随机选择一个点作为第一个聚类中心 $\mu_1$；
2. 对于数据集中的每一个点 $x_i$，计算它与已选择的聚类中心中距离最近的聚类中心距离：

$$
D\left(x_{i}\right)=\arg \min \left\|x_{i}-\mu_{r}\right\|_{2}^{2} r=1,2, \ldots k_{\text {selected}}
$$

{start="3"}

1. 选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x)$ 较大的点，被选取到作为聚类中心的概率就越大。
2. 重复步骤 2 和步骤 3 直到选择出 $k$ 个聚类质心。
3. 利用这 $k$ 个质心来作为初始化质心去运行标准的 K-Means 算法。


## References
1. [K-Means聚类算法原理](https://www.cnblogs.com/pinard/p/6164214.html)