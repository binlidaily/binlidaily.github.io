---
layout: post
title: Feature Engineering
author: Bin Li
tags: [Machine Learning]
category: ""
comments: true
published: true
typora-root-url: ../../../../binlidaily.github.io
---

åœ¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­ï¼Œç‰¹å¾å·¥ç¨‹æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ•°æ®å’Œç‰¹å¾å†³å®šäº†æœºå™¨å­¦ä¹ ç®—æ³•çš„ä¸Šé™ï¼Œè€Œæ¨¡å‹çš„ç®—æ³•åªæ˜¯ä¸æ–­é€¼è¿‘è¿™ä¸ªä¸Šé™è€Œå·²ã€‚ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰ä»‹äºâ€œæ•°æ®â€å’Œâ€œæ¨¡å‹â€ä¹‹é—´ï¼Œæ˜¯åˆ©ç”¨æ•°æ®çš„ä¸“ä¸šé¢†åŸŸçŸ¥è¯†å’Œç°æœ‰æ•°æ®ï¼Œä»æºæ•°æ®ä¸­æŠ½å–å‡ºæ¥å¯¹é¢„æµ‹ç»“æœæœ‰ç”¨çš„ä¿¡æ¯ï¼Œç”¨åœ¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸Šçš„è¿‡ç¨‹ã€‚ç¾å›½è®¡ç®—æœºç§‘å­¦å®¶ Peter Norvig æœ‰ä¸¤å¥ç»å…¸çš„åè¨€ï¼š
* â€œåŸºäºå¤§é‡æ•°æ®çš„ç®€å•æ¨¡å‹èƒœè¿‡åŸºäºå°‘é‡æ•°æ®çš„å¤æ‚æ¨¡å‹ã€‚â€
* â€œæ›´å¤šçš„æ•°æ®èƒœè¿‡èªæ˜çš„ç®—æ³•ï¼Œè€Œå¥½çš„æ•°æ®èƒœè¿‡å¤šçš„æ•°æ®ã€‚â€

å´æ©è¾¾æ›´æ˜¯è¯´è¿‡â€œåº”ç”¨æœºå™¨å­¦ä¹ åŸºæœ¬ä¸Šå°±æ˜¯ç‰¹å¾å·¥ç¨‹â€ã€‚å¯¹äºå·¥ä¸šç•Œæ¥è¯´ï¼Œå¤§éƒ¨åˆ†å¤æ‚æ¨¡å‹çš„ç®—æ³•ç²¾è¿›éƒ½æ˜¯èµ„æ·±çš„æ•°æ®ç§‘å­¦å®¶çš„ä»»åŠ¡ï¼Œå¤§éƒ¨åˆ†äººå‘˜çš„å·¥ä½œè¿˜æ˜¯è·‘æ•°æ®ã€map-reduceï¼Œhive SQLï¼Œæ•°æ®ä»“åº“æ¬ç –ï¼Œåšä¸€äº›ä¸šåŠ¡åˆ†æã€æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ï¼ˆæ‰¾ç‰¹å¾ï¼‰çš„å·¥ä½œã€‚

ç‰¹å¾å·¥ç¨‹ä¸€èˆ¬åˆ†æˆç‰¹å¾æå– (Feature Extraction) å’Œç‰¹å¾é€‰æ‹© (Feature Selection) ä¸¤ä¸ªæ–¹é¢ï¼Œæ¥ä¸‹æ¥åˆ†åˆ«æ›´ç»†è‡´åœ°ä»‹ç»:

## ç‰¹å¾æå– (Feature Extraction) 
ç‰¹å¾çš„æŒ–æ˜ä¸€èˆ¬è·Ÿä¸“ä¸šé¢†åŸŸçŸ¥è¯†å¼ºç›¸å…³ï¼Œç‰¹å¾å·¥ç¨‹å¯ä»¥è¯´æ˜¯ä¸šåŠ¡é€»è¾‘çš„ä¸€ç§æ•°æ®å±‚é¢çš„è¡¨ç¤ºã€‚

### 1. æ¢ç´¢æ€§æ•°æ®åˆ†æ (Exploratory Data Analysis, EDA)
EDA çš„ç›®çš„æ˜¯å°½å¯èƒ½åœ°æ´å¯Ÿæ•°æ®é›†ã€å‘ç°æ•°æ®çš„å†…éƒ¨ç»“æ„ã€æå–é‡è¦çš„ç‰¹å¾ã€æ£€æŸ¥å¼‚å¸¸å€¼ã€æ£€éªŒåŸºæœ¬å‡è®¾ã€å»ºç«‹åˆæ­¥çš„æ¨¡å‹ã€‚EDA æŠ€æœ¯ä¸€èˆ¬åˆ†ä¸ºä¸¤ç±»ï¼š
* å¯è§†åŒ–æŠ€æœ¯
    * ç®±å‹å›¾ã€ç›´æ–¹å›¾ã€å¤šå˜é‡å›¾ã€é“¾å›¾ã€å¸•ç´¯æ‰˜å›¾ã€æ•£ç‚¹å›¾ã€èŒå¶å›¾
    * å¹³è¡Œåæ ‡ã€è®©æ­¥æ¯”ã€å¤šç»´å°ºåº¦åˆ†æã€ç›®æ ‡æŠ•å½±è¿½è¸ª
    * ä¸»æˆåˆ†åˆ†æã€é™ç»´ã€éçº¿æ€§é™ç»´ç­‰
* å®šé‡æŠ€æœ¯
    * æ ·æœ¬å‡å€¼ã€æ–¹å·®åˆ†ä½æ•°ã€å³°åº¦ã€ååº¦ç­‰

ç»†èŠ‚å¯ä»¥å‚è€ƒä¹‹å‰æ•´ç†è¿‡çš„æœ‰å…³ EDA çš„[åšæ–‡](https://binlidaily.github.io/2019-01-10-exploratory-data-analysis/)ã€‚

### 2. æ•°å€¼ç‰¹å¾ /å®šé‡ç‰¹å¾ (Numerical Features)
å¯¹äºæ•°å€¼ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸»è¦è€ƒè™‘çš„å› ç´ æ˜¯å®ƒçš„**å¤§å°å’Œåˆ†å¸ƒ**ï¼Œä¸€èˆ¬åˆ†ä¸º`è¿ç»­å‹`ï¼ˆèº«é«˜ä½“é‡ç­‰ï¼‰å’Œ`ç¦»æ•£å‹`ï¼ˆè®¡æ•°ç­‰ï¼‰ã€‚å¯¹äºé‚£äº›ç›®æ ‡å˜é‡ä¸ºè¾“å…¥ç‰¹å¾çš„**å…‰æ»‘å‡½æ•°**çš„æ¨¡å‹ï¼Œä¾‹å¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ç­‰ï¼Œå…¶å¯¹è¾“å…¥ç‰¹å¾çš„å¤§å°å¾ˆæ•æ„Ÿï¼Œæ‰€ä»¥éœ€è¦å½’ä¸€åŒ–ã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬éœ€è¦è¿›è¡Œç‰¹å¾å˜æ¢æ¥æ»¡è¶³éçº¿æ€§æ¨¡å‹çš„å‡è®¾ï¼Œè¿˜å¯ä»¥è¿›è¡Œç‰¹å¾äº¤å‰æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè®©çº¿æ€§æ¨¡å‹å…·æœ‰éçº¿æ€§æ¨¡å‹çš„æ€§è´¨ã€‚ä»¥ä¸‹ä»‹ç»å‡ ç§å¸¸è§çš„æ•°å€¼ç‰¹å¾çš„å¤„ç†æ–¹æ³•ã€‚

### 2.1 æˆªæ–­ / ç¦»ç¾¤ç‚¹ç›–å¸½ï¼Ÿ
* å¯¹äºè¿ç»­å‹æ•°å€¼ç‰¹å¾ï¼Œè¶…å‡ºåˆç†èŒƒå›´çš„å¾ˆå¯èƒ½æ˜¯å™ªå£°ï¼Œéœ€è¦æˆªæ–­
* åœ¨ä¿ç•™é‡è¦ä¿¡æ¯çš„å‰æä¸‹è¿›è¡Œæˆªæ–­ï¼Œæˆªæ–­åçš„ä¹Ÿå¯ä½œä¸ºç±»åˆ«ç‰¹å¾
* é•¿å°¾æ•°æ®å¯ä»¥å…ˆè¿›è¡Œå¯¹æ•°å˜æ¢ï¼Œå†æˆªæ–­

ä¸€èˆ¬çš„åšæ³•æ˜¯åœ¨ EDA åçœ‹åˆ°æŸç‰¹å¾æœ‰ä¸€äº›ç¦»ç¾¤ç‚¹ï¼Œå°±å¯ä»¥ç”¨æˆªæ–­çš„æ–¹å¼å°†å…¶å¤„ç†ä¸€ä¸‹ï¼š
```python
up_limit = np.percentile(data_df[col].values, 99.9) # 99.9%åˆ†ä½æ•°
low_limit = np.percentile(data_df[col].values, 0.1) # 0.1%åˆ†ä½æ•°
data_df.loc[data_df[col] > up_limit, col] = up_limit
data_df.loc[data_df[col] < low_limit, col] = low_limit
```

ä¾‹å­ï¼šå°†è¿™äº›åŸå§‹å¹´é¾„å€¼é™¤ä»¥ 10ï¼Œç„¶åé€šè¿‡ floor å‡½æ•°å¯¹åŸå§‹å¹´é¾„æ•°å€¼è¿›è¡Œæˆªæ–­ã€‚

```python
fcc_survey_df['Age_bin_round'] = np.array(np.floor(np.array(fcc_survey_df['Age']) / 10.))
fcc_survey_df[['ID.x', 'Age','Age_bin_round']].iloc[1071:1076]
```

![](/img/media/15523795162029.jpg)

è¿™æ ·è¿ç»­æ•°å€¼å°±æ²¡æœ‰é‚£ä¹ˆç²¾ç»†äº†ï¼Œä¹Ÿèƒ½åæ˜ å‡ºç›¸äº’ä¹‹é—´çš„å·®åˆ«ã€‚

### 2.2 ç¦»æ•£åŒ–ï¼ˆDiscretizationï¼‰

ç¦»æ•£åŒ–åˆè¢«ç§°ä¸ºé‡åŒ–æˆ–è€…å«åšåˆ†æ¡¶ï¼ˆäºŒå€¼åŒ–ä¹Ÿæ˜¯ä¸€ç§åˆ†æ¡¶ï¼‰ï¼Œæ˜¯ä¸€ç§å°†è¿ç»­å‹ç‰¹å¾è½¬æ¢åˆ°ç¦»æ•£ç‰¹å¾ä¸Šçš„ä¸€ç§æ–¹å¼ï¼Œè€Œç¦»æ•£ç‰¹å¾å¯ä»¥è¢«ç”¨åšç±»åˆ«ç‰¹å¾ï¼Œè¿™å¯¹å¤§å¤šæ•°æ¨¡å‹æ¥è¯´æ¯”è¾ƒå‹å¥½ã€‚é€šè¿‡ç¦»æ•£åŒ–ç”šè‡³å¯ä»¥å°†éçº¿æ€§ç‰¹æ€§å¼•å…¥åˆ°çº¿æ€§æ¨¡å‹ä¸­ï¼Œä»è€Œä½¿å¾—çº¿æ€§æ¨¡å‹æ›´å…·æ³›åŒ–æ€§ã€‚

### 2.2.1 äºŒå€¼åŒ– (Binarization)
è®¡æ•°ç‰¹å¾å¯ä»¥è€ƒè™‘è½¬æ¢ä¸ºæ˜¯å¦çš„äºŒå€¼åŒ–å½¢å¼ï¼ŒåŸºäºè¦è§£å†³çš„é—®é¢˜æ„å»ºæ¨¡å‹æ—¶ï¼Œé€šå¸¸åŸå§‹é¢‘æ•°æˆ–æ€»æ•°å¯èƒ½ä¸æ­¤ä¸ç›¸å…³ã€‚æ¯”å¦‚å¦‚æœæˆ‘è¦å»ºç«‹ä¸€ä¸ªæ¨èç³»ç»Ÿç”¨æ¥æ¨èæ­Œæ›²ï¼Œæˆ‘åªå¸Œæœ›çŸ¥é“ä¸€ä¸ªäººæ˜¯å¦æ„Ÿå…´è¶£æˆ–æ˜¯å¦å¬è¿‡æŸæ­Œæ›²ã€‚æˆ‘ä¸éœ€è¦çŸ¥é“ä¸€é¦–æ­Œè¢«å¬è¿‡çš„æ¬¡æ•°ï¼Œå› ä¸ºæˆ‘æ›´å…³å¿ƒçš„æ˜¯ä¸€ä¸ªäººæ‰€å¬è¿‡çš„å„ç§å„æ ·çš„æ­Œæ›²ã€‚

```python
watched = np.array(popsong_df['listen_count'])
watched[watched >= 1] = 1
popsong_df['watched'] = watched
# å½“ç„¶ä¹Ÿå¯ä»¥ç”¨ Pandas ä¸­ DataFrame çš„æ–¹å¼
popsong_df['watched'] = 0
popsong_df.loc[popsong_df['listen_count'] >= 1, 'watched'] = 1
```

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ scikit-learn ä¸­ preprocessing æ¨¡å—çš„ Binarizer ç±»æ¥æ‰§è¡ŒåŒæ ·çš„ä»»åŠ¡ï¼Œè€Œä¸ä¸€å®šä½¿ç”¨ numpy æ•°ç»„ã€‚

```python
from sklearn.preprocessing import Binarizer
bn = Binarizer(threshold=0.9)
pd_watched =bn.transform([popsong_df['listen_count']])[0]
popsong_df['pd_watched'] = pd_watched
popsong_df.head(11)
```
![](/img/media/15523797913264.jpg)



### 2.2.2 åˆ†æ¡¶ (Binning) 
å¦‚æœç›´æ¥åˆ©ç”¨åŸå§‹çš„è¿ç»­æ•°å€¼å‹ç‰¹å¾æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯è¿™äº›ç‰¹å¾çš„æ•°å€¼**åˆ†å¸ƒ**é€šå¸¸æ˜¯æœ‰åå‘çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰äº›æ•°æ®ç‰¹åˆ«å¤šè€Œä¸€äº›å€¼å°±ç›¸å¯¹å¾ˆå°‘å‡ºç°ã€‚å¦å¤–ï¼Œè¿™äº›ç‰¹å¾çš„**å¤§å°**å˜åŒ–èŒƒå›´ä¹Ÿæ˜¯éœ€è¦æ³¨æ„çš„ã€‚å¦‚æœç›´æ¥åˆ©ç”¨è¿™äº›ç‰¹å¾ï¼Œæ¨¡å‹çš„æ•ˆæœä¸€èˆ¬ä¸å¥½ï¼Œäºæ˜¯éœ€è¦å¤„ç†è¿™äº›ç‰¹å¾ï¼Œæœ‰åˆ†æ¡¶å’Œå˜æ¢çš„æ–¹å¼ã€‚

å¯¹éœ€è¦åˆ†æ¡¶çš„æƒ…å†µåšä¸€ä¸ªç»éªŒæ€§çš„æ€»ç»“ï¼š
* è¿ç»­å‹æ•°å€¼ç‰¹å¾çš„æ•°å€¼åˆ†å¸ƒæœ‰åå‘çš„å¯ä»¥åˆ†æ¡¶
* ç¦»æ•£å‹æ•°å€¼ç‰¹å¾çš„æ•°å€¼è·¨è¶Šäº†ä¸åŒçš„æ•°é‡çº§å¯ä»¥åˆ†æ¡¶

åˆ†æ¡¶å¯ä»¥å°†è¿ç»­æ€§æ•°å€¼ç‰¹å¾è½¬æ¢ä¸ºç¦»æ•£å‹ç‰¹å¾ï¼ˆç±»åˆ«ï¼‰ï¼Œæ¯ä¸€ä¸ªæ¡¶ä»£è¡¨äº†æŸä¸€ä¸ªèŒƒå›´çš„è¿ç»­æ€§æ•°å€¼ç‰¹å¾çš„å¯†åº¦ã€‚

### 2.2.2.1 å›ºå®šå®½åº¦åˆ†æ¡¶ (Fixed-Width Binning)

å›ºå®šæ¯ä¸ªåˆ†æ¡¶çš„å®½åº¦ï¼Œå³æ¯ä¸ªæ¡¶çš„å€¼åŸŸæ˜¯å›ºå®šçš„ï¼Œå¦‚æœæ¯ä¸ªæ¡¶çš„å¤§å°ä¸€æ ·ï¼Œä¹Ÿç§°ä¸ºå‡åŒ€åˆ†æ¡¶ã€‚è¿™é‡Œç”¨å¹´é¾„ä½œä¸ºä¾‹å­è¿›è¡Œè¯´æ˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºå¹´é¾„æœ‰ä¸€ç‚¹å³åçš„æ•°æ®åˆ†å¸ƒï¼š

![](/img/media/15523887500985.jpg)

æˆ‘ä»¬å°è¯•ç”¨å¦‚ä¸‹çš„å›ºå®šå®½åº¦æ¥åˆ†æ¡¶ï¼š
```shell
Age Range: Bin
---------------
 0 -  9  : 0
10 - 19  : 1
20 - 29  : 2
30 - 39  : 3
40 - 49  : 4
50 - 59  : 5
60 - 69  : 6
  ... and so on
```

1ã€å¦‚æœé‡‡ç”¨æ•°æ®èˆå…¥çš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æµ®ç‚¹å‹çš„å¹´é¾„ç‰¹å¾é™¤ä»¥10ï¼š

```python
fcc_survey_df['Age_bin_round'] = np.array(np.floor(
                              np.array(fcc_survey_df['Age']) / 10.))
fcc_survey_df[['ID.x', 'Age', 'Age_bin_round']].iloc[1071:1076]
```

![](/img/media/15523898042213.jpg)

2ã€é‚£å¦‚æœæˆ‘ä»¬éœ€è¦æƒ³è¦æ›´çµæ´»çš„æ–¹å¼ï¼ˆæŒ‰ç…§è‡ªå·±çš„æ„æ„¿ï¼‰æ¥æ“ä½œè¦æ€ä¹ˆåšå‘¢ï¼Ÿæ¯”å¦‚è¿™æ ·åˆ†æ¡¶ï¼š

```python
Age Range : Bin
---------------
 0 -  15  : 1
16 -  30  : 2
31 -  45  : 3
46 -  60  : 4
61 -  75  : 5
75 - 100  : 6
```

å¯ä»¥ç”¨ Pandas çš„ cut å‡½æ•°ï¼š
```python
bin_ranges = [0, 15, 30, 45, 60, 75, 100]
bin_names = [1, 2, 3, 4, 5, 6]
fcc_survey_df['Age_bin_custom_range'] = pd.cut(
                                           np.array(
                                              fcc_survey_df['Age']), 
                                              bins=bin_ranges)
fcc_survey_df['Age_bin_custom_label'] = pd.cut(
                                           np.array(
                                              fcc_survey_df['Age']), 
                                              bins=bin_ranges,            
                                              labels=bin_names)
# view the binned features 
fcc_survey_df[['ID.x', 'Age', 'Age_bin_round', 
               'Age_bin_custom_range',   
               'Age_bin_custom_label']].iloc[10a71:1076]
```

![](/img/media/15523901338278.jpg)

3ã€å¯ä»¥é‡‡ç”¨ [sklearn.preprocessing.KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) çš„æ–¹å¼ï¼š

```python
>>> X = [[-2, 1, -4,   -1],
...      [-1, 2, -3, -0.5],
...      [ 0, 3, -2,  0.5],
...      [ 1, 4, -1,    2]]
>>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
>>> est.fit(X)  
KBinsDiscretizer(...)
>>> Xt = est.transform(X)
>>> Xt  
array([[ 0., 0., 0., 0.],
       [ 1., 1., 1., 0.],
       [ 2., 2., 2., 1.],
       [ 2., 2., 2., 2.]])
# çœ‹ä¸‹åˆ†æ¡¶è¾¹ç•Œ
>>> est.bin_edges_[0]
array([-2., -1.,  0.,  1.])
>>> est.inverse_transform(Xt)
array([[-1.5,  1.5, -3.5, -0.5],
       [-0.5,  2.5, -2.5, -0.5],
       [ 0.5,  3.5, -1.5,  0.5],
       [ 0.5,  3.5, -1.5,  1.5]])
```



### 2.2.2.2 è‡ªå®šä¹‰åˆ†æ¡¶

1ã€è‡ªå®šä¹‰åˆ†æ¡¶å¯ä»¥åˆ©ç”¨ä¸Šé¢å›ºå®šå®½åº¦åˆ†æ¡¶çš„æœ€åä¸€ç§æ–¹å¼ï¼Œä¿®æ”¹æˆè‡ªå·±æƒ³è¦çš„åˆ†æ¡¶é—´éš”å°±å¥½ã€‚

2ã€ä¹Ÿå¯ä»¥é‡‡ç”¨ Pandas çš„ map æ–¹å¼ï¼š

```python
def map_age(age_x):
    if age_x <= 18:
        return 1
    elif x <= 20:
        return 2
    elif x <= 35:
        return 3
    elif x <= 45:
        return 4
    else:
        return 5

data_df['age'] = data_df['age'].map(lambda x : map_age(x))
```

### 2.2.2.3 è‡ªé€‚åº”åˆ†æ¡¶ / åˆ†ä½æ•°åˆ†æ¡¶ (Adaptive Binning)

ä¸ç®¡æ˜¯å›ºå®šå®½åº¦åˆ†æ¡¶è¿˜æ˜¯è‡ªå®šä¹‰åˆ†æ¡¶ï¼Œåˆ†æ¡¶çš„æ•ˆæœéƒ½å¾ˆéš¾ä½¿å¾—ç»“æœèƒ½å¤Ÿå‘ˆç°å‡åŒ€åˆ†å¸ƒï¼Œæœ‰çš„æ¡¶å¤šï¼Œæœ‰çš„æ¡¶å¾ˆå°‘ç”šè‡³ä¸ºç©ºã€‚äºæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨åˆ†ä½æ•°åˆ†æ¡¶æ¥è‡ªé€‚åº”åœ°åšåˆ’åˆ†ï¼Œä½¿å¾—ç»“æœæ›´åŠ å‡åŒ€ä¸€äº›ã€‚ä¸€èˆ¬å¸¸ç”¨çš„æœ‰2åˆ†ä½ç‚¹ï¼Œ4åˆ†ä½ç‚¹å’Œ10åˆ†ä½ç‚¹ç”¨ä»¥åˆ†æ¡¶ã€‚

![](/img/media/15523923167184.jpg)

è§‚å¯Ÿæ•°æ®å¯ä»¥çœ‹å‡ºæœ‰ä¸€å®šå³åçš„è¶‹åŠ¿ï¼Œæˆ‘ä»¬å…ˆåˆ©ç”¨å››åˆ†ä½ç‚¹çœ‹ä¸‹æ•°æ®æƒ…å†µï¼š

```python
quantile_list = [0, .25, .5, .75, 1.]
quantiles = fcc_survey_df['Income'].quantile(quantile_list)
quantiles

# Output
------
0.00      6000.0
0.25     20000.0
0.50     37000.0
0.75     60000.0
1.00    200000.0
Name: Income, dtype: float64
```

åœ¨æŸ±çŠ¶å›¾ä¸Šç”»å‡ºåˆ†ä½ç‚¹æ ‡çº¿ï¼š

```python
fig, ax = plt.subplots()
fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3', 
                             edgecolor='black', grid=False)
for quantile in quantiles:
    qvl = plt.axvline(quantile, color='r')
ax.legend([qvl], ['Quantiles'], fontsize=10)
ax.set_title('Developer Income Histogram with Quantiles', 
             fontsize=12)
ax.set_xlabel('Developer Income', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
```

![](/img/media/15523937291720.jpg)


åˆ©ç”¨ qcut åŸºäºåˆ†ä½ç‚¹æ¥åˆ†æ¡¶ï¼š
```python
quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']
fcc_survey_df['Income_quantile_range'] = pd.qcut(
                                            fcc_survey_df['Income'], 
                                            q=quantile_list)
fcc_survey_df['Income_quantile_label'] = pd.qcut(
                                            fcc_survey_df['Income'], 
                                            q=quantile_list,       
                                            labels=quantile_labels)

fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_quantile_range', 
               'Income_quantile_label']].iloc[4:9]
```
![](/img/media/15523938726097.jpg)

å½“ç„¶ï¼Œåˆ†æ¡¶ä¹‹åå¾—åˆ°äº†ç¦»æ•£å‹çš„æ•°å€¼å‹ç‰¹å¾ï¼Œæˆ–è€…å¯ä»¥çœ‹æˆç±»åˆ«ç‰¹å¾ï¼Œè¿˜éœ€è¦ä¸€å®šçš„å¤„ç†æ‰èƒ½æ›´å¥½åœ°æœåŠ¡äºæ¨¡å‹ã€‚

### 2.3 æ•°æ®èˆå…¥(Rounding)

å¤„ç†è¿ç»­æ€§æ•°æ®ç‰¹å¾å¦‚æ¯”ä¾‹æˆ–è€…ç™¾åˆ†æ¯”ç±»å‹çš„ç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é«˜ç²¾åº¦çš„åŸå§‹æ•°å€¼ï¼Œé€šå¸¸æˆ‘ä»¬å°†å…¶èˆå…¥è¿‘ä¼¼åˆ°æ•°å€¼æ•´å‹å°±å¤Ÿç”¨äº†ï¼Œè¿™äº›æ•´å‹æ•°å€¼å¯ä»¥è¢«è§†ä½œç±»åˆ«ç‰¹å¾æˆ–è€…åŸå§‹æ•°å€¼ï¼ˆå³ç¦»æ•£ç‰¹å¾ï¼‰éƒ½å¯ä»¥ã€‚

ä¸¾ä¸ªä¾‹å­ï¼š

```python
items_popularity = pd.read_csv('datasets/item_popularity.csv',  
                               encoding='utf-8')
items_popularity['popularity_scale_10'] = np.array(
                               np.round((items_popularity['pop_percent'] * 10)),  
                               dtype='int')
items_popularity['popularity_scale_100'] = np.array(
                               np.round((items_popularity['pop_percent'] * 100)),    
                               dtype='int')
items_popularity
```

![](/img/media/15523893068452.jpg)

å¯ä»¥å¾—åˆ°ä¸åŒç²’åº¦ä¸‹çš„è¿‘ä¼¼ç»“æœã€‚å½“ç„¶ï¼Œèˆå…¥è¿‘ä¼¼ç»“æœä¸ä¸€å®šéƒ½æ˜¯ä¹˜ä»¥æŸä¸ªæ•°ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢è®²åˆ†æ¡¶çš„æ—¶å€™å¯ä»¥çœ‹åˆ°ï¼Œå¯ä»¥ç”¨èˆå…¥è¿‘ä¼¼çš„æ–¹å¼æ¥åšï¼Œæ•ˆæœå¯ä»¥åˆ†æ¡¶ã€‚


### 2. å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰
ä½¿ç”¨ [sklearn.preprocessing.Normalizer](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) æ¥å½’ä¸€åŒ–ï¼ŒæŠŠæ¯ä¸€è¡Œæ•°æ®å½’ä¸€åŒ–ï¼Œä½¿ä¹‹æœ‰ unit normï¼Œnorm çš„ç§ç±»å¯ä»¥é€‰l1ã€l2æˆ–maxã€‚ä¸å…ç–«outlierã€‚

$$
\vec{x^{\prime}}=\frac{\vec{x}}{l(\vec{x})}
$$

å…¶ä¸­ $l$ è¡¨ç¤º $norm$ å‡½æ•°ã€‚

### 3. åŒºé—´ç¼©æ”¾ï¼ˆscalingï¼‰
ä½¿ç”¨ [sklearn.preprocessing.MaxAbsScaler](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)ï¼Œå°†ä¸€åˆ—çš„æ•°å€¼ï¼Œé™¤ä»¥è¿™ä¸€åˆ—çš„æœ€å¤§ç»å¯¹å€¼ã€‚ä¸å…ç–«outlierã€‚

$$
x^{\prime}=\frac{x}{\max (|X|)}
$$





### 2.5 ç¼©æ”¾
ç¼©æ”¾æ˜¯å°†æ•°å€¼å˜é‡ç¼©æ”¾åˆ°ä¸€ä¸ªç¡®å®šçš„èŒƒå›´ï¼ŒæŠŠæœ‰é‡çº²è¡¨è¾¾å¼å˜ä¸ºæ— é‡çº²è¡¨è¾¾å¼ã€‚

### 2.5.1 æ ‡å‡†åŒ–ç¼©æ”¾ (åˆç§° Z ç¼©æ”¾)
æ ‡å‡†åŒ–ï¼ˆæ— é‡é’¢åŒ–/ä¸­å¿ƒåŒ–ï¼‰æŠŠç‰¹å¾è½¬åŒ–ä¸ºæœä»æ ‡å‡†æ­£å¤ªåˆ†å¸ƒçš„å½¢å¼ï¼Œå…¶å®æ˜¯è®¡ç®—æ ‡å‡†åˆ†æ•° (Standard Score, Z-score)ï¼Œç»è¿‡å¤„ç†çš„æ•°æ®ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä½¿å¾—æ•°å€¼ç‰¹å¾çš„ç®—æœ¯å¹³å‡æ•°ä¸ºé›¶ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚

$$
x^\prime =\frac{x-\mu}{\sigma}
$$

å…¶ä¸­ $\mu$ ä¸ºæ‰€æœ‰æ ·æœ¬æ•°æ®çš„å‡å€¼ï¼Œ$\sigma$ ä¸ºæ‰€æœ‰æ ·æœ¬æ•°æ®çš„æ ‡å‡†å·®ã€‚

Sklearn æœ‰ä¸¤ç§æ–¹æ³•å®ç°ï¼š
1ã€ä½¿ç”¨ [sklearn.preprocessing.scale()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥å°†ç»™å®šæ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.scale(X)

>>> X_scaled                                          
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([ 0.,  0.,  0.])

>>> X_scaled.std(axis=0)
array([ 1.,  1.,  1.])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) ç±»ï¼Œä½¿ç”¨è¯¥ç±»çš„å¥½å¤„åœ¨äºå¯ä»¥ä¿å­˜è®­ç»ƒé›†ä¸­çš„å‚æ•°ï¼ˆå‡å€¼ã€æ–¹å·®ï¼‰ç›´æ¥ä½¿ç”¨å…¶å¯¹è±¡è½¬æ¢æµ‹è¯•é›†æ•°æ®ã€‚

```python
>>> scaler = preprocessing.StandardScaler().fit(X)
>>> scaler
StandardScaler(copy=True, with_mean=True, with_std=True)

>>> scaler.mean_                                      
array([ 1. ...,  0. ...,  0.33...])

>>> scaler.std_                                       
array([ 0.81...,  0.81...,  1.24...])

>>> scaler.transform(X)                               
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> # å¯ä»¥ç›´æ¥ä½¿ç”¨è®­ç»ƒé›†å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œè½¬æ¢
>>> scaler.transform([[-1.,  1., 0.]])                
array([[-2.44...,  1.22..., -0.26...]])
```

æ³¨æ„ï¼š
* è®¡ç®—æ—¶å¯¹æ¯ä¸ªç‰¹å¾åˆ†åˆ«è¿›è¡Œã€‚å°†æ•°æ®æŒ‰ç‰¹å¾ï¼ˆæŒ‰åˆ—è¿›è¡Œï¼‰å‡å»å…¶å‡å€¼ï¼Œå¹¶é™¤ä»¥å…¶æ–¹å·®ã€‚å¾—åˆ°çš„ç»“æœæ˜¯ï¼Œå¯¹äºæ¯ä¸ªç‰¹å¾æ¥è¯´æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨ 0 é™„è¿‘ï¼Œæ–¹å·®ä¸º 1ã€‚
* å¦‚æœä¸ªåˆ«ç‰¹å¾æˆ–å¤šæˆ–å°‘çœ‹èµ·æ¥ä¸æ˜¯å¾ˆåƒ**æ ‡å‡†æ­£æ€åˆ†å¸ƒ(å…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®)**ï¼Œé‚£ä¹ˆå®ƒä»¬çš„è¡¨ç°åŠ›å¯èƒ½ä¼šè¾ƒå·®ã€‚
* ä¸å…ç–« outlierï¼Ÿ
* å¯¹ç›®æ ‡å˜é‡ä¸ºè¾“å…¥ç‰¹å¾çš„å…‰æ»‘å‡½æ•°çš„æ¨¡å‹ï¼Œå…¶è¾“å…¥ç‰¹å¾çš„å¤§å°æ¯”è¾ƒæ•æ„Ÿï¼Œå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ç¼©æ”¾æ¯”è¾ƒæœ‰æ•ˆã€‚
* å¯¹äºç¨€ç–æ•°æ®ï¼Œå¯ä»¥æ¥å— scipy.sparse çš„çŸ©é˜µä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶æŒ‡å®šå‚æ•°with_mean=False å–æ¶ˆä¸­å¿ƒåŒ–ï¼ˆcentering æ˜¯ç ´åæ•°æ®ç¨€ç–æ€§çš„åŸå› ï¼‰ï¼Œwith_std=False åˆ™ä¸åš scaling å¤„ç†ã€‚

å¦‚æœæ•°å€¼ç‰¹å¾åˆ—ä¸­å­˜åœ¨æ•°å€¼æå¤§æˆ–æå°çš„ outlierï¼ˆé€šè¿‡EDAå‘ç°ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ [sklearn.preprocessing.RobustScaler](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) ï¼Œåº”è¯¥ä½¿ç”¨æ›´ç¨³å¥ï¼ˆrobustï¼‰çš„ç»Ÿè®¡æ•°æ®ï¼šç”¨ä¸­ä½æ•°è€Œä¸æ˜¯ç®—æœ¯å¹³å‡æ•°ï¼Œç”¨åˆ†ä½æ•°ï¼ˆquantileï¼‰è€Œä¸æ˜¯æ–¹å·®ã€‚è¿™ç§æ ‡å‡†åŒ–æ–¹æ³•æœ‰ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼šï¼ˆåˆ†ä½æ•°ä¸‹é™ï¼Œåˆ†ä½æ•°ä¸Šé™ï¼‰ï¼Œæœ€å¥½é€šè¿‡EDAçš„æ•°æ®å¯è§†åŒ–ç¡®å®šã€‚å…ç–« outlierã€‚

### 2.5.2 æœ€å¤§æœ€å°å€¼ç¼©æ”¾

æœ€å¤§æœ€å°ç¼©æ”¾æ˜¯å°†ç‰¹å¾ç¼©æ”¾åˆ°ç»™å®šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´ï¼Œé€šå¸¸åœ¨é›¶å’Œä¸€ä¹‹é—´ã€‚
$$
{x}^\prime=\frac{x-x_{Min}}{x_{Max}-x_{Min}}
$$
ä½¿ç”¨ [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) å®ç°ï¼š

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> min_max_scaler = preprocessing.MinMaxScaler()
>>> X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[ 0.5       ,  0.        ,  1.        ],
       [ 1.        ,  0.5       ,  0.33333333],
       [ 0.        ,  1.        ,  0.        ]])

>>> # å°†ç›¸åŒçš„ç¼©æ”¾åº”ç”¨åˆ°æµ‹è¯•é›†æ•°æ®ä¸­
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5       ,  0.        ,  1.66666667]])


>>> # ç¼©æ”¾å› å­ç­‰å±æ€§
>>> min_max_scaler.scale_                             
array([ 0.5       ,  0.5       ,  0.33...])

>>> min_max_scaler.min_                               
array([ 0.        ,  0.5       ,  0.33...])
```

å½“ç„¶ï¼Œåœ¨æ„é€ ç±»å¯¹è±¡çš„æ—¶å€™ä¹Ÿå¯ä»¥ç›´æ¥æŒ‡å®šæœ€å¤§æœ€å°å€¼çš„èŒƒå›´ï¼šfeature_range=(min, max)ï¼Œæ­¤æ—¶åº”ç”¨çš„å…¬å¼å˜ä¸ºï¼š

```python
X_std=(X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0))
X_scaled=X_std/(max-min)+min
```

ğŸ½ æ³¨æ„ï¼š
* è¿™ç§å½’ä¸€åŒ–æ–¹æ³•æ¯”è¾ƒé€‚ç”¨åœ¨æ•°å€¼æ¯”è¾ƒé›†ä¸­çš„æƒ…å†µã€‚
* ä¸¤ä¸ªç¼ºé™·ï¼š
  * å½“æœ‰æ–°æ•°æ®åŠ å…¥æ—¶ï¼Œå¯èƒ½å¯¼è‡´ max å’Œ min å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°å®šä¹‰ã€‚
  * å¦‚æœ max å’Œ min ä¸ç¨³å®šï¼Œå¾ˆå®¹æ˜“ä½¿å¾—å½’ä¸€åŒ–ç»“æœä¸ç¨³å®šï¼Œä½¿å¾—åç»­ä½¿ç”¨æ•ˆæœä¹Ÿä¸ç¨³å®šã€‚å®é™…ä½¿ç”¨ä¸­å¯ä»¥ç”¨ç»éªŒå¸¸é‡å€¼æ¥æ›¿ä»£ max å’Œ minã€‚

### 2.5. æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾

åœ¨å®é™…æƒ…å†µä¸­,æˆ‘ä»¬ç»å¸¸å¿½ç•¥ç‰¹å¾çš„åˆ†å¸ƒå½¢çŠ¶ï¼Œç›´æ¥ç»è¿‡å»å‡å€¼æ¥å¯¹æŸä¸ªç‰¹å¾è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œå†é€šè¿‡é™¤ä»¥éå¸¸é‡ç‰¹å¾(non-constant features)çš„æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾ã€‚è€Œå¯¹ç¨€ç–æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–ä¼šç ´åç¨€ç–æ•°æ®çš„ç»“æ„ï¼Œè¿™æ ·åšæ²¡ä»€ä¹ˆæ„ä¹‰ã€‚ä½†å¦‚æœç¨€ç–æ•°æ®çš„ç‰¹å¾è·¨è¶Šä¸åŒæ•°é‡çº§çš„æƒ…å†µä¸‹ä¹Ÿæœ€å¥½è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾å°±å¯ä»¥æ´¾ä¸Šç”¨åœºäº†ã€‚

æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾æŒ‰ç…§æ¯ä¸ªç‰¹å¾çš„æœ€å¤§ç»å¯¹å€¼è¿›è¡Œç¼©æ”¾ï¼ˆé™¤ä»¥æœ€å¤§ç»å¯¹å€¼ï¼‰ï¼Œä½¿å¾—æ¯ä¸ªç‰¹å¾çš„èŒƒå›´å˜æˆäº† $[-1, 1]$ï¼Œè¯¥æ“ä½œä¸ä¼šç§»åŠ¨æˆ–è€…å±…ä¸­æ•°æ®ï¼Œæ‰€ä»¥ä¸ä¼šç ´åç¨€ç–æ€§ã€‚

ä½¿ç”¨ [sklearn.preprocessing.MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler) å®ç°ï¼š

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> max_abs_scaler = preprocessing.MaxAbsScaler()
>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
>>> X_train_maxabs                # doctest +NORMALIZE_WHITESPACE^
array([[ 0.5, -1\. ,  1\. ],
 [ 1\. ,  0\. ,  0\. ],
 [ 0\. ,  1\. , -0.5]])
# æµ‹è¯•é›†
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_maxabs = max_abs_scaler.transform(X_test)
>>> X_test_maxabs                 
array([[-1.5, -1\. ,  2\. ]])
>>> max_abs_scaler.scale_         
array([ 2.,  1.,  2.])
```

ğŸ½æ³¨æ„ï¼š
* ä½¿ç”¨æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ä¹‹å‰åº”è¯¥ç¡®è®¤ï¼Œè®­ç»ƒæ•°æ®åº”è¯¥æ˜¯å·²ç»é›¶ä¸­å¿ƒåŒ–æˆ–è€…æ˜¯ç¨€ç–æ•°æ®ã€‚
* è¯¥æ“ä½œä¸ä¼šç§»åŠ¨æˆ–è€…å±…ä¸­æ•°æ®ï¼Œæ‰€ä»¥ä¸ä¼šç ´åç¨€ç–æ€§ã€‚

æœ€å¤§æœ€å°å€¼ç¼©æ”¾å’Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ä¸¤ç§ç¼©æ”¾å±äº**åŒºé—´ç¼©æ”¾**ï¼Œä½¿ç”¨è¿™ç§ç¼©æ”¾çš„ç›®çš„åŒ…æ‹¬å®ç°ç‰¹å¾æå°æ–¹å·®çš„é²æ£’æ€§ä»¥åŠåœ¨ç¨€ç–çŸ©é˜µä¸­ä¿ç•™é›¶å…ƒç´ ã€‚

### 2.5.3 åŸºäºæŸç§èŒƒæ•°çš„ç¼©æ”¾

### 2.5.4 å¹³æ–¹æ ¹ç¼©æ”¾æˆ–è€…å¯¹æ•°ç¼©æ”¾

### 2.5.5 Box-Cox è½¬æ¢


### å¸¦æœ‰å¼‚å¸¸å€¼çš„ç¼©æ”¾

### ç¨€ç–æ•°æ®çš„ç¼©æ”¾






### ç¼ºå¤±å€¼å¤„ç†
* è¡¥å€¼
    * ç®€å•çš„å¯ä»¥æ˜¯è¡¥ä¸€ä¸ªå¹³å‡å€¼ã€æˆ–è€…ä¼—æ•°
    * å¯¹äºå«å¼‚å¸¸å€¼çš„å˜é‡ï¼Œæ›´å¥å£®çš„åšæ³•æ˜¯è¡¥ä¸­ä½æ•°
    * è¿˜å¯ä»¥é€šè¿‡æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼
* ç›´æ¥å¿½ç•¥
    * å°†ç¼ºå¤±ä½œä¸ºä¸€ç§ä¿¡æ¯ç¼–ç å–‚ç»™æ¨¡å‹è¿›è¡Œå­¦ä¹ 

### ç‰¹å¾äº¤å‰
* è¡¨ç¤ºæ•°å€¼ç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨
* å¯ä»¥å¯¹ä¸¤ä¸ªæ•°å€¼å˜é‡è¿›è¡ŒåŠ ã€å‡ã€ä¹˜ã€é™¤ç­‰æ“ä½œ

### éçº¿æ€§ç¼–ç 
* å¤šé¡¹å¼æ ¸ã€é«˜æ–¯æ ¸ç­‰ç¼–ç 
* å°†éšæœºæ£®æ—æ¨¡å‹çš„å¶èŠ‚ç‚¹è¿›è¡Œç¼–ç å–‚ç»™çº¿æ€§æ¨¡å‹
* åŸºå› ç®—æ³•ä»¥åŠå±€éƒ¨çº¿æ€§åµŒå…¥ã€è°±åµŒå…¥ã€t-SNE ç­‰

### è¡Œç»Ÿè®¡é‡
* å¯¹è¡Œå‘é‡è¿›è¡Œç»Ÿè®¡ä½œä¸ºä¸€ç±»ç‰¹å¾
* ä¾‹å¦‚ç»Ÿè®¡è¡Œå‘é‡ä¸­çš„ç©ºå€¼ä¸ªæ•°ã€é›¶å€¼ä¸ªæ•°ã€æ­£è´Ÿå€¼ä¸ªæ•°
* ä»¥åŠå‡å€¼ã€æ–¹å·®ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€ååº¦ã€å³°åº¦ç­‰







## ç‰¹å¾é€‰æ‹© (Feature Selection) 




![](/img/media/15427027069088.jpg)

Features:
1. numeric
2. categorical
3. ordinal
4. datetime
5. coordinates

å±‚æ¬¡åŒ–ç‰¹å¾ï¼Œå¾®è§‚ç‰¹å¾ï¼Œå®è§‚ç‰¹å¾ã€‚

## ç‰¹å¾å·¥ç¨‹-åˆ›é€ ç‰¹å¾

### 1)Â Â å¥½çš„ç‰¹å¾
å¥½çš„ç‰¹å¾ä»¥åŠæ•°æ®æ ·æœ¬å†³å®šæˆ‘ä»¬æ¨¡å‹ä¼˜åŒ–çš„ä¸Šé™ï¼Œæ‰€ä»¥æ‰¾åˆ°å¥½çš„ç‰¹å¾éå¸¸é‡è¦ã€‚å¥½çš„ç‰¹å¾æ¥æºäºå¯¹ä¸šåŠ¡çš„æ·±å…¥ç†è§£ã€‚é¦–å…ˆè‡ªå·±è¦æ·±å…¥ç†è§£ä¸šåŠ¡çš„è¿ä½œæ–¹å¼ï¼Œäº†è§£å½±å“æ¨¡å‹ label ç›®æ ‡çš„ä¸»è¦ä¸šåŠ¡å› ç´ ï¼›å…¶æ¬¡å¤šå’Œä¸šåŠ¡çš„ä¸“å®¶æ²Ÿé€šï¼Œè·å–åˆ°ä»ä»–ä»¬è§’åº¦è®¤ä¸ºé‡è¦çš„å› ç´ ï¼›æ‹‰å…¥æ›´å¤šäººå‘˜è¿›è¡Œå¤´è„‘é£æš´ï¼Œæ‰¾åˆ°å°½å¯èƒ½å¤šçš„å½±å“å› ç´ ã€‚

ä¸åŒç‰¹å¾å½“å‰å¯ç”¨æ€§ä¹Ÿä¸ä¸€æ ·ã€‚ åˆæœŸæˆ‘ä»¬è¦æ›´å¤šå…³æ³¨é‚£äº›å·²æœ‰æ•°æ®ã€çº¿ä¸Šæ˜“è·å–çš„ç‰¹å¾ï¼›ç„¶åå¯¹äºä¸€äº›æˆ‘ä»¬æ’åºå‡ºæ¥é‡è¦å› ç´ ï¼Œå¦‚æœå½“å‰æ²¡æœ‰æ•°æ®ä»¥åŠçº¿ä¸Šæ— æ³•è·å–ï¼Œæˆ‘ä»¬è¦å°½å¿«å‡†å¤‡ï¼›

å…·ä½“ç‰¹å¾éƒ½æ˜¯ä¸šåŠ¡ç›¸å…³çš„ï¼Œå®è§‚ä¸Šè®²ä¸€äº›å¯èƒ½çš„æ–¹å‘ä¾›å‚è€ƒï¼Œä¾‹å¦‚ç”¨æˆ·ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­å¯ä»¥æ„ŸçŸ¥çš„å› ç´ çš„å±æ€§ã€ç”¨æˆ·å†å²çš„ä¸šåŠ¡æ•°æ®å› ç´ ã€æ—¶é—´å› ç´ ã€åœ°åŸŸå› ç´ ã€ç”¨æˆ·çš„ä¸ªæ€§åŒ–å› ç´ ï¼ˆå¹´é¾„ã€çˆ±å¥½ç­‰ï¼‰ã€ç”¨æˆ·ä½¿ç”¨åœºæ™¯ä¸­å„ç§å†å²æ²‰æ·€çš„è¯„åˆ†å› ç´ ï¼ˆå¥½è¯„ã€å·®è¯„ç­‰æ•°é‡ï¼‰ã€åœºæ™¯å„ç§å¯¹è±¡çš„å±æ€§ç‰¹å¾ï¼ˆä¾‹å¦‚é•¿åº¦ã€é¢œè‰²ã€å½¢çŠ¶ç­‰ï¼‰ã€‚

### 2)Â Â ç‰¹å¾å¯è§†åŒ–
å°†ç‰¹å¾æ•°æ®é€šè¿‡æ•£ç‚¹å›¾ã€åˆ†å¸ƒå›¾ç­‰æ–¹å¼è§‚å¯Ÿä¸‹ç‰¹å¾æ•°æ®çš„ç‰¹ç‚¹ï¼Œä¸€æ–¹é¢å¯ä»¥è§‚å¯Ÿç‰¹å¾å¯¹äºåˆ†ç±»ç­‰æ•°æ®åŒºåˆ†åº¦ï¼Œæ›´é‡è¦çš„æ˜¯å¯ä»¥æ ¹æ®æ•°æ®åˆ†å¸ƒï¼Œç¡®è®¤ç‰¹å¾æ˜¯å¦å­˜åœ¨å¼‚å¸¸æƒ…å†µï¼Œä¾‹å¦‚ç”±äºçº¿ä¸Š bug å¯¼è‡´éƒ¨åˆ†æ•°æ®æ˜¯é”™è¯¯çš„ã€‚è¿™ä¸€å—å»ºè®®é‡ç‚¹å…³æ³¨ï¼Œå¯èƒ½æ¯”è¾ƒè´¹æ—¶ï¼Œä½†æ˜¯å¯ä»¥é¿å…åé¢æ¨¡å‹ä¼˜åŒ–æˆ– bad case æ’æŸ¥çš„å·¥ä½œé‡ã€‚

### 3)Â Â ç»Ÿè®¡ç‰¹å¾
æœ‰äº†åŸå§‹çš„ç‰¹å¾å› ç´ åï¼Œå¯ä»¥è®©è¿™ä¸ªç‰¹å¾å…·å¤‡æ›´å¼ºçš„è¡¨è¾¾æ€§ã€‚ç»Ÿè®¡åŒ–æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æ–¹å¼ï¼Œä¸»è¦æœ‰æœ€å¤§å€¼ã€æœ€å°å€¼ã€å¹³å‡å€¼ã€æ ‡å‡†å·®ã€æ–¹å·®ã€ä¸­ä½æ•°ã€åˆ†å¸ƒåŒºé—´ç»Ÿè®¡æ•°ç­‰ã€‚ä¾‹å¦‚å‘¨ä¸€çš„å¹³å‡è®¢å•æ•°ã€æœ€å¤§è®¢å•æ•°ç­‰ã€‚å¯ä»¥æŸ¥çœ‹ä¸‹èŠ‚ä¸­ç±»åˆ«ç‰¹å¾ä¸æ•°å€¼ç‰¹å¾çš„ç»„åˆã€‚

### 4)Â Â ç‰¹å¾ç»„åˆ
ç»„åˆå¤šä¸ªç›¸å…³ç‰¹å¾æå–å‡ºå…¶ç›¸å…³çš„è§„å¾‹ï¼Œä¾‹å¦‚å¤šä¸ªç‰¹å¾åŠ å’Œã€æ±‚å·®ã€ä¹˜é™¤ã€æ±‚æ–œç‡ã€å˜åŒ–æ¯”ç‡ã€å¢é•¿å€æ•°ç­‰ã€‚


**æ•°å€¼ç‰¹å¾çš„ç®€å•å˜æ¢**

1. å•ç‹¬ç‰¹å¾åˆ—ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°ï¼ˆconstant multiplicationï¼‰æˆ–è€…åŠ å‡ä¸€ä¸ªå¸¸æ•°ï¼šå¯¹äºåˆ›é€ æ–°çš„æœ‰ç”¨ç‰¹å¾æ¯«æ— ç”¨å¤„ï¼›åªèƒ½ä½œä¸ºå¯¹å·²æœ‰ç‰¹å¾çš„å¤„ç†ï¼Œä¹Ÿå°±æ˜¯è¯´æ•°æ®é¢„å¤„ç†ä¸­ç‰¹å¾å½’ä¸€åŒ–ç­‰æ“ä½œæ˜¯ä¸èƒ½äº§ç”Ÿæ–°çš„ç‰¹å¾çš„ã€‚
2. ä»»ä½•é’ˆå¯¹å•ç‹¬ç‰¹å¾åˆ—çš„å•è°ƒå˜æ¢ï¼ˆå¦‚å¯¹æ•°ï¼‰ï¼šä¸é€‚ç”¨äºå†³ç­–æ ‘ç±»ç®—æ³•ã€‚å¯¹äºå†³ç­–æ ‘è€Œè¨€ï¼Œ$X$ã€$X^3$ã€$X^5$ ä¹‹é—´æ²¡æœ‰å·®å¼‚ï¼Œ $|X|$ã€ $X^2$ã€ $X^4$ ä¹‹é—´æ²¡æœ‰å·®å¼‚ï¼Œé™¤éå‘ç”Ÿäº†èˆå…¥è¯¯å·®ã€‚
3. **çº¿æ€§ç»„åˆ**ï¼ˆlinear combinationï¼‰ï¼š**ä»…é€‚ç”¨äºå†³ç­–æ ‘**ä»¥åŠåŸºäºå†³ç­–æ ‘çš„ensembleï¼ˆå¦‚gradient boosting, random forestï¼‰ï¼Œå› ä¸ºå¸¸è§çš„axis-aligned split functionä¸æ“…é•¿æ•è·ä¸åŒç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼›**ä¸é€‚ç”¨äºSVMã€çº¿æ€§å›å½’ã€ç¥ç»ç½‘ç»œç­‰**ã€‚
4. å¤šé¡¹å¼ç‰¹å¾ï¼ˆpolynomial featureï¼‰ï¼š[sklearn.preprocessing.PolynomialFeatures](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)ã€‚
5. æ¯”ä¾‹ç‰¹å¾ï¼ˆratio featureï¼‰ï¼š$X_1 / X_2$ã€‚
6. ç»å¯¹å€¼ï¼ˆabsolute valueï¼‰ï¼š$|X_1 - X_2|$
7. å…¶ä»–ä¸€äº›æ“ä½œçš„ç‰¹å¾ï¼š
$max(X_1, X_2)$ï¼Œ$min(X_1, X_2)$ï¼Œ$X_1 xor X_2$ã€‚

**ç±»åˆ«ç‰¹å¾ä¸æ•°å€¼ç‰¹å¾çš„ç»„åˆ**

ç”¨ N1 å’Œ N2 è¡¨ç¤ºæ•°å€¼ç‰¹å¾ï¼Œç”¨ C1 å’Œ C2 è¡¨ç¤ºç±»åˆ«ç‰¹å¾ï¼Œåˆ©ç”¨ Pandas çš„ groupby æ“ä½œï¼Œå¯ä»¥åˆ›é€ å‡ºä»¥ä¸‹å‡ ç§æœ‰æ„ä¹‰çš„æ–°ç‰¹å¾ï¼ˆå…¶ä¸­ï¼ŒC2 è¿˜å¯ä»¥æ˜¯ç¦»æ•£åŒ–äº†çš„ N1ï¼‰ï¼š
```
median(N1)_by(C1)  \\ ä¸­ä½æ•°
mean(N1)_by(C1)  \\ ç®—æœ¯å¹³å‡æ•°
mode(N1)_by(C1)  \\ ä¼—æ•°
min(N1)_by(C1)  \\ æœ€å°å€¼
max(N1)_by(C1)  \\ æœ€å¤§å€¼
std(N1)_by(C1)  \\ æ ‡å‡†å·®
var(N1)_by(C1)  \\ æ–¹å·®
freq(C2)_by(C1)  \\ é¢‘æ•°

freq(C1) \\è¿™ä¸ªä¸éœ€è¦groupbyä¹Ÿæœ‰æ„ä¹‰
```

ä»…ä»…å°†å·²æœ‰çš„ç±»åˆ«å’Œæ•°å€¼ç‰¹å¾è¿›è¡Œä»¥ä¸Šçš„æœ‰æ•ˆç»„åˆï¼Œå°±èƒ½å¤Ÿå¤§é‡å¢åŠ ä¼˜ç§€çš„å¯ç”¨ç‰¹å¾ã€‚

å°†è¿™ç§æ–¹æ³•å’Œçº¿æ€§ç»„åˆç­‰åŸºç¡€ç‰¹å¾å·¥ç¨‹æ–¹æ³•ç»“åˆï¼ˆä»…ç”¨äºå†³ç­–æ ‘ï¼‰ï¼Œå¯ä»¥å¾—åˆ°æ›´å¤šæœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œå¦‚ï¼š
```
N1 - median(N1)_by(C1)
N1 - mean(N1)_by(C1)
```


å°†å¤šä¸ªç»´åº¦ç‰¹å¾ç›¸äº’äº¤å‰ï¼Œäº§ç”Ÿæ›´å¤šå…·ä½“åœºæ™¯åŒ–çš„ç‰¹å¾ï¼Œä¾‹å¦‚å’Œä¸åŒæ—¶æ®µæ®µã€å’Œä¸åŒçš„åœ°ç†ä½ç½®èŒƒå›´ç»„åˆã€‚


### 5)Â Â ç‰¹å¾æ‹†è§£
å°†ä¸€ä¸ªç‰¹å¾æ‹†ä¸ºå¤šä¸ª**æ›´æ˜“ç†è§£**çš„ç‰¹å¾ã€‚ ä¾‹å¦‚æ—¥æœŸï¼Œå¯ä»¥æ‹†ä¸ºå¹´ã€æœˆã€æ—¥ã€å°æ—¶ã€åˆ†ã€ç§’ã€æ˜ŸæœŸå‡ ã€æ˜¯å¦ä¸ºå‘¨æœ«ã€‚

### 6)Â Â æ•°å­—å‹ç‰¹å¾é‡æ„
é€šè¿‡è°ƒæ•´æ•°å­—å•ä½ç­‰æ–¹å¼ï¼Œå¯ä»¥è°ƒæ•´æ•°å­—å¤§å°ã€‚ ä¾‹å¦‚6500 å…‹ å¯ä»¥è¡¨è¾¾6.5åƒå…‹ï¼› ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥æ‹†è§£è¡¨è¾¾ä¸º6åƒå…‹ã€0.5åƒå…‹ç­‰ã€‚è¿™ä¸æ˜¯æ²¡å•¥é“ç†å˜›â€¦â€¦

### 7)Â Â One-Hot encoding
å°†ç±»å‹ç‰¹å¾æ˜ å°„å¤šä¸ªæ˜¯å¦ç‰¹å¾ï¼Œä¾‹å¦‚é¢œè‰²å¯æ˜ å°„æ˜¯å¦ä¸ºä¸ºçº¢è‰²ã€æ˜¯å¦ä¸ºç»¿è‰²ã€æ˜¯å¦ä¸ºè“è‰²ã€‚

### 8) ç»Ÿè®¡æ€§ç‰¹å¾æ˜ å°„ä¸ºè§£é‡Šå‹ç‰¹å¾
å°†ä¸€ä¸ªæ•°å­—å‹æˆ–ç»Ÿè®¡æ€§ç‰¹å¾ï¼Œæ˜ å°„ä¸ºå¤šä¸ªèŒƒå›´åŒºé—´ï¼Œç„¶åä¸ºæ¯ä¸ªåŒºé—´ä¸ºä¸€ä¸ªç±»åˆ«ï¼Œæ¥ç€å€ŸåŠ©äº onehot encoding å°±å˜ä¸ºä¸€ç³»åˆ—æ˜¯å¦çš„è§£é‡Šå‹ç‰¹å¾ã€‚ä¾‹å¦‚å†å²æœˆè®¢å• 0~5 ä¸ºä½é¢‘ã€6~15 ä¸ºä¸­é¢‘ã€ å¤§äº16ä¸ºé«˜é¢‘ï¼Œ è®¢å•é‡10æ•°å­—å°±å¯ä»¥å˜ä¸º [0,1,0] è¿™ä¸‰ç»´ç‰¹å¾ã€‚

### x) ç”¨åŸºå› ç¼–ç¨‹åˆ›é€ æ–°ç‰¹å¾
Welcome to gplearnâ€™s documentation!

åŸºäºgenetic programmingçš„symbolic regressionï¼Œå…·ä½“çš„åŸç†å’Œå®ç°å‚è§æ–‡æ¡£ã€‚ç›®å‰ï¼Œpythonç¯å¢ƒä¸‹æœ€å¥½ç”¨çš„åŸºå› ç¼–ç¨‹åº“ä¸ºgplearnã€‚åŸºå› ç¼–ç¨‹çš„ä¸¤å¤§ç”¨æ³•ï¼š

* è½¬æ¢ï¼ˆtransformationï¼‰ï¼šæŠŠå·²æœ‰çš„ç‰¹å¾è¿›è¡Œç»„åˆè½¬æ¢ï¼Œç»„åˆçš„æ–¹å¼ï¼ˆä¸€å…ƒã€äºŒå…ƒã€å¤šå…ƒç®—å­ï¼‰å¯ä»¥ç”±ç”¨æˆ·è‡ªè¡Œå®šä¹‰ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨åº“ä¸­è‡ªå¸¦çš„å‡½æ•°ï¼ˆå¦‚åŠ å‡ä¹˜é™¤ã€minã€maxã€ä¸‰è§’å‡½æ•°ã€æŒ‡æ•°ã€å¯¹æ•°ï¼‰ã€‚ç»„åˆçš„ç›®çš„ï¼Œæ˜¯åˆ›é€ å‡ºå’Œç›®æ ‡yå€¼æœ€â€œç›¸å…³â€çš„æ–°ç‰¹å¾ã€‚è¿™ç§ç›¸å…³ç¨‹åº¦å¯ä»¥ç”¨spearmanæˆ–è€…pearsonçš„ç›¸å…³ç³»æ•°è¿›è¡Œæµ‹é‡ã€‚spearmanå¤šç”¨äºå†³ç­–æ ‘ï¼ˆå…ç–«å•ç‰¹å¾å•è°ƒå˜æ¢ï¼‰ï¼Œpearsonå¤šç”¨äºçº¿æ€§å›å½’ç­‰å…¶ä»–ç®—æ³•ã€‚
* å›å½’ï¼ˆregressionï¼‰ï¼šåŸç†åŒä¸Šï¼Œåªä¸è¿‡ç›´æ¥ç”¨äºå›å½’è€Œå·²ã€‚

### x) ç”¨å†³ç­–æ ‘åˆ›é€ æ–°ç‰¹å¾
åœ¨å†³ç­–æ ‘ç³»åˆ—çš„ç®—æ³•ä¸­ï¼ˆå•æ£µå†³ç­–æ ‘ã€GBDTã€éšæœºæ£®æ—ï¼‰ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬éƒ½ä¼šè¢«æ˜ å°„åˆ°å†³ç­–æ ‘çš„ä¸€ç‰‡å¶å­ä¸Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ ·æœ¬ç»è¿‡æ¯ä¸€æ£µå†³ç­–æ ‘æ˜ å°„åçš„indexï¼ˆè‡ªç„¶æ•°ï¼‰æˆ–one-hot-vectorï¼ˆå“‘ç¼–ç å¾—åˆ°çš„ç¨€ç–çŸ¢é‡ï¼‰ä½œä¸ºä¸€é¡¹æ–°çš„ç‰¹å¾ï¼ŒåŠ å…¥åˆ°æ¨¡å‹ä¸­ã€‚

å…·ä½“å®ç°ï¼šapply() ä»¥åŠ decision_path() æ–¹æ³•ï¼Œåœ¨ scikit-learn å’Œ xgboost é‡Œéƒ½å¯ä»¥ç”¨ã€‚



* å†³ç­–æ ‘ã€åŸºäºå†³ç­–æ ‘çš„ ensemble
    * spearman correlation coefficient


* çº¿æ€§æ¨¡å‹ã€SVMã€ç¥ç»ç½‘ç»œ
    * å¯¹æ•°ï¼ˆlogï¼‰
    * pearson correlation coefficient



### 9) æŒ–æ˜ç‰¹å¾
å¯¹äºæœ‰äº›ç‰¹å¾æˆ‘ä»¬çš„æ•°æ®æ²¡æœ‰æ˜ç¡®çš„æ ‡æ³¨ï¼Œä½†æ˜¯æˆ‘ä»¬è®¤ä¸ºä¹Ÿå¾ˆé‡è¦ã€‚è¿™æ˜¯å¯ä»¥ç”¨æœºå™¨å­¦ä¹ ä»¥åŠéƒ¨åˆ†æ ·æœ¬æ ‡æ³¨æˆ–äººå·¥æ ‡æ³¨çš„æ–¹å¼æŒ–æ˜ä¸€äº›ç‰¹å¾ã€‚ ä¾‹å¦‚å‡è®¾å¤§éƒ¨åˆ†ç”¨æˆ·çš„æ€§åˆ«æˆ‘ä»¬ä¸çŸ¥é“ï¼Œä½†æ˜¯éƒ¨åˆ†ç”¨æˆ·å¯ä»¥é€šè¿‡å„ç§å…¶ä»–é€”å¾„çŸ¥é“ã€‚é‚£å¯ä»¥åŸºäºè¿™ä¸ªæ ·æœ¬è®­ç»ƒå‡ºä¸€ä¸ªæ€§åˆ«åˆ†ç±»é¢„æµ‹çš„æ¨¡å‹ï¼Œç„¶åé¢„æµ‹å‡ºæ‰€æœ‰ç”¨æˆ·çš„æ€§åˆ«ï¼Œå°†è¿™ä¸ªé¢„æµ‹ç»“æœåšä¸ºç‰¹å¾ã€‚

### 10) ç‰¹å¾è‡ªå­¦ä¹ 
åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œåœ¨æ„é€ å‡ºå¾—åˆ°å¥½çš„åŸå§‹ç‰¹å¾å’Œé€‚ç”¨äºç‰¹å¾çš„ç½‘ç»œç»“æ„åï¼Œç‰¹å¾ç»„åˆå’ŒæŠ½è±¡ä¼šäº¤ç»™æ·±åº¦å­¦ä¹ è‡ªè¡Œå­¦ä¹ ã€‚ å…¸å‹å¯ä»¥å‚è€ƒ CNN ç®—æ³•ï¼Œé€šè¿‡ç½‘ç»œçš„å·ç§¯ã€æ± åŒ–ç­‰æ“ä½œå°†åŸå§‹å›¾ç‰‡ç‰¹å¾ï¼Œé€šè¿‡ç½‘ç»œå±‚å­¦ä¹ æŠ½è±¡å‡ºäº†é«˜å±‚æ¬¡çš„è¾¹ç¼˜ã€è½®å»“ç­‰ç‰¹å¾ã€‚

### 11) ç‰¹å¾ç­›é€‰
å½“æœ‰å¾ˆå¤šç‰¹å¾æ—¶ï¼Œæœ‰éƒ¨åˆ†ç‰¹å¾æ˜¯å¼ºç›¸å…³çš„ï¼Œå±äºå†—ä½™ç‰¹å¾ï¼›æœ‰éƒ¨åˆ†ç‰¹å¾å¯èƒ½è´¡çŒ®å°ç”šè‡³è´Ÿé¢è´¡çŒ®ã€‚ è¿™æ—¶å€™éœ€è¦åšä¸€äº›ç‰¹å¾ç­›é€‰ã€‚ä¾‹å¦‚ç‰¹å¾ä¸¤ä¸¤ç»„åˆç¡®è®¤ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ç³»æ•°ï¼Œå¯¹äºç›¸å…³æ€§éå¸¸é«˜çš„ç‰¹å¾åªä¿ç•™ä¸€ä¸ªï¼›**ç‰¹å¾å’Œ Label æ ‡æ³¨åšç›¸å…³æ€§åˆ¤æ–­ï¼Œå»æ‰ä¸€äº›ç›¸å…³æ€§å·®çš„ç‰¹å¾ï¼›**å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡æ§åˆ¶ç‰¹å¾å¢é•¿çš„è¿‡ç¨‹ï¼Œä»åŸºç¡€ç‰¹å¾é›†åˆå¼€å§‹ï¼Œé€æ¸åŠ å…¥æ–°ç‰¹å¾æˆ–æ–°ç‰¹å¾é›†å®éªŒï¼Œå¦‚æœæ–°ç‰¹å¾æ•ˆæœä¸å¥½ï¼Œåˆ™ä¸¢å¼ƒã€‚

æœ‰äº›æ¨¡å‹è‡ªå¸¦ç‰¹å¾ç­›é€‰çš„èƒ½åŠ›ï¼Œä¾‹å¦‚gbdtï¼ˆ xgboostï¼‰ã€å›å½’ä¸­æ­£åˆ™åŒ–ç­‰ã€‚é€šè¿‡è¿™äº›æ¨¡å‹ä¸€å®šç¨‹åº¦ä¹Ÿèƒ½è¾¾åˆ°ç‰¹å¾ç­›é€‰çš„ç›®æ ‡ã€‚ä¸è¿‡å¦‚æœç‰¹å¾é‡ç‰¹åˆ«å¤šï¼Œå»ºè®®åœ¨ä¸Šçº¿å‰ï¼Œå»æ‰æ— æ•ˆç‰¹å¾ï¼Œè¿™æ ·æ—¢å¯ä»¥é¿å…ç‰¹å¾ç»´æŠ¤çš„å·¥ä½œé‡ï¼ŒåŒæ—¶ä¹Ÿèƒ½æé«˜çº¿ä¸Šæ€§èƒ½ã€‚

### x) ç‰¹å¾å¹³æ»‘å¤„ç†
1. é•¿å°¾æ•°æ®ï¼šè¿›è¡Œå–å¯¹æ•°ã€‚è¿™é‡Œå¯ä»¥å‚è€ƒæ•°æ®é¢„å¤„ç†çš„åšæ–‡ã€‚

```python
train_df[col] = train_df[col].map(lambda x : p.log1p(x))
```



---

Feature engineering is a vital component of modelling process, and it is the toughest to automate. It takes domain expertise and a lot of exploratory analysis on the data to engineer features

1. single variable Basic transformations: x, x^2 ,sqrt x ,log x, scaling

2. If variable's distribution has a long tail, apply Box-Cox transformation (taking log() is a quick & dirty way).

3. One could also perform analysis of residuals or log-odds (for linear model) to check for strong nonlinearities.

4. Create a feature which captures the frequency of the occurrence of each level of the categorical variable. For high cardinality, this helps a lot. One might use ratio/percentage of a particular level to all the levels present.

5. For every possible value of the variable, estimate the mean of the target variable; use the result as an engineered feature.

6. Encode a variable with the ratio of the target variable.

7. Take the two most important variables and throw in second order interactions between them and the rest of the variables - compare the resulting model to the original linear one

8. if you feel your solutions should be smooth, you can apply a radial basis function kernel .  This is like applying a smoothing transform.  

9. If you feel you need covariates , you can apply a polynomial kernel, or add the covariates explicitly

10. High cardinality features : convert to numeric by preprocessing: out-of-fold average two variable combinations

11. Additive transformation

12. difference relative to baseline

13. Multiplicative transformation : interactive effects

14. divisive : scaling/normalisation

15. thresholding numerical features to get boolean values

16. Cartesian Product Transformation

17. Feature crosses: cross product of all features -- Consider a feature A, with two possible values {A1, A2}. Let B be a feature with possibilities {B1, B2}. Then, a feature-cross between A & B (letâ€™s call it AB) would take one of the following values: {(A1, B1), (A1, B2), (A2, B1), (A2, B2)}. You can basically give these â€˜combinationsâ€™ any names you like. Just remember that every combination denotes a synergy between the information contained by the corresponding values of A and B.

18. Normalization Transformation: -- One of the implicit assumptions often made in machine learning algorithms (and somewhat explicitly in Naive Bayes) is that the the features follow a normal distribution. However, sometimes we may find that the features are not following a normal distribution but a log normal distribution instead. One of the common things to do in this situation is to take the log of the feature values (that exhibit log normal distribution) so that it exhibits a normal distribution.If the algorithm being used is making the implicit/explicit assumption of the features being normally distributed, then such a transformation of a log-normally distributed feature to a normally distributed feature can help improve the performance of that algorithm.

19. Quantile Binning Transformation

20. whitening the data

21. Windowing -- If points are distributed in time axis, previous points in the same window are often very informative

22. Min-max normalization : does not necessarily preserve order

23. sigmoid / tanh / log transformations

24. Handling zeros distinctly â€“ potentially important for Count based features

25. Decorrelate / transform variables

26. Reframe Numerical Quantities

27. Map infrequent categorical variables to a new/separate category.

28.Sequentially apply a list of transforms.

29. One Hot Encoding

30. Target rate encoding

Hash Trick Multivariate:

31. PCA

32. MODEL STACKING

33. compressed sensing

34..guess the averageâ€ or â€œguess the average segmented by variable Xâ€

Projection : new basis

35. Hack projection:

Perform clustering and use distance between points to the cluster center as a feature
PCA/SVD -- Useful technique to analyze the interrelationships between variables and perform dimensionality reduction with minimum loss of information (find the axis through the data with highest variance / repeat with the next orthogonal axis and so on , until you run out of data or dimensions; Each axis acts a new feature)
36.Sparse coding -- choose basis : evaluate the basis  based on how well  you can use it to reconstruct the input and how sparse it is take some sort of gradient step to improve that evaluation

efficient sparse coding algorithms
deep auto encoders
37 :Random forest: train bunch of decision trees :use each leaf as a feature

## References
1. [æœºå™¨å­¦ä¹ ä¹‹ ç‰¹å¾å·¥ç¨‹](https://juejin.im/post/5b569edff265da0f7b2f6c65)
2. [The Comprehensive Guide for Feature Engineering](https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/)
3. [ã€æŒç»­æ›´æ–°ã€‘æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹å®ç”¨æŠ€å·§å¤§å…¨](https://zhuanlan.zhihu.com/p/26444240)
4. âœ³ï¸ [Machine Learning Kaggle Competition Part Two: Improving Feature engineering, feature selection, and model evaluation](https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8)
5. [Feature Engineering ç‰¹å¾µå·¥ç¨‹ä¸­å¸¸è¦‹çš„æ–¹æ³•](https://vinta.ws/code/feature-engineering.html)
6. [æœºå™¨å­¦ä¹ ä¹‹step by stepå®æˆ˜åŠçŸ¥è¯†ç§¯ç´¯ç¬”è®°](https://www.cnblogs.com/kidsitcn/p/9176602.html)
7. [ç‰¹å¾å·¥ç¨‹ï¼šæ•°æ®ç§‘å­¦å®¶çš„ç§˜å¯†æ­¦å™¨ï¼](https://yq.aliyun.com/articles/82611)
8. [æœºå™¨å­¦ä¹ é¡¹ç›®çš„å®Œæ•´æµç¨‹](https://blog.csdn.net/qq_24831889/article/details/83241104#53_badcase_186)
9. [è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•](https://www.leiphone.com/news/201801/T9JlyTOAMxFZvWly.html)
10. [æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å¤„ç†ç»¼è¿°](https://tech.meituan.com/2015/02/10/machinelearning-data-feature-process.html)
11. [pythonå¼€å‘ï¼šç‰¹å¾å·¥ç¨‹ä»£ç æ¨¡ç‰ˆ(ä¸€)](http://shataowei.com/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%B8%80/)
12. [pythonå¼€å‘ï¼šç‰¹å¾å·¥ç¨‹ä»£ç æ¨¡ç‰ˆ(äºŒ)](http://shataowei.com/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%BA%8C/)
13. [Normalization(æ ‡å‡†åŒ–)çš„åŸç†å’Œå®ç°è¯¦è§£](http://www.dongdongbai.com/index.php/2017/12/11/97/)
14. [æ•°æ®æŒ–æ˜çš„æµç¨‹å’Œæ–¹æ³•ã€æŠ€å·§æ€»ç»“](https://zhuanlan.zhihu.com/p/33429338)
15. [4.3. é¢„å¤„ç†æ•°æ®](http://doc.codingdict.com/sklearn/59/)
16. [scikit-learn preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
