---
layout: post
title: Feature Engineering
author: Bin Li
tags: [Machine Learning]
category: ""
comments: true
published: true
typora-root-url: ../../../../binlidaily.github.io
---

åœ¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­ï¼Œç‰¹å¾å·¥ç¨‹æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚æ•°æ®å’Œç‰¹å¾å†³å®šäº†æœºå™¨å­¦ä¹ ç®—æ³•çš„ä¸Šé™ï¼Œè€Œæ¨¡å‹çš„ç®—æ³•åªæ˜¯ä¸æ–­é€¼è¿‘è¿™ä¸ªä¸Šé™è€Œå·²ã€‚ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰ä»‹äºâ€œæ•°æ®â€å’Œâ€œæ¨¡å‹â€ä¹‹é—´ï¼Œæ˜¯åˆ©ç”¨æ•°æ®çš„ä¸“ä¸šé¢†åŸŸçŸ¥è¯†å’Œç°æœ‰æ•°æ®ï¼Œä»æºæ•°æ®ä¸­æŠ½å–å‡ºæ¥å¯¹é¢„æµ‹ç»“æœæœ‰ç”¨çš„ä¿¡æ¯ï¼Œç”¨åœ¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸Šçš„è¿‡ç¨‹ã€‚ç¾å›½è®¡ç®—æœºç§‘å­¦å®¶ Peter Norvig æœ‰ä¸¤å¥ç»å…¸çš„åè¨€ï¼š
* â€œåŸºäºå¤§é‡æ•°æ®çš„ç®€å•æ¨¡å‹èƒœè¿‡åŸºäºå°‘é‡æ•°æ®çš„å¤æ‚æ¨¡å‹ã€‚â€
* â€œæ›´å¤šçš„æ•°æ®èƒœè¿‡èªæ˜çš„ç®—æ³•ï¼Œè€Œå¥½çš„æ•°æ®èƒœè¿‡å¤šçš„æ•°æ®ã€‚â€

å´æ©è¾¾æ›´æ˜¯è¯´è¿‡â€œåº”ç”¨æœºå™¨å­¦ä¹ åŸºæœ¬ä¸Šå°±æ˜¯ç‰¹å¾å·¥ç¨‹â€ã€‚å¯¹äºå·¥ä¸šç•Œæ¥è¯´ï¼Œå¤§éƒ¨åˆ†å¤æ‚æ¨¡å‹çš„ç®—æ³•ç²¾è¿›éƒ½æ˜¯èµ„æ·±çš„æ•°æ®ç§‘å­¦å®¶çš„ä»»åŠ¡ï¼Œå¤§éƒ¨åˆ†äººå‘˜çš„å·¥ä½œè¿˜æ˜¯è·‘æ•°æ®ã€map-reduceï¼Œhive SQLï¼Œæ•°æ®ä»“åº“æ¬ç –ï¼Œåšä¸€äº›ä¸šåŠ¡åˆ†æã€æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ï¼ˆæ‰¾ç‰¹å¾ï¼‰çš„å·¥ä½œã€‚

ç‰¹å¾å·¥ç¨‹ä¸€èˆ¬åˆ†æˆç‰¹å¾æå– (Feature Extraction) å’Œç‰¹å¾é€‰æ‹© (Feature Selection) ä¸¤ä¸ªæ–¹é¢ï¼Œæ¥ä¸‹æ¥åˆ†åˆ«æ›´ç»†è‡´åœ°ä»‹ç»:

## ä¸€.  ç‰¹å¾æå– (Feature Extraction) 
ç‰¹å¾çš„æŒ–æ˜ä¸€èˆ¬è·Ÿä¸“ä¸šé¢†åŸŸçŸ¥è¯†å¼ºç›¸å…³ï¼Œç‰¹å¾å·¥ç¨‹å¯ä»¥è¯´æ˜¯ä¸šåŠ¡é€»è¾‘çš„ä¸€ç§æ•°æ®å±‚é¢çš„è¡¨ç¤ºã€‚

### 1. æ¢ç´¢æ€§æ•°æ®åˆ†æ (Exploratory Data Analysis, EDA)
EDA çš„ç›®çš„æ˜¯å°½å¯èƒ½åœ°æ´å¯Ÿæ•°æ®é›†ã€å‘ç°æ•°æ®çš„å†…éƒ¨ç»“æ„ã€æå–é‡è¦çš„ç‰¹å¾ã€æ£€æŸ¥å¼‚å¸¸å€¼ã€æ£€éªŒåŸºæœ¬å‡è®¾ã€å»ºç«‹åˆæ­¥çš„æ¨¡å‹ã€‚EDA æŠ€æœ¯ä¸€èˆ¬åˆ†ä¸ºä¸¤ç±»ï¼š
* å¯è§†åŒ–æŠ€æœ¯
    * ç®±å‹å›¾ã€ç›´æ–¹å›¾ã€å¤šå˜é‡å›¾ã€é“¾å›¾ã€å¸•ç´¯æ‰˜å›¾ã€æ•£ç‚¹å›¾ã€èŒå¶å›¾
    * å¹³è¡Œåæ ‡ã€è®©æ­¥æ¯”ã€å¤šç»´å°ºåº¦åˆ†æã€ç›®æ ‡æŠ•å½±è¿½è¸ª
    * ä¸»æˆåˆ†åˆ†æã€é™ç»´ã€éçº¿æ€§é™ç»´ç­‰
* å®šé‡æŠ€æœ¯
    * æ ·æœ¬å‡å€¼ã€æ–¹å·®åˆ†ä½æ•°ã€å³°åº¦ã€ååº¦ç­‰

ç»†èŠ‚å¯ä»¥å‚è€ƒä¹‹å‰æ•´ç†è¿‡çš„æœ‰å…³ EDA çš„[åšæ–‡](https://binlidaily.github.io/2019-01-10-exploratory-data-analysis/)ã€‚

### 2. æ•°å€¼ç‰¹å¾ /å®šé‡ç‰¹å¾ (Numerical Features)
å¯¹äºæ•°å€¼ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸»è¦è€ƒè™‘çš„å› ç´ æ˜¯å®ƒçš„**å¤§å°å’Œåˆ†å¸ƒ**ï¼Œä¸€èˆ¬åˆ†ä¸º`è¿ç»­å‹`ï¼ˆèº«é«˜ä½“é‡ç­‰ï¼‰å’Œ`ç¦»æ•£å‹`ï¼ˆè®¡æ•°ç­‰ï¼‰ã€‚å¯¹äºé‚£äº›ç›®æ ‡å˜é‡ä¸ºè¾“å…¥ç‰¹å¾çš„**å…‰æ»‘å‡½æ•°**çš„æ¨¡å‹ï¼Œä¾‹å¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ç­‰ï¼Œå…¶å¯¹è¾“å…¥ç‰¹å¾çš„å¤§å°å¾ˆæ•æ„Ÿï¼Œæ‰€ä»¥éœ€è¦å½’ä¸€åŒ–ã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬éœ€è¦è¿›è¡Œç‰¹å¾å˜æ¢æ¥æ»¡è¶³éçº¿æ€§æ¨¡å‹çš„å‡è®¾ï¼Œè¿˜å¯ä»¥è¿›è¡Œç‰¹å¾äº¤å‰æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè®©çº¿æ€§æ¨¡å‹å…·æœ‰éçº¿æ€§æ¨¡å‹çš„æ€§è´¨ã€‚ä»¥ä¸‹ä»‹ç»å‡ ç§å¸¸è§çš„æ•°å€¼ç‰¹å¾çš„å¤„ç†æ–¹æ³•ã€‚

### 2.1 æˆªæ–­ / ç¦»ç¾¤ç‚¹ç›–å¸½ï¼Ÿ
* å¯¹äºè¿ç»­å‹æ•°å€¼ç‰¹å¾ï¼Œè¶…å‡ºåˆç†èŒƒå›´çš„å¾ˆå¯èƒ½æ˜¯å™ªå£°ï¼Œéœ€è¦æˆªæ–­
* åœ¨ä¿ç•™é‡è¦ä¿¡æ¯çš„å‰æä¸‹è¿›è¡Œæˆªæ–­ï¼Œæˆªæ–­åçš„ä¹Ÿå¯ä½œä¸ºç±»åˆ«ç‰¹å¾
* é•¿å°¾æ•°æ®å¯ä»¥å…ˆè¿›è¡Œå¯¹æ•°å˜æ¢ï¼Œå†æˆªæ–­

ä¸€èˆ¬çš„åšæ³•æ˜¯åœ¨ EDA åçœ‹åˆ°æŸç‰¹å¾æœ‰ä¸€äº›ç¦»ç¾¤ç‚¹ï¼Œå°±å¯ä»¥ç”¨æˆªæ–­çš„æ–¹å¼å°†å…¶å¤„ç†ä¸€ä¸‹ï¼š
```python
up_limit = np.percentile(data_df[col].values, 99.9) # 99.9%åˆ†ä½æ•°
low_limit = np.percentile(data_df[col].values, 0.1) # 0.1%åˆ†ä½æ•°
data_df.loc[data_df[col] > up_limit, col] = up_limit
data_df.loc[data_df[col] < low_limit, col] = low_limit
```

ä¾‹å­ï¼šå°†è¿™äº›åŸå§‹å¹´é¾„å€¼é™¤ä»¥ 10ï¼Œç„¶åé€šè¿‡ floor å‡½æ•°å¯¹åŸå§‹å¹´é¾„æ•°å€¼è¿›è¡Œæˆªæ–­ã€‚

```python
fcc_survey_df['Age_bin_round'] = np.array(np.floor(np.array(fcc_survey_df['Age']) / 10.))
fcc_survey_df[['ID.x', 'Age','Age_bin_round']].iloc[1071:1076]
```

![](/img/media/15523795162029.jpg)

è¿™æ ·è¿ç»­æ•°å€¼å°±æ²¡æœ‰é‚£ä¹ˆç²¾ç»†äº†ï¼Œä¹Ÿèƒ½åæ˜ å‡ºç›¸äº’ä¹‹é—´çš„å·®åˆ«ã€‚

### 2.2 ç¦»æ•£åŒ–ï¼ˆDiscretizationï¼‰

ç¦»æ•£åŒ–åˆè¢«ç§°ä¸ºé‡åŒ–æˆ–è€…å«åšåˆ†æ¡¶ï¼ˆäºŒå€¼åŒ–ä¹Ÿæ˜¯ä¸€ç§åˆ†æ¡¶ï¼‰ï¼Œæ˜¯ä¸€ç§å°†è¿ç»­å‹ç‰¹å¾è½¬æ¢åˆ°ç¦»æ•£ç‰¹å¾ä¸Šçš„ä¸€ç§æ–¹å¼ï¼Œè€Œç¦»æ•£ç‰¹å¾å¯ä»¥è¢«ç”¨åšç±»åˆ«ç‰¹å¾ï¼Œè¿™å¯¹å¤§å¤šæ•°æ¨¡å‹æ¥è¯´æ¯”è¾ƒå‹å¥½ã€‚é€šè¿‡ç¦»æ•£åŒ–ç”šè‡³å¯ä»¥å°†éçº¿æ€§ç‰¹æ€§å¼•å…¥åˆ°çº¿æ€§æ¨¡å‹ä¸­ï¼Œä»è€Œä½¿å¾—çº¿æ€§æ¨¡å‹æ›´å…·æ³›åŒ–æ€§ã€‚

### 2.2.1 äºŒå€¼åŒ– (Binarization)
è®¡æ•°ç‰¹å¾å¯ä»¥è€ƒè™‘è½¬æ¢ä¸ºæ˜¯å¦çš„äºŒå€¼åŒ–å½¢å¼ï¼ŒåŸºäºè¦è§£å†³çš„é—®é¢˜æ„å»ºæ¨¡å‹æ—¶ï¼Œé€šå¸¸åŸå§‹é¢‘æ•°æˆ–æ€»æ•°å¯èƒ½ä¸æ­¤ä¸ç›¸å…³ã€‚æ¯”å¦‚å¦‚æœæˆ‘è¦å»ºç«‹ä¸€ä¸ªæ¨èç³»ç»Ÿç”¨æ¥æ¨èæ­Œæ›²ï¼Œæˆ‘åªå¸Œæœ›çŸ¥é“ä¸€ä¸ªäººæ˜¯å¦æ„Ÿå…´è¶£æˆ–æ˜¯å¦å¬è¿‡æŸæ­Œæ›²ã€‚æˆ‘ä¸éœ€è¦çŸ¥é“ä¸€é¦–æ­Œè¢«å¬è¿‡çš„æ¬¡æ•°ï¼Œå› ä¸ºæˆ‘æ›´å…³å¿ƒçš„æ˜¯ä¸€ä¸ªäººæ‰€å¬è¿‡çš„å„ç§å„æ ·çš„æ­Œæ›²ã€‚

```python
watched = np.array(popsong_df['listen_count'])
watched[watched >= 1] = 1
popsong_df['watched'] = watched
# å½“ç„¶ä¹Ÿå¯ä»¥ç”¨ Pandas ä¸­ DataFrame çš„æ–¹å¼
popsong_df['watched'] = 0
popsong_df.loc[popsong_df['listen_count'] >= 1, 'watched'] = 1
```

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ scikit-learn ä¸­ preprocessing æ¨¡å—çš„ Binarizer ç±»æ¥æ‰§è¡ŒåŒæ ·çš„ä»»åŠ¡ï¼Œè€Œä¸ä¸€å®šä½¿ç”¨ numpy æ•°ç»„ã€‚

```python
from sklearn.preprocessing import Binarizer
bn = Binarizer(threshold=0.9)
pd_watched =bn.transform([popsong_df['listen_count']])[0]
popsong_df['pd_watched'] = pd_watched
popsong_df.head(11)
```
![](/img/media/15523797913264.jpg)



### 2.2.2 åˆ†æ¡¶ (Binning) 
å¦‚æœç›´æ¥åˆ©ç”¨åŸå§‹çš„è¿ç»­æ•°å€¼å‹ç‰¹å¾æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯è¿™äº›ç‰¹å¾çš„æ•°å€¼**åˆ†å¸ƒ**é€šå¸¸æ˜¯æœ‰åå‘çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰äº›æ•°æ®ç‰¹åˆ«å¤šè€Œä¸€äº›å€¼å°±ç›¸å¯¹å¾ˆå°‘å‡ºç°ã€‚å¦å¤–ï¼Œè¿™äº›ç‰¹å¾çš„**å¤§å°**å˜åŒ–èŒƒå›´ä¹Ÿæ˜¯éœ€è¦æ³¨æ„çš„ã€‚å¦‚æœç›´æ¥åˆ©ç”¨è¿™äº›ç‰¹å¾ï¼Œæ¨¡å‹çš„æ•ˆæœä¸€èˆ¬ä¸å¥½ï¼Œäºæ˜¯éœ€è¦å¤„ç†è¿™äº›ç‰¹å¾ï¼Œæœ‰åˆ†æ¡¶å’Œå˜æ¢çš„æ–¹å¼ã€‚

å¯¹éœ€è¦åˆ†æ¡¶çš„æƒ…å†µåšä¸€ä¸ªç»éªŒæ€§çš„æ€»ç»“ï¼š
* è¿ç»­å‹æ•°å€¼ç‰¹å¾çš„æ•°å€¼åˆ†å¸ƒæœ‰åå‘çš„å¯ä»¥åˆ†æ¡¶
* ç¦»æ•£å‹æ•°å€¼ç‰¹å¾çš„æ•°å€¼è·¨è¶Šäº†ä¸åŒçš„æ•°é‡çº§å¯ä»¥åˆ†æ¡¶

åˆ†æ¡¶å¯ä»¥å°†è¿ç»­æ€§æ•°å€¼ç‰¹å¾è½¬æ¢ä¸ºç¦»æ•£å‹ç‰¹å¾ï¼ˆç±»åˆ«ï¼‰ï¼Œæ¯ä¸€ä¸ªæ¡¶ä»£è¡¨äº†æŸä¸€ä¸ªèŒƒå›´çš„è¿ç»­æ€§æ•°å€¼ç‰¹å¾çš„å¯†åº¦ã€‚

### 2.2.2.1 å›ºå®šå®½åº¦åˆ†æ¡¶ (Fixed-Width Binning)

å›ºå®šæ¯ä¸ªåˆ†æ¡¶çš„å®½åº¦ï¼Œå³æ¯ä¸ªæ¡¶çš„å€¼åŸŸæ˜¯å›ºå®šçš„ï¼Œå¦‚æœæ¯ä¸ªæ¡¶çš„å¤§å°ä¸€æ ·ï¼Œä¹Ÿç§°ä¸ºå‡åŒ€åˆ†æ¡¶ã€‚è¿™é‡Œç”¨å¹´é¾„ä½œä¸ºä¾‹å­è¿›è¡Œè¯´æ˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºå¹´é¾„æœ‰ä¸€ç‚¹å³åçš„æ•°æ®åˆ†å¸ƒï¼š

![](/img/media/15523887500985.jpg)

æˆ‘ä»¬å°è¯•ç”¨å¦‚ä¸‹çš„å›ºå®šå®½åº¦æ¥åˆ†æ¡¶ï¼š
```shell
Age Range: Bin
---------------
 0 -  9  : 0
10 - 19  : 1
20 - 29  : 2
30 - 39  : 3
40 - 49  : 4
50 - 59  : 5
60 - 69  : 6
  ... and so on
```

1ã€å¦‚æœé‡‡ç”¨æ•°æ®èˆå…¥çš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æµ®ç‚¹å‹çš„å¹´é¾„ç‰¹å¾é™¤ä»¥10ï¼š

```python
fcc_survey_df['Age_bin_round'] = np.array(np.floor(
                              np.array(fcc_survey_df['Age']) / 10.))
fcc_survey_df[['ID.x', 'Age', 'Age_bin_round']].iloc[1071:1076]
```

![](/img/media/15523898042213.jpg)

2ã€é‚£å¦‚æœæˆ‘ä»¬éœ€è¦æƒ³è¦æ›´çµæ´»çš„æ–¹å¼ï¼ˆæŒ‰ç…§è‡ªå·±çš„æ„æ„¿ï¼‰æ¥æ“ä½œè¦æ€ä¹ˆåšå‘¢ï¼Ÿæ¯”å¦‚è¿™æ ·åˆ†æ¡¶ï¼š

```python
Age Range : Bin
---------------
 0 -  15  : 1
16 -  30  : 2
31 -  45  : 3
46 -  60  : 4
61 -  75  : 5
75 - 100  : 6
```

å¯ä»¥ç”¨ Pandas çš„ cut å‡½æ•°ï¼š
```python
bin_ranges = [0, 15, 30, 45, 60, 75, 100]
bin_names = [1, 2, 3, 4, 5, 6]
fcc_survey_df['Age_bin_custom_range'] = pd.cut(
                                           np.array(
                                              fcc_survey_df['Age']), 
                                              bins=bin_ranges)
fcc_survey_df['Age_bin_custom_label'] = pd.cut(
                                           np.array(
                                              fcc_survey_df['Age']), 
                                              bins=bin_ranges,            
                                              labels=bin_names)
# view the binned features 
fcc_survey_df[['ID.x', 'Age', 'Age_bin_round', 
               'Age_bin_custom_range',   
               'Age_bin_custom_label']].iloc[10a71:1076]
```

![](/img/media/15523901338278.jpg)

3ã€å¯ä»¥é‡‡ç”¨ [sklearn.preprocessing.KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) çš„æ–¹å¼ï¼š

```python
>>> X = [[-2, 1, -4,   -1],
...      [-1, 2, -3, -0.5],
...      [ 0, 3, -2,  0.5],
...      [ 1, 4, -1,    2]]
>>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
>>> est.fit(X)  
KBinsDiscretizer(...)
>>> Xt = est.transform(X)
>>> Xt  
array([[ 0., 0., 0., 0.],
       [ 1., 1., 1., 0.],
       [ 2., 2., 2., 1.],
       [ 2., 2., 2., 2.]])
# çœ‹ä¸‹åˆ†æ¡¶è¾¹ç•Œ
>>> est.bin_edges_[0]
array([-2., -1.,  0.,  1.])
>>> est.inverse_transform(Xt)
array([[-1.5,  1.5, -3.5, -0.5],
       [-0.5,  2.5, -2.5, -0.5],
       [ 0.5,  3.5, -1.5,  0.5],
       [ 0.5,  3.5, -1.5,  1.5]])
```



### 2.2.2.2 è‡ªå®šä¹‰åˆ†æ¡¶

å°†ä¸€ä¸ªæ•°å­—å‹æˆ–ç»Ÿè®¡æ€§ç‰¹å¾ï¼Œæ˜ å°„ä¸ºå¤šä¸ªèŒƒå›´åŒºé—´ï¼Œç„¶åä¸ºæ¯ä¸ªåŒºé—´ä¸ºä¸€ä¸ªç±»åˆ«ï¼Œæ¥ç€å€ŸåŠ©äº onehot encoding å°±å˜ä¸ºä¸€ç³»åˆ—æ˜¯å¦çš„è§£é‡Šå‹ç‰¹å¾ã€‚ä¾‹å¦‚å†å²æœˆè®¢å• 0~5 ä¸ºä½é¢‘ã€6~15 ä¸ºä¸­é¢‘ã€ å¤§äº16ä¸ºé«˜é¢‘ï¼Œ è®¢å•é‡10æ•°å­—å°±å¯ä»¥å˜ä¸º [0,1,0] è¿™ä¸‰ç»´ç‰¹å¾ã€‚

1ã€è‡ªå®šä¹‰åˆ†æ¡¶å¯ä»¥åˆ©ç”¨ä¸Šé¢å›ºå®šå®½åº¦åˆ†æ¡¶çš„æœ€åä¸€ç§æ–¹å¼ï¼Œä¿®æ”¹æˆè‡ªå·±æƒ³è¦çš„åˆ†æ¡¶é—´éš”å°±å¥½ã€‚

2ã€ä¹Ÿå¯ä»¥é‡‡ç”¨ Pandas çš„ map æ–¹å¼ï¼š

```python
def map_age(age_x):
    if age_x <= 18:
        return 1
    elif x <= 20:
        return 2
    elif x <= 35:
        return 3
    elif x <= 45:
        return 4
    else:
        return 5

data_df['age'] = data_df['age'].map(lambda x : map_age(x))
```

### 2.2.2.3 è‡ªé€‚åº”åˆ†æ¡¶ / åˆ†ä½æ•°åˆ†æ¡¶ (Adaptive Binning)

ä¸ç®¡æ˜¯å›ºå®šå®½åº¦åˆ†æ¡¶è¿˜æ˜¯è‡ªå®šä¹‰åˆ†æ¡¶ï¼Œåˆ†æ¡¶çš„æ•ˆæœéƒ½å¾ˆéš¾ä½¿å¾—ç»“æœèƒ½å¤Ÿå‘ˆç°å‡åŒ€åˆ†å¸ƒï¼Œæœ‰çš„æ¡¶å¤šï¼Œæœ‰çš„æ¡¶å¾ˆå°‘ç”šè‡³ä¸ºç©ºã€‚äºæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨åˆ†ä½æ•°åˆ†æ¡¶æ¥è‡ªé€‚åº”åœ°åšåˆ’åˆ†ï¼Œä½¿å¾—ç»“æœæ›´åŠ å‡åŒ€ä¸€äº›ã€‚ä¸€èˆ¬å¸¸ç”¨çš„æœ‰2åˆ†ä½ç‚¹ï¼Œ4åˆ†ä½ç‚¹å’Œ10åˆ†ä½ç‚¹ç”¨ä»¥åˆ†æ¡¶ã€‚

![](/img/media/15523923167184.jpg)

è§‚å¯Ÿæ•°æ®å¯ä»¥çœ‹å‡ºæœ‰ä¸€å®šå³åçš„è¶‹åŠ¿ï¼Œæˆ‘ä»¬å…ˆåˆ©ç”¨å››åˆ†ä½ç‚¹çœ‹ä¸‹æ•°æ®æƒ…å†µï¼š

```python
quantile_list = [0, .25, .5, .75, 1.]
quantiles = fcc_survey_df['Income'].quantile(quantile_list)
quantiles

# Output
------
0.00      6000.0
0.25     20000.0
0.50     37000.0
0.75     60000.0
1.00    200000.0
Name: Income, dtype: float64
```

åœ¨æŸ±çŠ¶å›¾ä¸Šç”»å‡ºåˆ†ä½ç‚¹æ ‡çº¿ï¼š

```python
fig, ax = plt.subplots()
fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3', 
                             edgecolor='black', grid=False)
for quantile in quantiles:
    qvl = plt.axvline(quantile, color='r')
ax.legend([qvl], ['Quantiles'], fontsize=10)
ax.set_title('Developer Income Histogram with Quantiles', 
             fontsize=12)
ax.set_xlabel('Developer Income', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
```

![](/img/media/15523937291720.jpg)


åˆ©ç”¨ qcut åŸºäºåˆ†ä½ç‚¹æ¥åˆ†æ¡¶ï¼š
```python
quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']
fcc_survey_df['Income_quantile_range'] = pd.qcut(
                                            fcc_survey_df['Income'], 
                                            q=quantile_list)
fcc_survey_df['Income_quantile_label'] = pd.qcut(
                                            fcc_survey_df['Income'], 
                                            q=quantile_list,       
                                            labels=quantile_labels)

fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_quantile_range', 
               'Income_quantile_label']].iloc[4:9]
```
![](/img/media/15523938726097.jpg)

å½“ç„¶ï¼Œåˆ†æ¡¶ä¹‹åå¾—åˆ°äº†ç¦»æ•£å‹çš„æ•°å€¼å‹ç‰¹å¾ï¼Œæˆ–è€…å¯ä»¥çœ‹æˆç±»åˆ«ç‰¹å¾ï¼Œè¿˜éœ€è¦ä¸€å®šçš„å¤„ç†æ‰èƒ½æ›´å¥½åœ°æœåŠ¡äºæ¨¡å‹ã€‚

### 2.3 æ•°æ®èˆå…¥(Rounding)

å¤„ç†è¿ç»­æ€§æ•°æ®ç‰¹å¾å¦‚æ¯”ä¾‹æˆ–è€…ç™¾åˆ†æ¯”ç±»å‹çš„ç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é«˜ç²¾åº¦çš„åŸå§‹æ•°å€¼ï¼Œé€šå¸¸æˆ‘ä»¬å°†å…¶èˆå…¥è¿‘ä¼¼åˆ°æ•°å€¼æ•´å‹å°±å¤Ÿç”¨äº†ï¼Œè¿™äº›æ•´å‹æ•°å€¼å¯ä»¥è¢«è§†ä½œç±»åˆ«ç‰¹å¾æˆ–è€…åŸå§‹æ•°å€¼ï¼ˆå³ç¦»æ•£ç‰¹å¾ï¼‰éƒ½å¯ä»¥ã€‚

ä¸¾ä¸ªä¾‹å­ï¼š

```python
items_popularity = pd.read_csv('datasets/item_popularity.csv',  
                               encoding='utf-8')
items_popularity['popularity_scale_10'] = np.array(
                               np.round((items_popularity['pop_percent'] * 10)),  
                               dtype='int')
items_popularity['popularity_scale_100'] = np.array(
                               np.round((items_popularity['pop_percent'] * 100)),    
                               dtype='int')
items_popularity
```

![](/img/media/15523893068452.jpg)

å¯ä»¥å¾—åˆ°ä¸åŒç²’åº¦ä¸‹çš„è¿‘ä¼¼ç»“æœã€‚å½“ç„¶ï¼Œèˆå…¥è¿‘ä¼¼ç»“æœä¸ä¸€å®šéƒ½æ˜¯ä¹˜ä»¥æŸä¸ªæ•°ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢è®²åˆ†æ¡¶çš„æ—¶å€™å¯ä»¥çœ‹åˆ°ï¼Œå¯ä»¥ç”¨èˆå…¥è¿‘ä¼¼çš„æ–¹å¼æ¥åšï¼Œæ•ˆæœå¯ä»¥åˆ†æ¡¶ã€‚

### 2.4 ç¼ºå¤±å€¼å¤„ç†

å› ä¸ºå„ç§å„æ ·çš„åŸå› ï¼ŒçœŸå®ä¸–ç•Œä¸­çš„è®¸å¤šæ•°æ®é›†éƒ½åŒ…å«ç¼ºå¤±æ•°æ®ï¼Œè¿™ç±»æ•°æ®ç»å¸¸è¢«ç¼–ç æˆç©ºæ ¼ã€NaNsï¼Œæˆ–è€…æ˜¯å…¶ä»–çš„å ä½ç¬¦ï¼ˆæœ‰çš„æ—¶å€™æ˜¯ 0ï¼Œéœ€è¦å…·ä½“åˆ†æï¼‰ã€‚å¯¹äºç¼ºå¤±å€¼ä¸€èˆ¬æœ‰ä¸¤å¤§ç±»å¤„ç†æ–¹å¼ï¼š

- 1. è¡¥å€¼

  - ç®€å•çš„å¯ä»¥æ˜¯è¡¥ä¸€ä¸ªå¹³å‡å€¼ (mean)ã€æˆ–è€…ä¼—æ•° (mode)
  - å¯¹äºå«å¼‚å¸¸å€¼çš„å˜é‡ï¼Œæ›´å¥å£®çš„åšæ³•æ˜¯è¡¥ä¸­ä½æ•° (median)
  - è¿˜å¯ä»¥é€šè¿‡æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼

- 2. ç›´æ¥å¿½ç•¥

  - å°†ç¼ºå¤±ä½œä¸ºä¸€ç§ä¿¡æ¯ç¼–ç å–‚ç»™æ¨¡å‹è¿›è¡Œå­¦ä¹ 

- å¯¹äºç«èµ›è€Œè¨€æœ€å¥½ä¸è¦ç›´æ¥åˆ é™¤ï¼Œæœ€å¥½å¦ä½œ`ç‰¹æ®Šç¼–ç `ï¼Œæˆ–è€…æƒ³åŠæ³•æœ€å¤§ç¨‹åº¦ä¿ç•™ç¼ºå¤±å€¼æ‰€å¸¦æ¥çš„`ä¿¡æ¯`ã€‚ï¼š

  - `ç»Ÿè®¡`æ ·æœ¬çš„ç¼ºå¤±å€¼æ•°é‡ï¼Œä½œä¸ºæ–°çš„ç‰¹å¾ã€‚
  - å°†ç¼ºå¤±æ•°é‡åšä¸€ä¸ª`æ’åº`ï¼Œå¦‚æœå‘ç°3ä»½æ•°æ®ï¼ˆtrainã€testã€unlabeledï¼‰éƒ½å‘ˆé˜¶æ¢¯çŠ¶ï¼Œäºæ˜¯å°±å¯ä»¥æ ¹æ®ç¼ºå¤±æ•°é‡å°†æ•°æ®åˆ’åˆ†ä¸ºè‹¥å¹²éƒ¨åˆ†ï¼Œä½œä¸ºæ–°çš„ç‰¹å¾ã€‚
  - ä½¿ç”¨`éšæœºæ£®æ—`ä¸­çš„ä¸´è¿‘çŸ©é˜µå¯¹ç¼ºå¤±å€¼è¿›è¡Œ`æ’å€¼`ï¼Œä½†è¦æ±‚æ•°æ®çš„å› å˜é‡æ²¡æœ‰ç¼ºå¤±å€¼ã€‚

å¯¹äºç»Ÿè®¡é‡çš„è¡¥å€¼æœ‰ä¸¤ç§æ“ä½œæ–¹å¼ï¼š

1ã€é’ˆå¯¹ Pandas æ–¹å¼ï¼š

```python
# combi.isnull().sum()
# combi.isna().sum()
# isnull is an alias for isna
# imputing missing data
# numerical data
combi['Item_Weight'].fillna(combi['Item_Weight'].mean(), inplace = True)
# categorical data
combi['Outlet_Size'].fillna("missing", inplace = True)
```

2ã€ä½¿ç”¨ [Imputer](https://sklearn.org/modules/generated/sklearn.preprocessing.Imputer.html) ç±»ï¼Œå¯ä»¥æ›´æ–¹ä¾¿çš„æ¥ç»Ÿè®¡åˆ°è¡Œåˆ—ä¸åŒç»´åº¦çš„ä¿¡æ¯ï¼š

```python
>>> import numpy as np
>>> from sklearn.preprocessing import Imputer
>>> imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
>>> print(imp.transform(X))                           
[[ 4\.          2\.        ]
 [ 6\.          3.666...]
 [ 7\.          6\.        ]]
```

ä»£ç æ¨¡æ¿ï¼š

```python
df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)  #å¯¹äºå¤§é‡ç¼ºå¤±æ•°æ®çš„åˆ—å¯ç›´æ¥åˆ é™¤
df = df.dropna()                                               #åˆ é™¤å«æœ‰NaNæ•°æ®çš„è¡Œ
df = df.fillna('-1')                                           #å…¨éƒ¨ç›´æ¥äººå·¥èµ‹å€¼
```

æ³¨æ„ï¼š

* çœ‹å¡«å……æ—¶è¦ä¸è¦åŠ ä¸Š valuesï¼Œä¸ç„¶ç»“æœæ˜¯ä¸€ä¸ª Seriesï¼

### 2.5 ç¼©æ”¾

è¿™é‡Œçš„ç¼©æ”¾å›Šæ‹¬äº†æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–ç­‰ï¼Œè¦å¼„æ¸…æ¥šè¿™ä¸¤è€…ä¹‹é—´çš„å…³ç³»ã€‚

### 2.5.1 æ ‡å‡†åŒ–ç¼©æ”¾ (åˆç§° Z ç¼©æ”¾)
æ ‡å‡†åŒ–ï¼ˆæ— é‡é’¢åŒ–/ä¸­å¿ƒåŒ–ï¼‰æŠŠç‰¹å¾è½¬åŒ–ä¸ºæœä»æ ‡å‡†æ­£å¤ªåˆ†å¸ƒçš„å½¢å¼ï¼Œå…¶å®æ˜¯è®¡ç®—æ ‡å‡†åˆ†æ•° (Standard Score, Z-score)ï¼Œç»è¿‡å¤„ç†çš„æ•°æ®ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä½¿å¾—æ•°å€¼ç‰¹å¾çš„ç®—æœ¯å¹³å‡æ•°ä¸ºé›¶ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚

$$
x^\prime =\frac{x-\mu}{\sigma}
$$

å…¶ä¸­ $\mu$ ä¸ºæ‰€æœ‰æ ·æœ¬æ•°æ®çš„å‡å€¼ï¼Œ$\sigma$ ä¸ºæ‰€æœ‰æ ·æœ¬æ•°æ®çš„æ ‡å‡†å·®ã€‚

Sklearn æœ‰ä¸¤ç§æ–¹æ³•å®ç°ï¼š
1ã€ä½¿ç”¨ [sklearn.preprocessing.scale()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥å°†ç»™å®šæ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.scale(X)

>>> X_scaled                                          
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([ 0.,  0.,  0.])

>>> X_scaled.std(axis=0)
array([ 1.,  1.,  1.])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) ç±»ï¼Œä½¿ç”¨è¯¥ç±»çš„å¥½å¤„åœ¨äºå¯ä»¥ä¿å­˜è®­ç»ƒé›†ä¸­çš„å‚æ•°ï¼ˆå‡å€¼ã€æ–¹å·®ï¼‰ç›´æ¥ä½¿ç”¨å…¶å¯¹è±¡è½¬æ¢æµ‹è¯•é›†æ•°æ®ã€‚

```python
>>> scaler = preprocessing.StandardScaler().fit(X)
>>> scaler
StandardScaler(copy=True, with_mean=True, with_std=True)

>>> scaler.mean_                                      
array([ 1. ...,  0. ...,  0.33...])

>>> scaler.std_                                       
array([ 0.81...,  0.81...,  1.24...])

>>> scaler.transform(X)                               
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> # å¯ä»¥ç›´æ¥ä½¿ç”¨è®­ç»ƒé›†å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œè½¬æ¢
>>> scaler.transform([[-1.,  1., 0.]])                
array([[-2.44...,  1.22..., -0.26...]])
```

æ³¨æ„ï¼š
* è®¡ç®—æ—¶å¯¹æ¯ä¸ªç‰¹å¾åˆ†åˆ«è¿›è¡Œã€‚å°†æ•°æ®æŒ‰ç‰¹å¾ï¼ˆæŒ‰åˆ—è¿›è¡Œï¼‰å‡å»å…¶å‡å€¼ï¼Œå¹¶é™¤ä»¥å…¶æ–¹å·®ã€‚å¾—åˆ°çš„ç»“æœæ˜¯ï¼Œå¯¹äºæ¯ä¸ªç‰¹å¾æ¥è¯´æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨ 0 é™„è¿‘ï¼Œæ–¹å·®ä¸º 1ã€‚
* å¦‚æœä¸ªåˆ«ç‰¹å¾æˆ–å¤šæˆ–å°‘çœ‹èµ·æ¥ä¸æ˜¯å¾ˆåƒ**æ ‡å‡†æ­£æ€åˆ†å¸ƒ(å…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®)**ï¼Œé‚£ä¹ˆå®ƒä»¬çš„è¡¨ç°åŠ›å¯èƒ½ä¼šè¾ƒå·®ã€‚
* ä¸å…ç–« outlierï¼Ÿ
* å¯¹ç›®æ ‡å˜é‡ä¸ºè¾“å…¥ç‰¹å¾çš„å…‰æ»‘å‡½æ•°çš„æ¨¡å‹ï¼Œå…¶è¾“å…¥ç‰¹å¾çš„å¤§å°æ¯”è¾ƒæ•æ„Ÿï¼Œå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ç¼©æ”¾æ¯”è¾ƒæœ‰æ•ˆã€‚
* å¯¹äºç¨€ç–æ•°æ®ï¼Œå¯ä»¥æ¥å— scipy.sparse çš„çŸ©é˜µä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶æŒ‡å®šå‚æ•°with_mean=False å–æ¶ˆä¸­å¿ƒåŒ–ï¼ˆcentering æ˜¯ç ´åæ•°æ®ç¨€ç–æ€§çš„åŸå› ï¼‰ï¼Œwith_std=False åˆ™ä¸åš scaling å¤„ç†ã€‚

å¦‚æœæ•°å€¼ç‰¹å¾åˆ—ä¸­å­˜åœ¨æ•°å€¼æå¤§æˆ–æå°çš„ outlierï¼ˆé€šè¿‡EDAå‘ç°ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ [sklearn.preprocessing.RobustScaler](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) ï¼Œåº”è¯¥ä½¿ç”¨æ›´ç¨³å¥ï¼ˆrobustï¼‰çš„ç»Ÿè®¡æ•°æ®ï¼šç”¨ä¸­ä½æ•°è€Œä¸æ˜¯ç®—æœ¯å¹³å‡æ•°ï¼Œç”¨åˆ†ä½æ•°ï¼ˆquantileï¼‰è€Œä¸æ˜¯æ–¹å·®ã€‚è¿™ç§æ ‡å‡†åŒ–æ–¹æ³•æœ‰ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼šï¼ˆåˆ†ä½æ•°ä¸‹é™ï¼Œåˆ†ä½æ•°ä¸Šé™ï¼‰ï¼Œæœ€å¥½é€šè¿‡EDAçš„æ•°æ®å¯è§†åŒ–ç¡®å®šã€‚å…ç–« outlierã€‚

### 2.5.2 åŒºé—´ç¼©æ”¾ (Scaling)

æœ€å¤§æœ€å°å€¼ç¼©æ”¾å’Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ä¸¤ç§ç¼©æ”¾å±äº**åŒºé—´ç¼©æ”¾**ï¼Œä½¿ç”¨è¿™ç§ç¼©æ”¾çš„ç›®çš„åŒ…æ‹¬å®ç°ç‰¹å¾æå°æ–¹å·®çš„é²æ£’æ€§ä»¥åŠåœ¨ç¨€ç–çŸ©é˜µä¸­ä¿ç•™é›¶å…ƒç´ ã€‚

### 2.5.2.1 æœ€å¤§æœ€å°å€¼ç¼©æ”¾

æœ€å¤§æœ€å°ç¼©æ”¾æ˜¯å°†ç‰¹å¾ç¼©æ”¾åˆ°ç»™å®šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´ï¼Œé€šå¸¸åœ¨é›¶å’Œä¸€ä¹‹é—´ã€‚


$$
{x}^\prime=\frac{x-x_{Min}}{x_{Max}-x_{Min}}
$$


1ã€ä½¿ç”¨ [sklearn.preprocessing.minmax_scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) å‡½æ•°å®ç°ï¼š

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.minmax_scale(X)

>>> X_scaled                                          
array([[0.5       , 0.        , 1.        ],
       [1.        , 0.5       , 0.33333333],
       [0.        , 1.        , 0.        ]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([0.5       , 0.5       , 0.44444444])

>>> X_scaled.std(axis=0)
array([0.40824829, 0.40824829, 0.41573971])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) å®ç°ï¼š

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> min_max_scaler = preprocessing.MinMaxScaler()
>>> X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[ 0.5       ,  0.        ,  1.        ],
       [ 1.        ,  0.5       ,  0.33333333],
       [ 0.        ,  1.        ,  0.        ]])

>>> # å°†ç›¸åŒçš„ç¼©æ”¾åº”ç”¨åˆ°æµ‹è¯•é›†æ•°æ®ä¸­
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5       ,  0.        ,  1.66666667]])


>>> # ç¼©æ”¾å› å­ç­‰å±æ€§
>>> min_max_scaler.scale_                             
array([ 0.5       ,  0.5       ,  0.33...])

>>> min_max_scaler.min_                               
array([ 0.        ,  0.5       ,  0.33...])
```

å½“ç„¶ï¼Œåœ¨æ„é€ ç±»å¯¹è±¡çš„æ—¶å€™ä¹Ÿå¯ä»¥ç›´æ¥æŒ‡å®šæœ€å¤§æœ€å°å€¼çš„èŒƒå›´ï¼šfeature_range=(min, max)ï¼Œæ­¤æ—¶åº”ç”¨çš„å…¬å¼å˜ä¸ºï¼š

```python
X_std=(X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0))
X_scaled=X_std/(max-min)+min
```

ğŸ½ æ³¨æ„ï¼š
* è¿™ç§å½’ä¸€åŒ–æ–¹æ³•æ¯”è¾ƒé€‚ç”¨åœ¨æ•°å€¼æ¯”è¾ƒé›†ä¸­çš„æƒ…å†µã€‚
* ä¸¤ä¸ªç¼ºé™·ï¼š
  * å½“æœ‰æ–°æ•°æ®åŠ å…¥æ—¶ï¼Œå¯èƒ½å¯¼è‡´ max å’Œ min å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°å®šä¹‰ã€‚
  * å¦‚æœ max å’Œ min ä¸ç¨³å®šï¼Œå¾ˆå®¹æ˜“ä½¿å¾—å½’ä¸€åŒ–ç»“æœä¸ç¨³å®šï¼Œä½¿å¾—åç»­ä½¿ç”¨æ•ˆæœä¹Ÿä¸ç¨³å®šã€‚å®é™…ä½¿ç”¨ä¸­å¯ä»¥ç”¨ç»éªŒå¸¸é‡å€¼æ¥æ›¿ä»£ max å’Œ minã€‚

### 2.5.2.2 æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾

åœ¨å®é™…æƒ…å†µä¸­,æˆ‘ä»¬ç»å¸¸å¿½ç•¥ç‰¹å¾çš„åˆ†å¸ƒå½¢çŠ¶ï¼Œç›´æ¥ç»è¿‡å»å‡å€¼æ¥å¯¹æŸä¸ªç‰¹å¾è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œå†é€šè¿‡é™¤ä»¥éå¸¸é‡ç‰¹å¾(non-constant features)çš„æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾ã€‚è€Œå¯¹ç¨€ç–æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–ä¼šç ´åç¨€ç–æ•°æ®çš„ç»“æ„ï¼Œè¿™æ ·åšæ²¡ä»€ä¹ˆæ„ä¹‰ã€‚ä½†å¦‚æœç¨€ç–æ•°æ®çš„ç‰¹å¾è·¨è¶Šä¸åŒæ•°é‡çº§çš„æƒ…å†µä¸‹ä¹Ÿæœ€å¥½è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾å°±å¯ä»¥æ´¾ä¸Šç”¨åœºäº†ã€‚

æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾æŒ‰ç…§æ¯ä¸ªç‰¹å¾çš„æœ€å¤§ç»å¯¹å€¼è¿›è¡Œç¼©æ”¾ï¼ˆé™¤ä»¥æœ€å¤§ç»å¯¹å€¼ï¼‰ï¼Œä½¿å¾—æ¯ä¸ªç‰¹å¾çš„èŒƒå›´å˜æˆäº† $[-1, 1]$ï¼Œè¯¥æ“ä½œä¸ä¼šç§»åŠ¨æˆ–è€…å±…ä¸­æ•°æ®ï¼Œæ‰€ä»¥ä¸ä¼šç ´åç¨€ç–æ€§ã€‚

1ã€ä½¿ç”¨ [sklearn.preprocessing.maxabs_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.maxabs_scale.html) å‡½æ•°å®ç°ï¼š

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.maxabs_scale(X)

>>> X_scaled                                          
array([[ 0.5, -1. ,  1. ],
       [ 1. ,  0. ,  0. ],
       [ 0. ,  1. , -0.5]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([0.5       , 0.        , 0.16666667])

>>> X_scaled.std(axis=0)
array([0.40824829, 0.81649658, 0.62360956])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler) ç±»å®ç°ï¼š

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> max_abs_scaler = preprocessing.MaxAbsScaler()
>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
>>> X_train_maxabs                # doctest +NORMALIZE_WHITESPACE^
array([[ 0.5, -1\. ,  1\. ],
 [ 1\. ,  0\. ,  0\. ],
 [ 0\. ,  1\. , -0.5]])
# æµ‹è¯•é›†
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_maxabs = max_abs_scaler.transform(X_test)
>>> X_test_maxabs                 
array([[-1.5, -1\. ,  2\. ]])
>>> max_abs_scaler.scale_         
array([ 2.,  1.,  2.])
```

ğŸ½æ³¨æ„ï¼š
* ä½¿ç”¨æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ä¹‹å‰åº”è¯¥ç¡®è®¤ï¼Œè®­ç»ƒæ•°æ®åº”è¯¥æ˜¯å·²ç»é›¶ä¸­å¿ƒåŒ–æˆ–è€…æ˜¯ç¨€ç–æ•°æ®ã€‚
* è¯¥æ“ä½œä¸ä¼šç§»åŠ¨æˆ–è€…å±…ä¸­æ•°æ®ï¼Œæ‰€ä»¥ä¸ä¼šç ´åç¨€ç–æ€§ã€‚

### 2.5.3 å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

å½’ä¸€åŒ–æ˜¯**ç¼©æ”¾å•ä¸ªæ ·æœ¬ä»¥å…·æœ‰å•ä½èŒƒæ•°**çš„è¿‡ç¨‹ï¼Œå³å˜æ¢åçš„å•è¡Œæ•°æ®æ ·æœ¬çš„èŒƒæ•°ç­‰äº1ï¼ˆå¥½å¤„ï¼ŸğŸ¤”ï¼‰ã€‚å¦‚æœä½ è®¡åˆ’ä½¿ç”¨äºŒæ¬¡å½¢å¼(å¦‚ç‚¹ç§¯æˆ–ä»»ä½•å…¶ä»–æ ¸å‡½æ•°)æ¥é‡åŒ–ä»»ä½•æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦ï¼Œåˆ™æ­¤è¿‡ç¨‹å°†éå¸¸æœ‰ç”¨ã€‚è¿™æ˜¯æ–‡æœ¬åˆ†ç±»æˆ–èšç±»çš„å¸¸ç”¨æ“ä½œï¼Œä¾‹å¦‚ï¼Œå¯¹äºä¸¤ä¸ª TF-IDF å‘é‡çš„ l2-norm è¿›è¡Œç‚¹ç§¯ï¼Œå°±å¯ä»¥å¾—åˆ°è¿™ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚

æ•°æ®å½’ä¸€åŒ–å°±æ˜¯å°†è®­ç»ƒé›†ä¸­æŸä¸€åˆ—æ•°å€¼ç‰¹å¾çš„å€¼ç¼©æ”¾åˆ°0å’Œ1ä¹‹é—´ã€‚

**æ³¨æ„å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–çš„åŒºåˆ«**ï¼šæ ‡å‡†åŒ–ä½œç”¨äºæ¯ä¸ªç‰¹å¾åˆ—ï¼Œé€šè¿‡å»å‡å€¼å’Œç¼©æ”¾ä»¥æ–¹å·®å€¼çš„æ–¹å¼å°†æ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾åˆ—è½¬åŒ–åˆ°åŒä¸€é‡çº²ä¸‹ï¼›å½’ä¸€åŒ–ä½œç”¨äºæ¯ä¸€æ•°æ®è¡Œï¼Œé€šè¿‡ç¼©æ”¾ä»¥åŸæ ·æœ¬çš„æŸä¸ªèŒƒæ•°ä½¿å¾—è®¡ç®—æ ·æœ¬é—´ç›¸ä¼¼åº¦çš„æ—¶å€™æœ‰ç»Ÿä¸€çš„æ ‡å‡†ã€‚

1ã€[sklearn.preprocessing.normalize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) å‡½æ•°æä¾›äº†ä¸€ä¸ªå¿«é€Ÿç®€å•çš„æ–¹æ³•åœ¨ç±»ä¼¼æ•°ç»„çš„æ•°æ®é›†ä¸Šæ‰§è¡Œæ“ä½œï¼Œä½¿ç”¨ `l1` æˆ– `l2`èŒƒå¼:

```python
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> X_normalized = preprocessing.normalize(X, norm='l2')

>>> X_normalized                                      
array([[ 0.40..., -0.40...,  0.81...],
 [ 1\.  ...,  0\.  ...,  0\.  ...],
 [ 0\.  ...,  0.70..., -0.70...]])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.Normalizer](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) ç±»æ¥å½’ä¸€åŒ–ï¼ŒæŠŠæ¯ä¸€è¡Œæ•°æ®å½’ä¸€åŒ–ï¼Œä½¿ä¹‹æœ‰å•ä½èŒƒæ•°ï¼ˆUnit Normï¼‰ï¼Œnorm çš„ç§ç±»å¯ä»¥é€‰l1ã€l2æˆ–maxã€‚ä¸å…ç–«outlierã€‚


$$
\vec{x^{\prime}}=\frac{\vec{x}}{l(\vec{x})}
$$


å…¶ä¸­ $l$ è¡¨ç¤º $norm$ å‡½æ•°ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ `fit` æ–¹æ³•æ˜¯æ— ç”¨çš„ï¼šè¯¥ç±»æ˜¯æ— çŠ¶æ€çš„ï¼Œå› ä¸ºè¯¥æ“ä½œç‹¬ç«‹å¯¹å¾…æ ·æœ¬ã€‚

```python
>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
>>> normalizer
Normalizer(copy=True, norm='l2')
>>> normalizer.transform(X)                            
array([[ 0.40..., -0.40...,  0.81...],
 [ 1\.  ...,  0\.  ...,  0\.  ...],
 [ 0\.  ...,  0.70..., -0.70...]])

>>> normalizer.transform([[-1.,  1., 0.]])             
array([[-0.70...,  0.70...,  0\.  ...]])
```

### 2.5.4 å¸¦æœ‰å¼‚å¸¸å€¼çš„ç¼©æ”¾

å¦‚æœä½ çš„æ•°æ®åŒ…å«è®¸å¤šå¼‚å¸¸å€¼ï¼Œä½¿ç”¨å‡å€¼å’Œæ–¹å·®ç¼©æ”¾å¯èƒ½å¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ robust_scale ä»¥åŠ RobustScaler ä½œä¸ºæ›¿ä»£å“ã€‚å®ƒä»¬å¯¹ä½ çš„æ•°æ®çš„ä¸­å¿ƒå’ŒèŒƒå›´ä½¿ç”¨æ›´æœ‰é²æ£’æ€§çš„ä¼°è®¡ã€‚

1ã€ä½¿ç”¨ [sklearn.preprocessing.robust_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html) å‡½æ•°ï¼š

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -2.,  2.],
...               [ -2.,  1.,  3.],
...               [ 4.,  1., -2.]])
>>> X_scaled = preprocessing.robust_scale(X)

>>> X_scaled                                          
array([[ 0. , -2. ,  0. ],
       [-1. ,  0. ,  0.4],
       [ 1. ,  0. , -1.6]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([ 0.        , -0.66666667, -0.4       ])

>>> X_scaled.std(axis=0)
array([0.81649658, 0.94280904, 0.86409876])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler) ç±»ï¼š

```python
>>> from sklearn.preprocessing import RobustScaler
>>> X = [[ 1., -2.,  2.],
...      [ -2.,  1.,  3.],
...      [ 4.,  1., -2.]]
>>> transformer = RobustScaler().fit(X)
>>> transformer
RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
       with_scaling=True)
>>> transformer.transform(X)
array([[ 0. , -2. ,  0. ],
       [-1. ,  0. ,  0.4],
       [ 1. ,  0. , -1.6]])
```

### 2.5.5 ç¨€ç–æ•°æ®çš„ç¼©æ”¾

ä¸­å¿ƒåŒ–ç¨€ç–ï¼ˆçŸ©é˜µï¼‰æ•°æ®ä¼šç ´åæ•°æ®çš„ç¨€ç–ç»“æ„ï¼Œå› æ­¤å¾ˆå°‘æœ‰ä¸€ä¸ªæ¯”è¾ƒæ˜æ™ºçš„å®ç°æ–¹å¼ã€‚ä½†æ˜¯ç¼©æ”¾ç¨€ç–è¾“å…¥æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå°¤å…¶æ˜¯å½“å‡ ä¸ªç‰¹å¾åœ¨ä¸åŒçš„é‡çº§èŒƒå›´æ—¶ï¼Œæœ€æ¨èçš„ç¼©æ”¾æ–¹å¼æ˜¯é‡‡ç”¨æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ï¼Œå…·ä½“æ“ä½œæ–¹å¼å‚è€ƒä¸Šè¿°å¯¹åº”ç« èŠ‚ã€‚

### 2.5.6 å¯¹æ•°ç¼©æ”¾ï¼ˆæœ‰ååº¦çš„æ­£æ€åˆ†å¸ƒï¼‰

å¦‚æœæ•°æ®ä¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œå°¤å…¶æ˜¯æ•°æ®çš„å¹³å‡æ•°å’Œä¸­ä½æ•°ç›¸å·®å¾ˆå¤§çš„æ—¶å€™ï¼ˆè¡¨ç¤ºæ•°æ®éå¸¸æ­ªæ–œï¼‰ã€‚

1ã€å¯¹ Numpy Array ç±»å‹çš„æ•°æ®å¤„ç†ï¼š

```python
log_data = np.log(data)
# fcc_survey_df['Income_log'] = np.log((1+ fcc_survey_df['Income']))
```

2ã€å¯¹ Pandas DataFrame æ•°æ®çš„å¤„ç†ï¼š

```python
data_df[col] = data_df[col].map(lambda x : np.log1p(x))
```

### 2.5.7 å…¶ä»–ç¼©æ”¾å¾…æ•´ç†

* å¹³æ–¹æ ¹ç¼©æ”¾
* åä½™åˆ‡å‡½æ•°ç¼©æ”¾

### 2.6 ç‰¹å¾äº¤å‰ (Feature Interaction) / ç‰¹å¾ç»„åˆ (Feature Crosses)
é€šè¿‡ç‰¹å¾ç»„åˆå¤šä¸ªç›¸å…³ç‰¹å¾æå–å‡ºå…¶ç›¸å…³çš„è§„å¾‹ã€‚

### 2.6.1 ç»„åˆç‰¹å¾

* å¯ä»¥å¯¹ä¸¤ä¸ªæ•°å€¼å˜é‡è¿›è¡ŒåŠ  ($X_1 + X_2$)ã€å‡ ($X_1 - X_2$)ã€ä¹˜ ($X_1 \times X_2$)ã€é™¤ ($X_1/X_2$)ã€ç»å¯¹å€¼ ($\vert X_1 - X_2\vert$)ç­‰æ“ä½œã€‚

* æ±‚æ–œç‡ã€å˜åŒ–æ¯”ç‡ã€å¢é•¿å€æ•°ã€$max(X_1, X_2)â€‹$ï¼Œ$min(X_1, X_2)â€‹$ï¼Œ$X_1 xor X_2â€‹$ç­‰ã€‚

### 2.6.2 ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé€šè¿‡å¢åŠ ä¸€äº›è¾“å…¥æ•°æ®çš„éçº¿æ€§ç‰¹å¾æ¥å¢åŠ æ¨¡å‹çš„å¤æ‚åº¦é€šå¸¸æ˜¯æœ‰æ•ˆçš„ã€‚ä¸€ä¸ªç®€å•é€šç”¨çš„åŠæ³•æ˜¯ä½¿ç”¨å¤šé¡¹å¼ç‰¹å¾ï¼Œè¿™å¯ä»¥è·å¾—ç‰¹å¾çš„æ›´é«˜ç»´åº¦å’Œäº’ç›¸é—´å…³ç³»çš„é¡¹ã€‚

1ã€ä½¿ç”¨ [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) ç±»å®ç°ï¼š

```python
>>> import numpy as np
>>> from sklearn.preprocessing import PolynomialFeatures
>>> X = np.arange(6).reshape(3, 2)
>>> X                                                 
array([[0, 1],
 [2, 3],
 [4, 5]])
>>> poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)                             
array([[  1.,   0.,   1.,   0.,   0.,   1.],
 [  1.,   2.,   3.,   4.,   6.,   9.],
 [  1.,   4.,   5.,  16.,  20.,  25.]])
```

$X$ çš„ç‰¹å¾å·²ç»ä» $(X_1, X_2)$  è½¬æ¢ä¸º $(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)$ã€‚

åœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œåªéœ€è¦ç‰¹å¾é—´çš„äº¤äº’é¡¹ï¼Œè¿™å¯ä»¥é€šè¿‡è®¾ç½® `interaction_only=True` æ¥å¾—åˆ°:

```python
>>> X = np.arange(9).reshape(3, 3)
>>> X                                                 
array([[0, 1, 2],
 [3, 4, 5],
 [6, 7, 8]])
>>> poly = PolynomialFeatures(degree=3, interaction_only=True)
>>> poly.fit_transform(X)                             
array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],
 [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],
 [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])
```

$X$ çš„ç‰¹å¾å·²ç»ä» $(X_1, X_2, X_3)$ è½¬æ¢ä¸º $(1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)$ã€‚

### 2.7 éçº¿æ€§è½¬æ¢ï¼ˆä¿®æ­£åˆ†å¸ƒï¼‰

### 2.7.1 æ˜ å°„åˆ°å‡åˆ†åˆ†å¸ƒ (Uniform distribution) ä¸Šçš„è½¬æ¢ï¼ˆåˆ†ä½ç‚¹è½¬æ¢ï¼‰

åˆ©ç”¨åˆ†ä½ç‚¹ä¿¡æ¯æ¥è½¬æ¢ç‰¹å¾ä½¿ä¹‹ç¬¦åˆå‡åŒ€åˆ†å¸ƒï¼Œè¿™ç§è½¬æ¢å€¾å‘äºå°†æœ€å¸¸è§çš„æ•°å€¼æ‰“æ•£ï¼Œå¦‚æ­¤èƒ½å‡å°‘ï¼ˆè¾¹é™…ï¼‰å¼‚å¸¸å€¼çš„å½±å“ (åœ¨è¿™æ–¹é¢æ¯”ç¼©æ”¾æ–¹æ³•å¥½)ã€‚ ç„¶è€Œï¼Œè¯¥è½¬æ¢ç¡®å®æ‰­æ›²äº†ç‰¹å¾å†…éƒ¨å’Œç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§å’Œè·ç¦»ã€‚å¯ä»¥é‡‡ç”¨ä»¥ä¸‹ä¸¤ç§æ–¹å¼ï¼ŒåŸºäºåˆ†ä½æ•°å‡½æ•°æä¾›éå‚æ•°å˜æ¢ï¼Œå°†æ•°æ®æ˜ å°„åˆ°å…·æœ‰ 0 å’Œ 1 ä¹‹é—´çš„å€¼çš„å‡åŒ€åˆ†å¸ƒã€‚

1ã€[`quantile_transform`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.quantile_transform.html#sklearn.preprocessing.quantile_transform) å‡½æ•°ï¼š

```python
>>> import numpy as np
>>> from sklearn.preprocessing import quantile_transform
>>> rng = np.random.RandomState(0)
>>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)
>>> quantile_transform(X, n_quantiles=10, random_state=0)
array([...])
```

2ã€[`QuantileTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer) ç±»ï¼š

```python
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)
>>> X_train_trans = quantile_transformer.fit_transform(X_train)
>>> X_test_trans = quantile_transformer.transform(X_test)
>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) 
array([ 4.3,  5.1,  5.8,  6.5,  7.9])
```

è¿™ä¸ªç»“æœå¯¹åº”äºä»¥ cm ä¸ºå•ä½çš„è¼ç‰‡é•¿åº¦ã€‚ åº”ç”¨åˆ†ä½æ•°å˜æ¢åï¼Œè¿™äº›æ ‡å¿—æ¥è¿‘å…ˆå‰å®šä¹‰çš„ç™¾åˆ†ä½æ•°ï¼š

```python
>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])
... 
array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])
# æµ‹è¯•
>>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])
... 
array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])
>>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])
... 
array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])
```

### 2.7.2 æ˜ å°„åˆ°æ­£æ€åˆ†å¸ƒ (Gaussian distribution) ä¸Šçš„è½¬æ¢

å¦‚æœæ•°æ®ä¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œå°¤å…¶æ˜¯æ•°æ®çš„å¹³å‡æ•°å’Œä¸­ä½æ•°ç›¸å·®å¾ˆå¤§çš„æ—¶å€™ï¼ˆè¡¨ç¤ºæ•°æ®éå¸¸æ­ªæ–œï¼‰ã€‚è¿™é‡Œä¸»è¦é‡‡ç”¨ä¸€ç§å«åš [Power Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer) çš„æ–¹æ³•ï¼Œè¿™ç§è½¬æ¢é€šè¿‡ä¸€äº›åˆ—å‚æ•°å•è°ƒå˜æ¢ä½¿å¾—æ•°æ®æ›´ç¬¦åˆæ­£å¤ªåˆ†å¸ƒã€‚[`PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer) ç°åœ¨æ”¯æŒä¸¤ç§è½¬æ¢ï¼Œä¸¤è€…éƒ½æœ‰ä¸€ä¸ªå‚æ•° $\lambda$ éœ€è¦è®¾å®šï¼š

* Box-Cox è½¬æ¢ï¼šè¦æ±‚è¾“å…¥æ•°æ®ä¸¥æ ¼ä¸ºæ­£æ•°ã€‚
* Yeo-Johnson å˜æ¢ï¼šåˆ™æ­£æ•°æˆ–è´Ÿæ•°éƒ½ã€‚

å®è·µæ–¹æ³•æœ‰å››ç§ï¼š

1ã€æ¯”è¾ƒç²—ç³™çš„ç‰ˆæœ¬å¯ä»¥ç›´æ¥æŸ¥çœ‹å¯¹æ•°ç¼©æ”¾çš„å®ç°ã€‚

2ã€å¦ä¸€ç§æ–¹å¼æ˜¯[`PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer) ç±»çš„ Box-Cox è½¬æ¢æ“ä½œï¼Œè¿™ä¸ªæ–¹æ³•èƒ½å¤Ÿè®¡ç®—å‡ºèƒ½å¤Ÿæœ€ä½³å‡å°æ•°æ®å€¾æ–œçš„æŒ‡æ•°å˜æ¢æ–¹æ³•ã€‚


$$
\begin{split}x_i^{(\lambda)} =
\begin{cases}
\dfrac{x_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\[8pt]
\ln{(x_i)} & \text{if } \lambda = 0,
\end{cases}\end{split}
$$


```python
>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)
>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))
>>> X_lognormal                                         
array([[1.28..., 1.18..., 0.84...],
       [0.94..., 1.60..., 0.38...],
       [1.35..., 0.21..., 1.09...]])
>>> pt.fit_transform(X_lognormal)                   
array([[ 0.49...,  0.17..., -0.15...],
       [-0.05...,  0.58..., -0.57...],
       [ 0.69..., -0.84...,  0.10...]])
```

ä¸Šé¢ä»£ç æ˜¾ç¤ºåœ°è®¾å®šäº†standardize=Falseï¼Œé»˜è®¤çš„æƒ…å†µä¸‹è½¬æ¢ç»“æœä¼šè¿›è¡Œé›¶å‡å€¼ã€å•ä½æ–¹å·®çš„å½’ä¸€åŒ–æ“ä½œï¼Œå³ç¬¦åˆæ­£æ€åˆ†å¸ƒã€‚

3ã€å¦ä¸€ç§æ–¹å¼æ˜¯[`PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer) ç±»çš„ Yeo-Johnson è½¬æ¢æ“ä½œï¼Œè¿™ä¹Ÿæ˜¯ Sklearn çš„é»˜è®¤é€‰é¡¹ï¼š


$$
\begin{split}x_i^{(\lambda)} =
\begin{cases}
 [(x_i + 1)^\lambda - 1] / \lambda & \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt]
\ln{(x_i) + 1} & \text{if } \lambda = 0, x_i \geq 0 \\[8pt]
-[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) & \text{if } \lambda \neq 2, x_i < 0, \\[8pt]- \ln (- x_i + 1) & \text{if } \lambda = 2, x_i < 0
\end{cases}\end{split}
$$


```python
>>> import numpy as np
>>> from sklearn.preprocessing import PowerTransformer
>>> pt = PowerTransformer()
>>> data = [[1, 2], [3, 2], [4, 5]]
>>> print(pt.fit(data))
PowerTransformer(copy=True, method='yeo-johnson', standardize=True)
>>> print(pt.lambdas_)
[ 1.386... -3.100...]
>>> print(pt.transform(data))
[[-1.316... -0.707...]
 [ 0.209... -0.707...]
 [ 1.106...  1.414...]]
```

4ã€è¿˜å¯ä»¥ä½¿ç”¨ä¸Šé¢æåˆ°çš„åˆ†ä½ç‚¹è½¬æ¢ï¼š

```python
>>> quantile_transformer = preprocessing.QuantileTransformer(
...     output_distribution='normal', random_state=0)
>>> X_trans = quantile_transformer.fit_transform(X)
>>> quantile_transformer.quantiles_ 
array([[4.3...,   2...,     1...,     0.1...],
       [4.31...,  2.02...,  1.01...,  0.1...],
       [4.32...,  2.05...,  1.02...,  0.1...],
       ...,
       [7.84...,  4.34...,  6.84...,  2.5...],
       [7.87...,  4.37...,  6.87...,  2.5...],
       [7.9...,   4.4...,   6.9...,   2.5...]])
```

### 2.8 éçº¿æ€§ç¼–ç 

- å¤šé¡¹å¼æ ¸ã€é«˜æ–¯æ ¸ç­‰ç¼–ç 
- å°†éšæœºæ£®æ—æ¨¡å‹çš„å¶èŠ‚ç‚¹è¿›è¡Œç¼–ç å–‚ç»™çº¿æ€§æ¨¡å‹
- åŸºå› ç®—æ³•ä»¥åŠå±€éƒ¨çº¿æ€§åµŒå…¥ã€è°±åµŒå…¥ã€t-SNE ç­‰

### 2.8.1 ç”¨åŸºå› ç¼–ç¨‹åˆ›é€ æ–°ç‰¹å¾

åŸºäºgenetic programmingçš„symbolic regressionï¼Œå…·ä½“çš„åŸç†å’Œå®ç°å‚è§æ–‡æ¡£ã€‚ç›®å‰ï¼Œpythonç¯å¢ƒä¸‹æœ€å¥½ç”¨çš„åŸºå› ç¼–ç¨‹åº“ä¸ºgplearnã€‚åŸºå› ç¼–ç¨‹çš„ä¸¤å¤§ç”¨æ³•ï¼š

- è½¬æ¢ï¼ˆtransformationï¼‰ï¼šæŠŠå·²æœ‰çš„ç‰¹å¾è¿›è¡Œç»„åˆè½¬æ¢ï¼Œç»„åˆçš„æ–¹å¼ï¼ˆä¸€å…ƒã€äºŒå…ƒã€å¤šå…ƒç®—å­ï¼‰å¯ä»¥ç”±ç”¨æˆ·è‡ªè¡Œå®šä¹‰ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨åº“ä¸­è‡ªå¸¦çš„å‡½æ•°ï¼ˆå¦‚åŠ å‡ä¹˜é™¤ã€minã€maxã€ä¸‰è§’å‡½æ•°ã€æŒ‡æ•°ã€å¯¹æ•°ï¼‰ã€‚ç»„åˆçš„ç›®çš„ï¼Œæ˜¯åˆ›é€ å‡ºå’Œç›®æ ‡yå€¼æœ€â€œç›¸å…³â€çš„æ–°ç‰¹å¾ã€‚è¿™ç§ç›¸å…³ç¨‹åº¦å¯ä»¥ç”¨spearmanæˆ–è€…pearsonçš„ç›¸å…³ç³»æ•°è¿›è¡Œæµ‹é‡ã€‚spearmanå¤šç”¨äºå†³ç­–æ ‘ï¼ˆå…ç–«å•ç‰¹å¾å•è°ƒå˜æ¢ï¼‰ï¼Œpearsonå¤šç”¨äºçº¿æ€§å›å½’ç­‰å…¶ä»–ç®—æ³•ã€‚
- å›å½’ï¼ˆregressionï¼‰ï¼šåŸç†åŒä¸Šï¼Œåªä¸è¿‡ç›´æ¥ç”¨äºå›å½’è€Œå·²ã€‚

### 2.8.2 ç”¨å†³ç­–æ ‘åˆ›é€ æ–°ç‰¹å¾

åœ¨å†³ç­–æ ‘ç³»åˆ—çš„ç®—æ³•ä¸­ï¼ˆå•æ£µå†³ç­–æ ‘ã€GBDTã€éšæœºæ£®æ—ï¼‰ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬éƒ½ä¼šè¢«æ˜ å°„åˆ°å†³ç­–æ ‘çš„ä¸€ç‰‡å¶å­ä¸Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ ·æœ¬ç»è¿‡æ¯ä¸€æ£µå†³ç­–æ ‘æ˜ å°„åçš„indexï¼ˆè‡ªç„¶æ•°ï¼‰æˆ–one-hot-vectorï¼ˆå“‘ç¼–ç å¾—åˆ°çš„ç¨€ç–çŸ¢é‡ï¼‰ä½œä¸ºä¸€é¡¹æ–°çš„ç‰¹å¾ï¼ŒåŠ å…¥åˆ°æ¨¡å‹ä¸­ã€‚

å…·ä½“å®ç°ï¼šapply() ä»¥åŠ decision_path() æ–¹æ³•ï¼Œåœ¨ scikit-learn å’Œ xgboost é‡Œéƒ½å¯ä»¥ç”¨ã€‚

- å†³ç­–æ ‘ã€åŸºäºå†³ç­–æ ‘çš„ ensemble
  - spearman correlation coefficient

- çº¿æ€§æ¨¡å‹ã€SVMã€ç¥ç»ç½‘ç»œ
  - å¯¹æ•°ï¼ˆlogï¼‰
  - pearson correlation coefficient

### 2.9 è¡Œç»Ÿè®¡é‡

é™¤äº†å¯¹åŸå§‹æ•°å€¼å˜é‡è¿›è¡Œå¤„ç†å¤–ï¼Œç›´æ¥å¯¹è¡Œå‘é‡è¿›è¡Œç»Ÿè®¡ä¹Ÿä½œä¸ºä¸€ç±»ç‰¹å¾ã€‚

* ä¾‹å¦‚ç»Ÿè®¡è¡Œå‘é‡ä¸­çš„ç©ºå€¼ä¸ªæ•°ã€é›¶å€¼ä¸ªæ•°ã€æ­£è´Ÿå€¼ä¸ªæ•°
* ä»¥åŠå‡å€¼ã€æ–¹å·®ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€[ååº¦ã€å³°åº¦](https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/how-skewness-and-kurtosis-affect-your-distribution/)ç­‰

1ã€ååº¦ã€å³°åº¦è®¡ç®—ï¼š

```python
import pandas as pd
x = [53, 61, 49, 66, 78, 47]
s = pd.Series(x)
print(s.skew())
print(s.kurt())
```

### 2.10 æ•°å­—å‹ç‰¹å¾é‡æ„

é€šè¿‡è°ƒæ•´æ•°å­—å•ä½ç­‰æ–¹å¼ï¼Œå¯ä»¥è°ƒæ•´æ•°å­—å¤§å°ã€‚ ä¾‹å¦‚ 6500 å…‹ å¯ä»¥è¡¨è¾¾6.5åƒå…‹ï¼› ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥æ‹†è§£è¡¨è¾¾ä¸º6åƒå…‹ã€0.5åƒå…‹ç­‰ã€‚ä¼¼ä¹æ˜¯æ²¡å•¥é“ç†ï¼Œä½†æ˜¯ç¡®æœ‰æ—¶æœ‰ç”¨ã€‚æ¯”å¦‚è¿™ä¸ª[æ¯”èµ›](https://www.datafountain.cn/competitions/337/details/rule?id=84982)ï¼Œå…¶ä¸­ä¸€ä¸ªå……å€¼é‡‘é¢çš„ç‰¹å¾ï¼Œåˆ¤æ–­çœ‹æ˜¯å¦æ•°å€¼ä¸ºæ•´æ•°å¯ä»¥æ„æˆä¸€ä¸ªå¼ºç‰¹å¾ã€‚

### 3. ç±»åˆ«ç‰¹å¾ / æ ‡ç§°ç‰¹å¾ / å®šæ€§ç‰¹å¾ (Categorical Features)

ç±»åˆ«ç‰¹å¾å–å€¼å¯ä»¥æ˜¯æ•°å€¼ç±»å‹ï¼Œä½†æ˜¯æ•°å€¼æ²¡æœ‰ä»»ä½•æ•°å­¦æ„ä¹‰ï¼Œä¸èƒ½åšæ•°å­¦è¿ç®—ã€‚ç±»åˆ«ç‰¹å¾ä¸ä»…å¯ä»¥ä»åŸå§‹æ•°æ®ä¸­ç›´æ¥è·å¾—ï¼Œè¿˜å¯ä»¥é€šè¿‡æ•°å€¼ç‰¹å¾ç¦»æ•£åŒ–å¾—åˆ°ã€‚

### 3.1 è‡ªç„¶æ•°ç¼–ç 

- ç±»åˆ«ç‰¹å¾è¦å˜æˆæ•°å€¼æ‰èƒ½å–‚ç»™æ¨¡å‹
- é‡‡ç”¨è‡ªç„¶æ•°ç¼–ç ç»™æ¯ä¸€ä¸ªç±»åˆ«åˆ†é…ä¸€ä¸ªç¼–å·
- é™¤éç±»åˆ«ç‰¹å¾æœ¬èº«æœ‰é¡ºåºç‰¹å¾å¤–ï¼Œç±»åˆ«ç‰¹å¾çš„æ•°å€¼å¤§å°æ²¡æœ‰æ„ä¹‰ï¼Œæ‰€ä»¥è‡ªç„¶æ•°ç¼–ç æ•ˆæœä¸€èˆ¬ä¸æ˜¯å¾ˆå¥½ï¼Œå¯ä»¥å¯¹ç±»åˆ«ç¼–å·è¿›è¡Œæ´—ç‰Œï¼Œè®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œèåˆè¿›ä¸€æ­¥æå‡æ¨¡å‹æ•ˆæœ
- ä½†æ˜¯ä¸€èˆ¬æ¥è¯´æ“ä½œæ¶ˆè€—å†…å­˜å°ï¼Œè®­ç»ƒæ—¶é—´å¿«

1ã€ä½¿ç”¨ [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) ç±»å°†ç±»åˆ«ç‰¹å¾ç¼–ç åˆ°ä¸€ä¸ª $n\_samples$ å¤§å°çš„ $[0, n\_classes-1]$ å†…å–å€¼çš„çŸ¢é‡ï¼Œæ¯ä¸ªæ ·æœ¬ä»…å¯¹åº”ä¸€ä¸ª labelï¼Œå³è¾“å…¥å¤§å°ä¸º (n_samples, n_features) çš„æ•°ç»„ï¼š

```python
>>> from sklearn.preprocessing import OrdinalEncoder
>>> enc = OrdinalEncoder()
>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
>>> enc.fit(X)
... 
OrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)
>>> enc.categories_
[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
>>> enc.transform([['Female', 3], ['Male', 1]])
array([[0., 2.],
       [1., 0.]])
>>> enc.inverse_transform([[1, 0], [0, 1]])
array([['Male', 1],
       ['Female', 2]], dtype=object)
```

fit_transform() å‡½æ•°å°±æ˜¯å…ˆ fit() å®Œç›´æ¥ transform()ã€‚

2ã€ä½¿ç”¨ [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) ç±»å°†ç±»åˆ«æ ‡ç­¾ï¼ˆTarget labelsï¼‰ç¼–ç åˆ° $[0, n\_classes-1]â€‹$ å†…å–å€¼çš„ç»“æœï¼Œè¾“å…¥å¤§å°ä¸º(n_samples,) çš„æ•°ç»„ï¼š

```python
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1, 2, 2, 6])
LabelEncoder()
>>> le.classes_
array([1, 2, 6])
>>> le.transform([1, 1, 2, 6]) 
array([0, 0, 1, 2]...)
>>> le.inverse_transform([0, 0, 1, 2])
array([1, 1, 2, 6])
```

### 3.2 ç‹¬çƒ­ç¼–ç  (One-Hot Encoding)

é™¤éç±»åˆ«ç‰¹å¾æœ¬èº«æœ‰é¡ºåºç‰¹å¾ï¼Œé‚£ä¹ˆå¯ä»¥ç”¨è‡ªç„¶ç¼–ç ï¼Œé™¤æ­¤ä¹‹å¤–ç±»åˆ«ç‰¹å¾å¤§å°æ²¡æœ‰æ„ä¹‰ï¼Œä¸€èˆ¬é‡‡ç”¨ç‹¬çƒ­ç¼–ç å¾—åˆ°ç¨€ç–çŸ©é˜µã€‚å°†ä¸€ä¸ªç±»åˆ«ç‰¹å¾ç¼–ç æˆ $n\_classesâ€‹$ ç»´åº¦çš„ $0/1â€‹$ å‘é‡ï¼Œå–å¯¹åº”ç±»åˆ«çš„åœ°æ–¹å– 1ï¼Œå…¶ä»–å…¨ä¸ºäº† 0ï¼Œæ‰€ä»¥å¾ˆç¨€ç–ã€‚

1ã€ä½¿ç”¨ [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) ç±»é’ˆå¯¹æ— é¡ºåºæ€§ç±»åˆ«ç‰¹å¾è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œè¾“å…¥å¤§å°ä¸º (n_samples, n_features) çš„æ•°ç»„ï¼š

```python
>>> from sklearn.preprocessing import OneHotEncoder
>>> enc = OneHotEncoder(handle_unknown='ignore')
>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
>>> enc.fit(X)
... 
OneHotEncoder(categorical_features=None, categories=None,
       dtype=<... 'numpy.float64'>, handle_unknown='ignore',
       n_values=None, sparse=True)
>>> enc.categories_
[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
>>> enc.transform([['Female', 1], ['Male', 4]]).toarray()
array([[1., 0., 1., 0., 0.],
       [0., 1., 0., 0., 0.]])
>>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])
array([['Male', 1],
       [None, 2]], dtype=object)
>>> enc.get_feature_names()
array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)
```

å¾—åˆ°çš„ç»“æœå¤§å°æ˜¯ $(ç‰¹å¾ä¸ªæ•° \times æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«ä¸ªæ•°)$ï¼Œä¾‹å¦‚è¿™é‡Œçš„ç»“æœæ˜¯ 5 ç»´çš„å‘é‡ï¼Œå‰ä¸¤ä¸ªè¡¨ç¤ºç”·å¥³çš„ç‰¹å¾ï¼Œåä¸‰ä¸ªæ˜¯æ•´æ•°å‹ç‰¹å¾ã€‚

2ã€ä½¿ç”¨ [LabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) ç±»é’ˆå¯¹ç±»åˆ«æ ‡ç­¾ï¼ˆTarget labelsï¼‰ç‹¬çƒ­ç¼–ç ï¼Œè¾“å…¥å¤§å°ä¸º(n_samples,) çš„æ•°ç»„ï¼š

```python
>>> from sklearn import preprocessing
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6])
array([[1, 0, 0, 0],
       [0, 0, 0, 1]])
```

3ã€ä½¿ç”¨ [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)ï¼š

```python
>>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],
...                    'C': [1, 2, 3]})
>>> pd.get_dummies(df, prefix=['col1', 'col2'])
   C  col1_a  col1_b  col2_a  col2_b  col2_c
0  1       1       0       0       1       0
1  2       0       1       1       0       0
2  3       1       0       0       0       1
```

å½“ç„¶è¿™é‡Œçš„ get_dummies æ˜¯åš One-Hot Encodingï¼Œä¸ Dummy Encodingï¼ˆå“‘ç¼–ç ï¼‰è¿˜æ˜¯æœ‰äº›åŒºåˆ«çš„ï¼ŒçœŸæ­£çš„Dummy Encoding æ˜¯å°†ä¸€ä¸ªç±»åˆ«ç‰¹å¾ç¼–ç æˆ $n\_classes- 1 $ ç»´åº¦çš„ $0/1$ å‘é‡ï¼Œç¼–ç æ—¶è¿™ $n\_classes- 1 $ ä¸ªç±»çš„å¯¹åº”åœ¨å…¶ä½ç½®ä¸Šå–å€¼ä¸º 1ï¼Œå…¶ä»–å– 0ï¼Œå‰©ä¸‹çš„é‚£ä¸ªç±»ç”¨è¿™ $n\_classes- 1 $ å…¨éƒ¨å» 0 çš„çŠ¶æ€è¡¨ç¤ºã€‚

ğŸ½æ³¨æ„ï¼š

* ä½¿ç”¨ç‹¬çƒ­ç¼–ç å°†ç¦»æ•£ç‰¹å¾çš„å–å€¼æ‹“å±•åˆ°äº†æ¬§å¼ç©ºé—´ï¼Œç¦»æ•£ç‰¹å¾çš„æŸä¸ªå–å€¼å°±å¯¹åº”æ¬§å¼ç©ºé—´çš„æŸä¸ªç‚¹ã€‚
* ç¦»æ•£ç‰¹å¾ç‹¬çƒ­ç¼–ç åï¼Œä¼šè®©ç‰¹å¾ä¹‹é—´çš„è·ç¦»è®¡ç®—æ›´åŠ åˆç†ï¼Œæ²¡æœ‰æ²¡æœ‰é¡ºåºæ€§ï¼Œ$x_1 = (1)$, $x_2 = (2)$, $x_3 = (3)$ ä¹‹é—´çš„è·ç¦»å°±æ²¡æœ‰æ„ä¹‰ï¼Œè€Œ $x_1 = (1, 0, 0)$, $x_2 = (0, 1, 0)$, $x_3 = (0, 0, 1)$ ä¹‹é—´çš„è·ç¦»å°±æ›´ make senseã€‚
* ç”¨ï¼šç‹¬çƒ­ç¼–ç ç”¨æ¥è§£å†³ç±»åˆ«å‹æ•°æ®çš„ç¦»æ•£å€¼é—®é¢˜ï¼Œä¸ç”¨ï¼šå°†ç¦»æ•£å‹ç‰¹å¾è¿›è¡Œone-hotç¼–ç çš„ä½œç”¨ï¼Œæ˜¯ä¸ºäº†è®©è·ç¦»è®¡ç®—æ›´åˆç†ï¼Œä½†å¦‚æœç‰¹å¾æ˜¯ç¦»æ•£çš„ï¼Œå¹¶ä¸”ä¸ç”¨ One-Hot ç¼–ç å°±å¯ä»¥å¾ˆåˆç†çš„è®¡ç®—å‡ºè·ç¦»ï¼Œé‚£ä¹ˆå°±æ²¡å¿…è¦è¿›è¡Œ One-Hot ç¼–ç ã€‚
* One-Hot ç¼–ç å¯èƒ½å¼•èµ· dummy variable trapï¼Œå³æˆªå–ï¼ˆæˆ–å« biasï¼‰ä¼šå¼•èµ·[å…±çº¿é—®é¢˜](http://www.jiehuozhe.com/article/3)ï¼Œæ‰€ä»¥è¿™ä¸ªæ—¶å€™ç”¨ Dummy Encoding æ¯”è¾ƒå¥½ã€‚

### 3.3 åˆ†å±‚ç¼–ç 

è¿™ç§ç¼–ç å°±æ˜¯ä¸šåŠ¡ç›¸å…³çš„äº†ï¼Œéœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€‚ä¾‹å¦‚å¯¹äºé‚®æ”¿ç¼–ç æˆ–è€…èº«ä»½è¯å·çš„ç±»åˆ«ç‰¹å¾ï¼Œå¯ä»¥å–ä¸åŒæ•°ä½è¿›è¡Œåˆ†å±‚ï¼Œç„¶åæŒ‰ç…§å±‚æ¬¡è¿›è¡Œè‡ªç„¶æ•°ç¼–ç ã€‚

- [ ] æ±‚å…·ä½“å®ä¾‹ã€‚ğŸ™„

### 3.4 æ•£åˆ—ç¼–ç 

* å¯¹äºæœ‰äº›å–å€¼ç‰¹åˆ«å¤šçš„ç±»åˆ«ç‰¹å¾ï¼Œåˆ©ç”¨ One-Hot Encoding å¾—åˆ°çš„ç‰¹å¾çŸ©é˜µå°±éå¸¸å¾—ç¨€ç–ï¼Œä¸ºå‡å°‘ç¨€ç–ç¨‹åº¦å¯ä»¥åœ¨ç‹¬çƒ­ç¼–ç ä¹‹å‰åˆ©ç”¨æ•£åˆ—ç¼–ç ã€‚
* å®é™…åº”ç”¨ä¸­å¯ä»¥é‡å¤é€‰å–ä¸åŒçš„æ•£åˆ—å‡½æ•°ï¼Œåˆ©ç”¨èåˆçš„æ–¹å¼æ¥æå‡æ¨¡å‹æ•ˆæœã€‚
* æ•£åˆ—æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´ç‰¹å¾å–å€¼å†²çªï¼Œè¿™äº›å†²çªä¼šå‰Šå¼±æ¨¡å‹çš„æ•ˆæœã€‚ğŸ¤”
* è‡ªç„¶æ•°ç¼–ç å’Œåˆ†å±‚ç¼–ç å¯ä»¥çœ‹åšæ•£åˆ—ç¼–ç çš„ç‰¹ä¾‹

* [ ] æ±‚å…·ä½“å®ä¾‹ã€‚Hash ç¼–ç è¯å‘é‡ï¼ŸğŸ™„

### 3.5 è®¡æ•°ç¼–ç  (Count encoding)

* è®¡æ•°ç¼–ç æ˜¯å°†ç±»åˆ«ç‰¹å¾ç”¨å…¶å¯¹åº”çš„è®¡æ•°ä»£æ›¿ï¼Œè¿™å¯¹çº¿æ€§å’Œéçº¿æ€§æ¨¡å‹éƒ½æœ‰æ•ˆã€‚
* è®¡æ•°ç¼–ç å¯¹å¼‚å¸¸å€¼æ¯”è¾ƒæ•æ„Ÿï¼Œç‰¹å¾å–å€¼ä¹Ÿå¯èƒ½å†²çªã€‚[å‚è€ƒ](https://wrosinski.github.io/fe_categorical_encoding/)ğŸ¤”

```python
def count_encode(X, categorical_features, normalize=False):
    print('Count encoding: {}'.format(categorical_features))
    X_ = pd.DataFrame()
    for cat_feature in categorical_features:
        X_[cat_feature] = X[cat_feature].astype(
            'object').map(X[cat_feature].value_counts())
        if normalize:
            X_[cat_feature] = X_[cat_feature] / np.max(X_[cat_feature])
    X_ = X_.add_suffix('_count_encoded')
    if normalize:
        X_ = X_.astype(np.float32)
        X_ = X_.add_suffix('_normalized')
    else:
        X_ = X_.astype(np.uint32)
    return X_
# run
train_count_subreddit = count_encode(X_train, ['subreddit'])
# not normalized
221941    221941
98233      98233
33559      33559
32010      32010
25567      25567
Name: subreddit_count_encoded, dtype: int64
# normalized
1.000000    221941
0.442609     98233
0.151207     33559
0.144228     32010
0.115197     25567
Name: subreddit_count_encoded_normalized, dtype: int64
```

### 3.6 è®¡æ•°æ’åç¼–ç  (LabelCount encoding)

* è®¡æ•°æ’åç¼–ç åˆ©ç”¨è®¡æ•°çš„æ’åå¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œå¯¹çº¿æ€§å’Œéçº¿æ€§æ¨¡å‹éƒ½æœ‰æ•ˆã€‚
* å¯¹å¼‚å¸¸ç‚¹ä¸æ•æ„Ÿï¼Œä¸”ç±»åˆ«ç‰¹å¾å–å€¼ä¸ä¼šå†²çªã€‚

```python
def labelcount_encode(X, categorical_features, ascending=False):
    print('LabelCount encoding: {}'.format(categorical_features))
    X_ = pd.DataFrame()
    for cat_feature in categorical_features:
        cat_feature_value_counts = X[cat_feature].value_counts()
        value_counts_list = cat_feature_value_counts.index.tolist()
        if ascending:
            # for ascending ordering
            value_counts_range = list(
                reversed(range(len(cat_feature_value_counts))))
        else:
            # for descending ordering
            value_counts_range = list(range(len(cat_feature_value_counts)))
        labelcount_dict = dict(zip(value_counts_list, value_counts_range))
        X_[cat_feature] = X[cat_feature].map(
            labelcount_dict)
    X_ = X_.add_suffix('_labelcount_encoded')
    if ascending:
        X_ = X_.add_suffix('_ascending')
    else:
        X_ = X_.add_suffix('_descending')
    X_ = X_.astype(np.uint32)
    return X_
# run
train_lc_subreddit = labelcount_encode(X_train, ['subreddit'])
# descending
0    221941
1     98233
2     33559
3     32010
4     25567
Name: subreddit_labelcount_encoded_descending, dtype: int64
# ascendign
40    221941
39     98233
38     33559
37     32010
36     25567
Name: subreddit_labelcount_encoded_ascending, dtype: int64
```

### 3.7 ç›®æ ‡ç¼–ç  (Target encoding)

* å¯¹äºåŸºæ•°ï¼ˆç±»åˆ«å˜é‡æ‰€æœ‰å¯èƒ½ä¸åŒå–å€¼çš„ä¸ªæ•°ï¼‰å¾ˆå¤§çš„ç¦»æ•£ç‰¹å¾ï¼Œä¾‹å¦‚ IP åœ°å€ã€ç½‘ç«™åŸŸåã€åŸå¸‚åã€å®¶åº­åœ°å€ã€è¡—é“ã€äº§å“ç¼–å·ç­‰ï¼Œä¹‹å‰ä»‹ç»çš„ç¼–ç æ–¹å¼æ•ˆæœå¾€å¾€ä¸å¥½ï¼Œæ¯”å¦‚ï¼š
  * å¯¹äºè‡ªç„¶æ•°ç¼–ç ï¼Œç®€å•æ¨¡å‹å®¹æ˜“æ¬ æ‹Ÿåˆï¼Œè€Œå¤æ‚æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
  * å¯¹äºç‹¬çƒ­ç¼–ç ï¼Œå¾—åˆ°çš„ç‰¹å¾çŸ©é˜µå¤ªç¨€ç–ã€‚
* å¯¹äºé«˜åŸºæ•°ç±»åˆ«å˜é‡çš„ä¸€ç§è§£å†³åŠæ³•æ˜¯åŸºäºç›®æ ‡å˜é‡å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œå³æœ‰ç›‘ç£çš„ç¼–ç æ–¹å¼ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºåˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚
* å¯¹äºåˆ†ç±»é—®é¢˜çš„é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼š
  * é‡‡ç”¨äº¤å‰éªŒè¯çš„æ–¹å¼ï¼Œå°†æ ·æœ¬åˆ’åˆ†ä¸º5ä»½ï¼Œé’ˆå¯¹å…¶ä¸­æ¯ä¸€ä»½æ•°æ®ï¼Œè®¡ç®—ç¦»æ•£ç‰¹å¾æ¯ä¸ªå–å€¼åœ¨å¦å¤–4ä»½æ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹ã€‚
  * ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œä¹Ÿå¯ä»¥é‡‡ç”¨åµŒå¥—çš„äº¤å‰éªŒè¯åˆ’åˆ†æ–¹æ³•ã€‚
* å¯¹äºå›å½’é—®é¢˜çš„é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼š
  * é‡‡ç”¨äº¤å‰éªŒè¯çš„æ–¹å¼ï¼Œè®¡ç®—ç›®æ ‡å˜é‡å‡å€¼å¯¹ç±»åˆ«å˜é‡ç¼–ç ã€‚[å‚è€ƒ](https://wrosinski.github.io/fe_categorical_encoding/) ğŸ¤”

- [ ] æ±‚å®ä¾‹ğŸ™„

```python
def target_encode(X, X_valid, categorical_features, X_test=None,
                  target_feature='target'):
    print('Target Encoding: {}'.format(categorical_features))
    X_ = pd.DataFrame()
    X_valid_ = pd.DataFrame()
    if X_test is not None:
        X_test_ = pd.DataFrame()
    for cat_feature in categorical_features:
        group_target_mean = X.groupby([cat_feature])[target_feature].mean()
        X_[cat_feature] = X[cat_feature].map(group_target_mean)
        X_valid_[cat_feature] = X_valid[cat_feature].map(group_target_mean)
    X_ = X_.astype(np.float32)
    X_ = X_.add_suffix('_target_encoded')
    X_valid_ = X_valid_.astype(np.float32)
    X_valid_ = X_valid_.add_suffix('_target_encoded')
    if X_test is not None:
        X_test_[cat_feature] = X_test[cat_feature].map(group_target_mean)
        X_test_ = X_test_.astype(np.float32)
        X_test_ = X_test_.add_suffix('_target_encoded')
        return X_, X_valid_, X_test_
    return X_, X_valid_
```

### 3.8 ç±»åˆ«ç‰¹å¾ä¹‹é—´äº¤å‰ç»„åˆ

* ç±»åˆ«ç‰¹å¾çš„ç¬›å¡å°”ç§¯æ“ä½œå¯ä»¥äº§ç”Ÿæ–°çš„ç±»åˆ«ç‰¹å¾ï¼Œä½†æ˜¯æ³¨æ„è¿™æ˜¯åœ¨ç±»åˆ«ç‰¹å¾åŸºæ•°ä¸å¤§çš„å‰æä¸‹ã€‚
* è¿˜æœ‰ä¸€ç§äº¤å‰ç»„åˆçš„æ€è·¯æ˜¯åŸºäºåˆ†ç»„ç»Ÿè®¡çš„ç»„åˆã€‚æ±‚å®ä¾‹ ğŸ™„
* å…¶ä»–çš„æ€è·¯å°±æ˜¯åˆ©ç”¨ä¸“ä¸šé¢†åŸŸçŸ¥è¯†è‡ªå·±è¯•äº†ã€‚

1ã€å¯¹äºç¬›å¡å°”ç§¯æ“ä½œä¹Ÿå°±æ˜¯æš´åŠ›ç‰¹å¾ç»„åˆæ—¶å¯ä»¥ç”¨ [itertools.combinations](https://docs.python.org/2/library/itertools.html#itertools.combinations)ï¼š

```python
from itertools import combinations
ralate_var = ['æ˜¯å¦ç»å¸¸é€›å•†åœºçš„äºº', 'æ˜¯å¦å»è¿‡é«˜æ¡£å•†åœº', 'å½“æœˆæ˜¯å¦çœ‹ç”µå½±', 
              'å½“æœˆæ˜¯å¦æ™¯ç‚¹æ¸¸è§ˆ', 'å½“æœˆæ˜¯å¦ä½“è‚²åœºé¦†æ¶ˆè´¹']
print('waiting for group pair features...')
for rv in combinations(ralate_var, 2):
    rv2 = '_'.join(rv) 
    data['relate_' + rv2] = data[rv[0]] * data[rv[1]]
    print(rv2 + 'finished!')
    
for rv in combinations(ralate_var, 3):
    rv2 = '_'.join(rv) 
    data['relate_' + rv2] = data[rv[0]] * data[rv[1]] * data[rv[2]]
    print(rv2 + 'finished!')
    
for rv in combinations(ralate_var, 4):
    rv2 = '_'.join(rv) 
    data['relate_' + rv2] = data[rv[0]] * data[rv[1]] * data[rv[2]] * data[rv[3]]
    print(rv2 + 'finished!')
    
print('All finished!!!')
```

### 3.9 ç±»åˆ«ç‰¹å¾å’Œæ•°å€¼ç‰¹å¾ä¹‹é—´äº¤å‰ç»„åˆ

### 3.9.1 ç‰¹å¾èšåˆ (feature aggregation)

* é€šå¸¸åŸºäºç±»åˆ«ç‰¹å¾çš„æŸä¸ªç±»åˆ«è®¡ç®—æ•°å€¼ç‰¹å¾çš„ä¸€äº›ç»Ÿè®¡é‡ï¼Œä¸€èˆ¬åœ¨å¤šä¸ªè¡¨å¥½æ“ä½œä¸€äº›ã€‚

1ã€ç”¨ N1 å’Œ N2 è¡¨ç¤ºæ•°å€¼ç‰¹å¾ï¼Œç”¨ C1 å’Œ C2 è¡¨ç¤ºç±»åˆ«ç‰¹å¾ï¼Œåˆ©ç”¨ Pandas çš„ groupby æ“ä½œï¼Œå¯ä»¥åˆ›é€ å‡ºä»¥ä¸‹å‡ ç§æœ‰æ„ä¹‰çš„æ–°ç‰¹å¾ï¼ˆå…¶ä¸­ï¼ŒC2 è¿˜å¯ä»¥æ˜¯ç¦»æ•£åŒ–äº†çš„ N1ï¼‰ï¼š

```
median(N1)_by(C1)  \\ ä¸­ä½æ•°
mean(N1)_by(C1)  \\ ç®—æœ¯å¹³å‡æ•°
mode(N1)_by(C1)  \\ ä¼—æ•°
min(N1)_by(C1)  \\ æœ€å°å€¼
max(N1)_by(C1)  \\ æœ€å¤§å€¼
std(N1)_by(C1)  \\ æ ‡å‡†å·®
var(N1)_by(C1)  \\ æ–¹å·®
freq(C2)_by(C1)  \\ é¢‘æ•°

freq(C1) \\è¿™ä¸ªä¸éœ€è¦groupbyä¹Ÿæœ‰æ„ä¹‰
```

ä»…ä»…å°†å·²æœ‰çš„ç±»åˆ«å’Œæ•°å€¼ç‰¹å¾è¿›è¡Œä»¥ä¸Šçš„æœ‰æ•ˆç»„åˆï¼Œå°±èƒ½å¤Ÿå¤§é‡å¢åŠ ä¼˜ç§€çš„å¯ç”¨ç‰¹å¾ã€‚

å°†è¿™ç§æ–¹æ³•å’Œçº¿æ€§ç»„åˆç­‰åŸºç¡€ç‰¹å¾å·¥ç¨‹æ–¹æ³•ç»“åˆï¼ˆä»…ç”¨äºå†³ç­–æ ‘ï¼‰ï¼Œå¯ä»¥å¾—åˆ°æ›´å¤šæœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œå¦‚ï¼š

```
N1 - median(N1)_by(C1)
N1 - mean(N1)_by(C1)
```

å°†å¤šä¸ªç»´åº¦ç‰¹å¾ç›¸äº’äº¤å‰ï¼Œäº§ç”Ÿæ›´å¤šå…·ä½“åœºæ™¯åŒ–çš„ç‰¹å¾ï¼Œä¾‹å¦‚å’Œä¸åŒæ—¶æ®µæ®µã€å’Œä¸åŒçš„åœ°ç†ä½ç½®èŒƒå›´ç»„åˆã€‚

```python
import pandas as pd

# æ ¹æ®å®¢æˆ· id ï¼ˆclient idï¼‰è¿›è¡Œè´·æ¬¾åˆ†ç»„ï¼Œå¹¶è®¡ç®—è´·æ¬¾å¹³å‡å€¼ã€æœ€å¤§å€¼ã€æœ€å°å€¼
stats = loans.groupby('client_id')['loan_amount'].agg(['mean', 'max', 'min'])
stats.columns = ['mean_loan_amount', 'max_loan_amount', 'min_loan_amount']

# å’Œå®¢æˆ·çš„ dataframe è¿›è¡Œåˆå¹¶
stats = clients.merge(stats, left_on = 'client_id', right_index=True, how = 'left')

stats.head(10)
```

![img](/img/media/1652824cd936d8b8imageslim.png)

2ã€äººå·¥æ“ä½œï¼š

```python
start_time = time.time()

for cat_feat in categorical_cols:
    for num_feat in numerical_cols:
        cat_num_mean = train_df.groupby(cat_feat)[num_feat].mean()
        train_df[cat_feat+'_'+num_feat+'_'+'mean'] = train_df[cat_feat].map(cat_num_mean)
        test_df[cat_feat+'_'+num_feat+'_'+'mean'] = test_df[cat_feat].map(cat_num_mean)
print 'elapsed time: ', time.time() - start_time
```



### 4. æ—¶é—´ç‰¹å¾

* æ—¶é—´ç±»ç‰¹å¾æ—¢å¯ä»¥çœ‹åšè¿ç»­å€¼ï¼Œä¹Ÿå¯ä»¥çœ‹åšç¦»æ•£å€¼
* å¯¹äºè¿ç»­å€¼æ¥è¯´ï¼Œæœ‰æŒç»­æ—¶é—´ï¼Œå¦‚ç”¨æˆ·æµè§ˆä¸€å®¶å•†æˆ·çš„æ—¶é—´ï¼›æœ‰é—´éš”æ—¶é—´ï¼Œå¦‚ç”¨æˆ·ä¸Šæ¬¡ç™»å½•ï¼ˆè´­ä¹°ã€ç‚¹å‡»ç­‰è¡Œä¸ºï¼‰è·ç°åœ¨çš„æ—¶é—´
* å¯¹äºç¦»æ•£å€¼æ¥è¯´ï¼Œæœ‰å¦‚ä¸‹ç‰¹å¾ï¼šä¸€å¤©ä¸­çš„å“ªä¸ªæ—¶é—´æ®µã€ä¸€å‘¨ä¸­çš„ç¬¬å‡ å¤©ã€ä¸€å¹´ä¸­çš„ç¬¬å‡ å‘¨ã€ä¸€å¹´ä¸­çš„ç¬¬å‡ ä¸ªæœˆã€ä¸€å¹´ä¸­çš„ç¬¬å‡ ä¸ªå­£åº¦ã€å·¥ä½œæ—¥orå‘¨æœ«ã€èŠ‚å‡æ—¥orä¿ƒé”€èŠ‚
* çª—ä½“å‹ç¼©åŒ– (Windowing)ï¼šå¦‚æœæ‰€æœ‰çš„ç‚¹éƒ½åˆ†å¸ƒåœ¨æ—¶é—´è½´ä¸Šï¼Œé‚£ä¹ˆåœ¨åŒä¸€ä¸ªçª—å£é‡Œçš„å…ˆå‰çš„ç‚¹å¾€å¾€åŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ã€‚

### 4.2 ç‰¹å¾æ‹†è§£

å°†ä¸€ä¸ªç‰¹å¾æ‹†ä¸ºå¤šä¸ª**æ›´æ˜“ç†è§£**çš„ç‰¹å¾ã€‚ ä¾‹å¦‚æ—¥æœŸï¼Œå¯ä»¥æ‹†ä¸ºå¹´ã€æœˆã€æ—¥ã€å°æ—¶ã€åˆ†ã€ç§’ã€æ˜ŸæœŸå‡ ã€æ˜¯å¦ä¸ºå‘¨æœ«ã€‚

### 5. ç©ºé—´ç‰¹å¾

### 6. æ–‡æœ¬ç‰¹å¾

* è¯è¢‹ï¼ˆword bagï¼‰:æŒ‡å¯¹äºæ–‡æœ¬æ•°æ®é¢„å¤„ç†åï¼Œå»æ‰åœç”¨è¯ï¼Œå‰©ä¸‹çš„è¯ç»„æˆçš„listï¼Œåœ¨è¯åº“ä¸­æ˜ å°„çš„ç¨€ç–å‘é‡
* nå…ƒè¯è¢‹ï¼šå°†è¯è¢‹ä¸­çš„è¯æ‰©å±•åˆ°n-gramï¼Œåˆ†è¯åç›¸é‚»çš„nä¸ªè¯ä¹Ÿè¿›å…¥è¯è¢‹
* TF-idfç‰¹å¾ï¼šä¸€ç§ç”¨æ¥è¯„ä¼°ä¸€ä¸ªå­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦çš„ç»Ÿè®¡æ–¹æ³•ã€‚å­—è¯çš„é‡è¦æ€§ä¸å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”ï¼Œä¸å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ã€‚TF(Term freqiency)ï¼ŒTF(t)=è¯tåœ¨å½“å‰æ–‡ä¸­å‡ºç°çš„æ¬¡æ•°/è¯tåœ¨å…¨éƒ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼ŒIDF(t)=ln(æ€»æ–‡æ¡£æ•°/å«tçš„æ–‡æ¡£æ•°)ï¼ŒTF-IDFæƒé‡=TF(t)*IDF(t)
* word2vecï¼šç°æœ‰çš„å·¥å…·æœ‰Google word2vecã€gensim

### 7. ç‰¹å¾å·¥ç¨‹ Tricks

* é€‰å‡ºæœ€é‡è¦çš„ä¸¤ä¸ªå˜é‡ï¼Œå¹¶è®¡ç®—ä»–ä»¬ç›¸äº’ä¹‹é—´ã€ä»¥åŠä¸å…¶å®ƒå˜é‡ä¹‹é—´çš„äºŒé˜¶äº¤å‰ä½œç”¨å¹¶æ”¾å…¥æ¨¡å‹ä¸­ï¼Œæ¯”è¾ƒç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹ç»“æœä¸æœ€åˆçš„çº¿æ€§æ¨¡å‹çš„ç»“æœã€‚
* ç™½åŒ–æ•°æ® (Whitening the Data)
* Så‹/æ­£åˆ‡/å¯¹æ•°è½¬æ¢ (Sigmoid / Tanh / Log Transformations)
* å»ç›¸å…³æ€§/è½¬æ¢å˜é‡ (decorrelation)



## äºŒ.  ç‰¹å¾é€‰æ‹© (Feature Selection) 











---

Feature engineering is a vital component of modelling process, and it is the toughest to automate. It takes domain expertise and a lot of exploratory analysis on the data to engineer features

1. single variable Basic transformations: x, x^2 ,sqrt x ,log x, scaling

2. If variable's distribution has a long tail, apply Box-Cox transformation (taking log() is a quick & dirty way).

3. One could also perform analysis of residuals or log-odds (for linear model) to check for strong nonlinearities.

4. Create a feature which captures the frequency of the occurrence of each level of the categorical variable. For high cardinality, this helps a lot. One might use ratio/percentage of a particular level to all the levels present.

5. For every possible value of the variable, estimate the mean of the target variable; use the result as an engineered feature.

6. Encode a variable with the ratio of the target variable.

7. Take the two most important variables and throw in second order interactions between them and the rest of the variables - compare the resulting model to the original linear one

8. if you feel your solutions should be smooth, you can apply a radial basis function kernel .  This is like applying a smoothing transform.  

9. If you feel you need covariates , you can apply a polynomial kernel, or add the covariates explicitly

10. High cardinality features : convert to numeric by preprocessing: out-of-fold average two variable combinations

11. Additive transformation

12. difference relative to baseline

13. Multiplicative transformation : interactive effects

14. divisive : scaling/normalisation

15. thresholding numerical features to get boolean values

16. Cartesian Product Transformation

17. Feature crosses: cross product of all features -- Consider a feature A, with two possible values {A1, A2}. Let B be a feature with possibilities {B1, B2}. Then, a feature-cross between A & B (letâ€™s call it AB) would take one of the following values: {(A1, B1), (A1, B2), (A2, B1), (A2, B2)}. You can basically give these â€˜combinationsâ€™ any names you like. Just remember that every combination denotes a synergy between the information contained by the corresponding values of A and B.

18. Normalization Transformation: -- One of the implicit assumptions often made in machine learning algorithms (and somewhat explicitly in Naive Bayes) is that the the features follow a normal distribution. However, sometimes we may find that the features are not following a normal distribution but a log normal distribution instead. One of the common things to do in this situation is to take the log of the feature values (that exhibit log normal distribution) so that it exhibits a normal distribution.If the algorithm being used is making the implicit/explicit assumption of the features being normally distributed, then such a transformation of a log-normally distributed feature to a normally distributed feature can help improve the performance of that algorithm.

19. Quantile Binning Transformation

20. whitening the data

21. Windowing -- If points are distributed in time axis, previous points in the same window are often very informative

22. Min-max normalization : does not necessarily preserve order

23. sigmoid / tanh / log transformations

24. Handling zeros distinctly â€“ potentially important for Count based features

25. Decorrelate / transform variables

26. Reframe Numerical Quantities

27. Map infrequent categorical variables to a new/separate category.

28.Sequentially apply a list of transforms.

29. One Hot Encoding

30. Target rate encoding

Hash Trick Multivariate:

31. PCA

32. MODEL STACKING

33. compressed sensing

34..guess the averageâ€ or â€œguess the average segmented by variable Xâ€

Projection : new basis

35. Hack projection:

Perform clustering and use distance between points to the cluster center as a feature
PCA/SVD -- Useful technique to analyze the interrelationships between variables and perform dimensionality reduction with minimum loss of information (find the axis through the data with highest variance / repeat with the next orthogonal axis and so on , until you run out of data or dimensions; Each axis acts a new feature)
36.Sparse coding -- choose basis : evaluate the basis  based on how well  you can use it to reconstruct the input and how sparse it is take some sort of gradient step to improve that evaluation

efficient sparse coding algorithms
deep auto encoders
37 :Random forest: train bunch of decision trees :use each leaf as a feature

## References
1. [æœºå™¨å­¦ä¹ ä¹‹ ç‰¹å¾å·¥ç¨‹](https://juejin.im/post/5b569edff265da0f7b2f6c65)
2. [The Comprehensive Guide for Feature Engineering](https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/)
3. [ã€æŒç»­æ›´æ–°ã€‘æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹å®ç”¨æŠ€å·§å¤§å…¨](https://zhuanlan.zhihu.com/p/26444240)
4. âœ³ï¸ [Machine Learning Kaggle Competition Part Two: Improving Feature engineering, feature selection, and model evaluation](https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8)
5. [Feature Engineering ç‰¹å¾µå·¥ç¨‹ä¸­å¸¸è¦‹çš„æ–¹æ³•](https://vinta.ws/code/feature-engineering.html)
6. [æœºå™¨å­¦ä¹ ä¹‹step by stepå®æˆ˜åŠçŸ¥è¯†ç§¯ç´¯ç¬”è®°](https://www.cnblogs.com/kidsitcn/p/9176602.html)
7. [ç‰¹å¾å·¥ç¨‹ï¼šæ•°æ®ç§‘å­¦å®¶çš„ç§˜å¯†æ­¦å™¨ï¼](https://yq.aliyun.com/articles/82611)
8. [æœºå™¨å­¦ä¹ é¡¹ç›®çš„å®Œæ•´æµç¨‹](https://blog.csdn.net/qq_24831889/article/details/83241104#53_badcase_186)
9. [è¿ç»­æ•°æ®çš„å¤„ç†æ–¹æ³•](https://www.leiphone.com/news/201801/T9JlyTOAMxFZvWly.html)
10. [æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å¤„ç†ç»¼è¿°](https://tech.meituan.com/2015/02/10/machinelearning-data-feature-process.html)
11. [pythonå¼€å‘ï¼šç‰¹å¾å·¥ç¨‹ä»£ç æ¨¡ç‰ˆ(ä¸€)](http://shataowei.com/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%B8%80/)
12. [pythonå¼€å‘ï¼šç‰¹å¾å·¥ç¨‹ä»£ç æ¨¡ç‰ˆ(äºŒ)](http://shataowei.com/2017/12/01/python%E5%BC%80%E5%8F%91%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81%E6%A8%A1%E7%89%88-%E4%BA%8C/)
13. [Normalization(æ ‡å‡†åŒ–)çš„åŸç†å’Œå®ç°è¯¦è§£](http://www.dongdongbai.com/index.php/2017/12/11/97/)
14. [æ•°æ®æŒ–æ˜çš„æµç¨‹å’Œæ–¹æ³•ã€æŠ€å·§æ€»ç»“](https://zhuanlan.zhihu.com/p/33429338)
15. [4.3. é¢„å¤„ç†æ•°æ®](http://doc.codingdict.com/sklearn/59/)
16. [scikit-learn preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
17. [A Comprehensive Guide to Data Exploration](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)
18. [**What are good ways to handle discrete and continuous inputs together?**](https://www.quora.com/What-are-good-ways-to-handle-discrete-and-continuous-inputs-together)
19. [One-hot vs dummy encoding in Scikit-learn](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn)
20. [å¦‚ä½•ç†è§£ç»Ÿè®¡å­¦ä¸­ã€Œè‡ªç”±åº¦ã€è¿™ä¸ªæ¦‚å¿µï¼Ÿ](https://www.zhihu.com/question/20983193)
21. [One-Hot ç¼–ç ä¸å“‘å˜é‡](http://www.jiehuozhe.com/article/3)
22. [Smarter Ways to Encode Categorical Data for Machine Learning (Part 1 of 3)](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159)
23. [pythonæ•°æ®å¤„ç†ï¼Œç‰¹å¾å·¥ç¨‹ï¼Œæ¯”èµ›ç­‰ä¸€å®šä¼šç”¨åˆ°çš„æ–¹æ³•](https://www.twblogs.net/a/5b8364342b71776c51e2d0b2/zh-cn)
24. [Feature Engineering: Data scientist's Secret Sauce !](https://www.linkedin.com/pulse/feature-engineering-data-scientists-secret-sauce-ashish-kumar)