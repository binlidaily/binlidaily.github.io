---
layout: post
title: Feature Extraction
author: Bin Li
tags: [Machine Learning]
category: ""
comments: true
published: true
typora-root-url: ../../../../binlidaily.github.io
---

è€Œåœ¨ SKlearn é‡Œé¢åŒºåˆ«äº†æ ‡å‡†åŒ– (StandardScaler) å’Œå½’ä¸€åŒ– (Normalizer) ç­‰ï¼Œè¦å¼„æ¸…æ¥šè¿™ä¸¤è€…ä¹‹é—´çš„[å…³ç³»](http://benalexkeen.com/feature-scaling-with-scikit-learn/)ã€‚

## 1. æ— é‡çº²åŒ–

ã€€ã€€ä¸ºäº†æ¶ˆé™¤æ•°æ®ç‰¹å¾ä¹‹é—´çš„é‡çº²å½±å“ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç‰¹å¾è¿›è¡Œæ— é‡çº²åŒ–å¤„ç†ï¼Œä½¿å¾—ä¸åŒæŒ‡æ ‡ä¹‹é—´å…·æœ‰å¯æ¯”æ€§ã€‚å¯ä»¥æ‹¿æ¢¯åº¦ä¸‹é™ä¸ºä¾‹æ¥æ¯”è¾ƒå¯¹æ•°å€¼å‹æ•°æ®æ— é‡çº²åŒ–å¤„ç†çš„ä¼˜åŠ¿ã€‚

![Scan Jun 13, 2019 at 11.28 AM](/img/media/Scan Jun 13, 2019 at 11.28 AM.jpg)

ã€€ã€€å¯ä»¥ä»ç­‰å€¼å›¾ä¸Šçœ‹å‡ºï¼Œæ²¡æœ‰å½’ä¸€åŒ–ï¼Œåœ¨ç›¸åŒçš„å­¦ä¹ é€Ÿç‡ä¸‹ï¼Œ$x_1$ çš„æ›´æ–°é€Ÿåº¦ä¼šå¤§äº $x_2$ï¼Œå³æ›´å¤šçš„è¿­ä»£æ‰èƒ½æ‰¾åˆ°æœ€ä¼˜è§£ï¼›å½’ä¸€åŒ–åï¼Œä¸¤è€…æ›´æ–°é€Ÿåº¦å˜å¾—ä¸€è‡´ï¼Œå®¹æ˜“æ›´å¿«åœ°é€šè¿‡æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æœ€ä¼˜è§£ã€‚

ã€€ã€€å½“ç„¶ï¼Œæ•°æ®å½’ä¸€åŒ–ä¹Ÿéä¸‡èƒ½ï¼Œä¸€èˆ¬æ¥è®²ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£çš„æ¨¡å‹é€šå¸¸éœ€è¦å½’ä¸€åŒ–ï¼Œåƒçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚åƒå†³ç­–æ ‘æ¨¡å‹ï¼Œä»¥ C4.5 ä¸ºä¾‹ï¼Œå†³ç­–æ ‘åœ¨èŠ‚ç‚¹åˆ†è£‚ä¸»è¦ä¾æ®æ•°æ®é›† $D$ å…³äºç‰¹å¾ $x$ çš„ä¿¡æ¯å¢ç›Šç‡ï¼Œä¿¡æ¯å¢ç›Šç‡è·Ÿç‰¹å¾æ˜¯å¦å½’ä¸€åŒ–æ— å…³ï¼Œæ‰€ä»¥å½’ä¸€åŒ–å¹¶ä¸ä¼šæ”¹å˜æ ·æœ¬åœ¨ç‰¹å¾ $x$ ä¸Šçš„ä¿¡æ¯å¢ç›Šã€‚

### 1.1 æ ‡å‡†åŒ–ç¼©æ”¾ (åˆç§° Z ç¼©æ”¾)

ã€€ã€€æ ‡å‡†åŒ–æŠŠç‰¹å¾è½¬åŒ–ä¸ºæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„å½¢å¼ï¼Œè®¡ç®—æ ‡å‡†åˆ†æ•° (Standard Score, Z-score)ï¼Œç»è¿‡å¤„ç†çš„æ•°æ®ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå¤„ç†æ–¹æ³•å¦‚ä¸‹ï¼š

$$
x^\prime =\frac{x-\mu}{\sigma}
$$

ã€€ã€€å…¶ä¸­ $\mu$ ä¸ºæ‰€æœ‰æ ·æœ¬æ•°æ®çš„å‡å€¼ï¼Œ$\sigma$ ä¸ºæ‰€æœ‰æ ·æœ¬æ•°æ®çš„æ ‡å‡†å·®ã€‚æ ‡å‡†åŒ–çš„åŸç†æ¯”è¾ƒå¤æ‚ï¼Œå®ƒè¡¨ç¤ºçš„æ˜¯åŸå§‹å€¼ä¸å‡å€¼ä¹‹é—´å·®å¤šå°‘ä¸ªæ ‡å‡†å·®ï¼Œæ˜¯ä¸€ä¸ªç›¸å¯¹å€¼ï¼Œæ‰€ä»¥ä¹Ÿæœ‰å»é™¤é‡çº²çš„åŠŸæ•ˆã€‚åŒæ—¶ï¼Œå®ƒè¿˜å¸¦æ¥ä¸¤ä¸ªé™„åŠ çš„å¥½å¤„ï¼šå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ã€‚

ã€€ã€€å‡å€¼ä¸º 0 æœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿå®ƒå¯ä»¥ä½¿æ•°æ®ä»¥ 0 ä¸ºä¸­å¿ƒå·¦å³åˆ†å¸ƒï¼Œè€Œæ•°æ®ä»¥ 0 ä¸ºä¸­å¿ƒå·¦å³åˆ†å¸ƒä¼šå¸¦æ¥å¾ˆå¤šä¾¿åˆ©ã€‚æ¯”å¦‚åœ¨å»ä¸­å¿ƒåŒ–çš„æ•°æ®ä¸Šåš SVD åˆ†è§£ç­‰ä»·äºåœ¨åŸå§‹æ•°æ®ä¸ŠåšPCAï¼›æœºå™¨å­¦ä¹ ä¸­å¾ˆå¤šå‡½æ•°å¦‚ Sigmoidã€Tanhã€Softmax ç­‰éƒ½ä»¥ 0 ä¸ºä¸­å¿ƒå·¦å³åˆ†å¸ƒï¼ˆä¸ä¸€å®šå¯¹ç§°ï¼‰ã€‚

ã€€ã€€è€Œé€šè¿‡[å…¬å¼å˜æ¢](https://www.jianshu.com/p/540d56ef350f)å¯ä»¥çŸ¥é“æ¯ä¸ªå˜é‡ï¼ˆç‰¹å¾ï¼‰çš„é‡è¦ç¨‹åº¦æ­£æ¯”äºè¿™ä¸ªå˜é‡åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šçš„æ–¹å·®ã€‚å¦‚æœæˆ‘ä»¬è®©æ¯ä¸€ç»´å˜é‡çš„æ ‡å‡†å·®éƒ½ä¸º 1ï¼ˆå³æ–¹å·®éƒ½ä¸º1ï¼‰ï¼Œæ¯ç»´å˜é‡åœ¨è®¡ç®—è·ç¦»çš„æ—¶å€™é‡è¦ç¨‹åº¦ç›¸åŒã€‚

ã€€ã€€Sklearn å®ç°ï¼š
1. ä½¿ç”¨ [sklearn.preprocessing.scale()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥å°†ç»™å®šæ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚
2. ä½¿ç”¨ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) ç±»ï¼Œä½¿ç”¨è¯¥ç±»çš„å¥½å¤„åœ¨äºå¯ä»¥ä¿å­˜è®­ç»ƒé›†ä¸­çš„å‚æ•°ï¼ˆå‡å€¼ã€æ–¹å·®ï¼‰ç›´æ¥ä½¿ç”¨å…¶å¯¹è±¡è½¬æ¢æµ‹è¯•é›†æ•°æ®ã€‚


ã€€ã€€æ³¨æ„ï¼š
* è®¡ç®—æ—¶å¯¹æ¯ä¸ªç‰¹å¾åˆ†åˆ«è¿›è¡Œã€‚å°†æ•°æ®æŒ‰ç‰¹å¾ï¼ˆæŒ‰åˆ—è¿›è¡Œï¼‰å‡å»å…¶å‡å€¼ï¼Œå¹¶é™¤ä»¥å…¶æ–¹å·®ã€‚å¾—åˆ°çš„ç»“æœæ˜¯ï¼Œå¯¹äºæ¯ä¸ªç‰¹å¾æ¥è¯´æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨ 0 é™„è¿‘ï¼Œæ–¹å·®ä¸º 1ã€‚
* å¦‚æœä¸ªåˆ«ç‰¹å¾æˆ–å¤šæˆ–å°‘çœ‹èµ·æ¥ä¸æ˜¯å¾ˆåƒ**æ ‡å‡†æ­£æ€åˆ†å¸ƒ(å…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®)**ï¼Œé‚£ä¹ˆå®ƒä»¬çš„è¡¨ç°åŠ›å¯èƒ½ä¼šè¾ƒå·®ã€‚
* ä¸å…ç–« outlierï¼Ÿ
* å¯¹ç›®æ ‡å˜é‡ä¸ºè¾“å…¥ç‰¹å¾çš„å…‰æ»‘å‡½æ•°çš„æ¨¡å‹ï¼Œå…¶è¾“å…¥ç‰¹å¾çš„å¤§å°æ¯”è¾ƒæ•æ„Ÿï¼Œå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ç¼©æ”¾æ¯”è¾ƒæœ‰æ•ˆã€‚
* å¯¹äºç¨€ç–æ•°æ®ï¼Œå¯ä»¥æ¥å— scipy.sparse çš„çŸ©é˜µä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶æŒ‡å®šå‚æ•°with_mean=False å–æ¶ˆä¸­å¿ƒåŒ–ï¼ˆcentering æ˜¯ç ´åæ•°æ®ç¨€ç–æ€§çš„åŸå› ï¼‰ï¼Œwith_std=False åˆ™ä¸åš scaling å¤„ç†ã€‚
* å¦‚æœæ•°å€¼ç‰¹å¾åˆ—ä¸­å­˜åœ¨æ•°å€¼æå¤§æˆ–æå°çš„ outlierï¼ˆé€šè¿‡EDAå‘ç°ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ [sklearn.preprocessing.RobustScaler](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) ï¼Œåº”è¯¥ä½¿ç”¨æ›´ç¨³å¥ï¼ˆrobustï¼‰çš„ç»Ÿè®¡æ•°æ®ï¼šç”¨ä¸­ä½æ•°è€Œä¸æ˜¯ç®—æœ¯å¹³å‡æ•°ï¼Œç”¨åˆ†ä½æ•°ï¼ˆquantileï¼‰è€Œä¸æ˜¯æ–¹å·®ã€‚è¿™ç§æ ‡å‡†åŒ–æ–¹æ³•æœ‰ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼šï¼ˆåˆ†ä½æ•°ä¸‹é™ï¼Œåˆ†ä½æ•°ä¸Šé™ï¼‰ï¼Œæœ€å¥½é€šè¿‡EDAçš„æ•°æ®å¯è§†åŒ–ç¡®å®šã€‚å…ç–« outlierã€‚

### 1.2 åŒºé—´ç¼©æ”¾ (Scaling)

ã€€ã€€æœ€å¤§æœ€å°å€¼ç¼©æ”¾å’Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ä¸¤ç§ç¼©æ”¾å±äº**åŒºé—´ç¼©æ”¾**ï¼Œä½¿ç”¨è¿™ç§ç¼©æ”¾çš„ç›®çš„åŒ…æ‹¬å®ç°ç‰¹å¾æå°æ–¹å·®çš„é²æ£’æ€§ä»¥åŠåœ¨ç¨€ç–çŸ©é˜µä¸­ä¿ç•™é›¶å…ƒç´ ã€‚

**1) æœ€å¤§æœ€å°å€¼ç¼©æ”¾**

ã€€ã€€æœ€å¤§æœ€å°ç¼©æ”¾æ˜¯å°†ç‰¹å¾ç¼©æ”¾åˆ°ç»™å®šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´ï¼Œé€šå¸¸åœ¨é›¶å’Œä¸€ä¹‹é—´ã€‚

$$
{x}^\prime=\frac{x-x_{Min}}{x_{Max}-x_{Min}}
$$

ã€€ã€€Sklearn å®ç°ï¼š
1. ä½¿ç”¨ [sklearn.preprocessing.minmax_scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) å‡½æ•°å¯¹æŒ‡å®šæ•°æ®ã€‚
2. ä½¿ç”¨ [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) ç±»æ–¹ä¾¿æ‹“å±•ã€‚
    * å½“ç„¶ï¼Œåœ¨æ„é€ ç±»å¯¹è±¡çš„æ—¶å€™ä¹Ÿå¯ä»¥ç›´æ¥æŒ‡å®šæœ€å¤§æœ€å°å€¼çš„èŒƒå›´ï¼šfeature_range=(min, max)ï¼Œæ­¤æ—¶åº”ç”¨çš„å…¬å¼å˜ä¸ºï¼š

```python
X_std=(X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0))
X_scaled=X_std/(max-min)+min
```

ã€€ã€€æ³¨æ„ï¼š
* è¿™ç§å½’ä¸€åŒ–æ–¹æ³•æ¯”è¾ƒé€‚ç”¨åœ¨æ•°å€¼æ¯”è¾ƒé›†ä¸­çš„æƒ…å†µã€‚
* ä¸¤ä¸ªç¼ºé™·ï¼š
  * å½“æœ‰æ–°æ•°æ®åŠ å…¥æ—¶ï¼Œå¯èƒ½å¯¼è‡´ max å’Œ min å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°å®šä¹‰ã€‚
  * å¦‚æœ max å’Œ min ä¸ç¨³å®šï¼Œå¾ˆå®¹æ˜“ä½¿å¾—å½’ä¸€åŒ–ç»“æœä¸ç¨³å®šï¼Œä½¿å¾—åç»­ä½¿ç”¨æ•ˆæœä¹Ÿä¸ç¨³å®šã€‚å®é™…ä½¿ç”¨ä¸­å¯ä»¥ç”¨ç»éªŒå¸¸é‡å€¼æ¥æ›¿ä»£ max å’Œ minã€‚

2) æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾

ã€€ã€€åœ¨å®é™…æƒ…å†µä¸­ï¼Œæˆ‘ä»¬ç»å¸¸å¿½ç•¥ç‰¹å¾çš„åˆ†å¸ƒå½¢çŠ¶ï¼Œç›´æ¥ç»è¿‡å»å‡å€¼æ¥å¯¹æŸä¸ªç‰¹å¾è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œå†é€šè¿‡é™¤ä»¥éå¸¸é‡ç‰¹å¾(non-constant features)çš„æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾ã€‚è€Œå¯¹ç¨€ç–æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–ä¼šç ´åç¨€ç–æ•°æ®çš„ç»“æ„ï¼Œè¿™æ ·åšæ²¡ä»€ä¹ˆæ„ä¹‰ã€‚ä½†å¦‚æœç¨€ç–æ•°æ®çš„ç‰¹å¾è·¨è¶Šä¸åŒæ•°é‡çº§çš„æƒ…å†µä¸‹ä¹Ÿæœ€å¥½è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾å°±å¯ä»¥æ´¾ä¸Šç”¨åœºäº†ã€‚

ã€€ã€€æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾æŒ‰ç…§æ¯ä¸ªç‰¹å¾çš„æœ€å¤§ç»å¯¹å€¼è¿›è¡Œç¼©æ”¾ï¼ˆé™¤ä»¥æœ€å¤§ç»å¯¹å€¼ï¼‰ï¼Œä½¿å¾—æ¯ä¸ªç‰¹å¾çš„èŒƒå›´å˜æˆäº† $[-1, 1]$ï¼Œè¯¥æ“ä½œä¸ä¼šç§»åŠ¨æˆ–è€…å±…ä¸­æ•°æ®ï¼Œæ‰€ä»¥ä¸ä¼šç ´åç¨€ç–æ€§ã€‚

1ã€ä½¿ç”¨ [sklearn.preprocessing.maxabs_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.maxabs_scale.html) å‡½æ•°å®ç°ï¼š

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.maxabs_scale(X)

>>> X_scaled                                          
array([[ 0.5, -1. ,  1. ],
       [ 1. ,  0. ,  0. ],
       [ 0. ,  1. , -0.5]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([0.5       , 0.        , 0.16666667])

>>> X_scaled.std(axis=0)
array([0.40824829, 0.81649658, 0.62360956])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler) ç±»å®ç°ï¼š

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> max_abs_scaler = preprocessing.MaxAbsScaler()
>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
>>> X_train_maxabs                # doctest +NORMALIZE_WHITESPACE^
array([[ 0.5, -1\. ,  1\. ],
 [ 1\. ,  0\. ,  0\. ],
 [ 0\. ,  1\. , -0.5]])
# æµ‹è¯•é›†
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_maxabs = max_abs_scaler.transform(X_test)
>>> X_test_maxabs                 
array([[-1.5, -1\. ,  2\. ]])
>>> max_abs_scaler.scale_         
array([ 2.,  1.,  2.])
```

ğŸ½æ³¨æ„ï¼š
* ä½¿ç”¨æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ä¹‹å‰åº”è¯¥ç¡®è®¤ï¼Œè®­ç»ƒæ•°æ®åº”è¯¥æ˜¯å·²ç»é›¶ä¸­å¿ƒåŒ–æˆ–è€…æ˜¯ç¨€ç–æ•°æ®ã€‚
* è¯¥æ“ä½œä¸ä¼šç§»åŠ¨æˆ–è€…å±…ä¸­æ•°æ®ï¼Œæ‰€ä»¥ä¸ä¼šç ´åç¨€ç–æ€§ã€‚

3) å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

å½’ä¸€åŒ–æ˜¯**ç¼©æ”¾å•ä¸ªæ ·æœ¬ä»¥å…·æœ‰å•ä½èŒƒæ•°**çš„è¿‡ç¨‹ï¼Œå³å˜æ¢åçš„å•è¡Œæ•°æ®æ ·æœ¬çš„èŒƒæ•°ç­‰äº1ï¼ˆå¥½å¤„ï¼ŸğŸ¤”ï¼‰ã€‚å¦‚æœä½ è®¡åˆ’ä½¿ç”¨äºŒæ¬¡å½¢å¼(å¦‚ç‚¹ç§¯æˆ–ä»»ä½•å…¶ä»–æ ¸å‡½æ•°)æ¥é‡åŒ–ä»»ä½•æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦ï¼Œåˆ™æ­¤è¿‡ç¨‹å°†éå¸¸æœ‰ç”¨ã€‚è¿™æ˜¯æ–‡æœ¬åˆ†ç±»æˆ–èšç±»çš„å¸¸ç”¨æ“ä½œï¼Œä¾‹å¦‚ï¼Œå¯¹äºä¸¤ä¸ª TF-IDF å‘é‡çš„ l2-norm è¿›è¡Œç‚¹ç§¯ï¼Œå°±å¯ä»¥å¾—åˆ°è¿™ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚

æ•°æ®å½’ä¸€åŒ–å°±æ˜¯å°†è®­ç»ƒé›†ä¸­æŸä¸€åˆ—æ•°å€¼ç‰¹å¾çš„å€¼ç¼©æ”¾åˆ°0å’Œ1ä¹‹é—´ã€‚

**æ³¨æ„å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–çš„åŒºåˆ«**ï¼šæ ‡å‡†åŒ–ä½œç”¨äºæ¯ä¸ªç‰¹å¾åˆ—ï¼Œé€šè¿‡å»å‡å€¼å’Œç¼©æ”¾ä»¥æ–¹å·®å€¼çš„æ–¹å¼å°†æ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾åˆ—è½¬åŒ–åˆ°åŒä¸€é‡çº²ä¸‹ï¼›å½’ä¸€åŒ–ä½œç”¨äºæ¯ä¸€æ•°æ®è¡Œï¼Œé€šè¿‡ç¼©æ”¾ä»¥åŸæ ·æœ¬çš„æŸä¸ªèŒƒæ•°ä½¿å¾—è®¡ç®—æ ·æœ¬é—´ç›¸ä¼¼åº¦çš„æ—¶å€™æœ‰ç»Ÿä¸€çš„æ ‡å‡†ã€‚

1ã€[sklearn.preprocessing.normalize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) å‡½æ•°æä¾›äº†ä¸€ä¸ªå¿«é€Ÿç®€å•çš„æ–¹æ³•åœ¨ç±»ä¼¼æ•°ç»„çš„æ•°æ®é›†ä¸Šæ‰§è¡Œæ“ä½œï¼Œä½¿ç”¨ `l1` æˆ– `l2`èŒƒå¼:

```python
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> X_normalized = preprocessing.normalize(X, norm='l2')

>>> X_normalized                                      
array([[ 0.40..., -0.40...,  0.81...],
 [ 1\.  ...,  0\.  ...,  0\.  ...],
 [ 0\.  ...,  0.70..., -0.70...]])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.Normalizer](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) ç±»æ¥å½’ä¸€åŒ–ï¼ŒæŠŠæ¯ä¸€è¡Œæ•°æ®å½’ä¸€åŒ–ï¼Œä½¿ä¹‹æœ‰å•ä½èŒƒæ•°ï¼ˆUnit Normï¼‰ï¼Œnorm çš„ç§ç±»å¯ä»¥é€‰l1ã€l2æˆ–maxã€‚ä¸å…ç–«outlierã€‚


$$
\vec{x^{\prime}}=\frac{\vec{x}}{l(\vec{x})}
$$


å…¶ä¸­ $l$ è¡¨ç¤º $norm$ å‡½æ•°ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ `fit` æ–¹æ³•æ˜¯æ— ç”¨çš„ï¼šè¯¥ç±»æ˜¯æ— çŠ¶æ€çš„ï¼Œå› ä¸ºè¯¥æ“ä½œç‹¬ç«‹å¯¹å¾…æ ·æœ¬ã€‚

```python
>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
>>> normalizer
Normalizer(copy=True, norm='l2')
>>> normalizer.transform(X)                            
array([[ 0.40..., -0.40...,  0.81...],
 [ 1\.  ...,  0\.  ...,  0\.  ...],
 [ 0\.  ...,  0.70..., -0.70...]])

>>> normalizer.transform([[-1.,  1., 0.]])             
array([[-0.70...,  0.70...,  0\.  ...]])
```

**2.5.4 å¸¦æœ‰å¼‚å¸¸å€¼çš„ç¼©æ”¾**

å¦‚æœä½ çš„æ•°æ®åŒ…å«è®¸å¤šå¼‚å¸¸å€¼ï¼Œä½¿ç”¨å‡å€¼å’Œæ–¹å·®ç¼©æ”¾å¯èƒ½å¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ robust_scale ä»¥åŠ RobustScaler ä½œä¸ºæ›¿ä»£å“ã€‚å®ƒä»¬å¯¹ä½ çš„æ•°æ®çš„ä¸­å¿ƒå’ŒèŒƒå›´ä½¿ç”¨æ›´æœ‰é²æ£’æ€§çš„ä¼°è®¡ã€‚

1ã€ä½¿ç”¨ [sklearn.preprocessing.robust_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html) å‡½æ•°ï¼š

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -2.,  2.],
...               [ -2.,  1.,  3.],
...               [ 4.,  1., -2.]])
>>> X_scaled = preprocessing.robust_scale(X)

>>> X_scaled                                          
array([[ 0. , -2. ,  0. ],
       [-1. ,  0. ,  0.4],
       [ 1. ,  0. , -1.6]])

>>> #å¤„ç†åæ•°æ®çš„å‡å€¼å’Œæ–¹å·®
>>> X_scaled.mean(axis=0)
array([ 0.        , -0.66666667, -0.4       ])

>>> X_scaled.std(axis=0)
array([0.81649658, 0.94280904, 0.86409876])
```

2ã€ä½¿ç”¨ [sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler) ç±»ï¼š

```python
>>> from sklearn.preprocessing import RobustScaler
>>> X = [[ 1., -2.,  2.],
...      [ -2.,  1.,  3.],
...      [ 4.,  1., -2.]]
>>> transformer = RobustScaler().fit(X)
>>> transformer
RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
       with_scaling=True)
>>> transformer.transform(X)
array([[ 0. , -2. ,  0. ],
       [-1. ,  0. ,  0.4],
       [ 1. ,  0. , -1.6]])
```

**2.5.5 ç¨€ç–æ•°æ®çš„ç¼©æ”¾**

ä¸­å¿ƒåŒ–ç¨€ç–ï¼ˆçŸ©é˜µï¼‰æ•°æ®ä¼šç ´åæ•°æ®çš„ç¨€ç–ç»“æ„ï¼Œå› æ­¤å¾ˆå°‘æœ‰ä¸€ä¸ªæ¯”è¾ƒæ˜æ™ºçš„å®ç°æ–¹å¼ã€‚ä½†æ˜¯ç¼©æ”¾ç¨€ç–è¾“å…¥æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå°¤å…¶æ˜¯å½“å‡ ä¸ªç‰¹å¾åœ¨ä¸åŒçš„é‡çº§èŒƒå›´æ—¶ï¼Œæœ€æ¨èçš„ç¼©æ”¾æ–¹å¼æ˜¯é‡‡ç”¨æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ï¼Œå…·ä½“æ“ä½œæ–¹å¼å‚è€ƒä¸Šè¿°å¯¹åº”ç« èŠ‚ã€‚

**2.5.6 å¯¹æ•°ç¼©æ”¾ï¼ˆæœ‰ååº¦çš„æ­£æ€åˆ†å¸ƒï¼‰**

å¦‚æœæ•°æ®ä¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œå°¤å…¶æ˜¯æ•°æ®çš„å¹³å‡æ•°å’Œä¸­ä½æ•°ç›¸å·®å¾ˆå¤§çš„æ—¶å€™ï¼ˆè¡¨ç¤ºæ•°æ®éå¸¸æ­ªæ–œï¼‰ã€‚

1ã€å¯¹ Numpy Array ç±»å‹çš„æ•°æ®å¤„ç†ï¼š

```python
log_data = np.log(data)
# fcc_survey_df['Income_log'] = np.log((1+ fcc_survey_df['Income']))
```

2ã€å¯¹ Pandas DataFrame æ•°æ®çš„å¤„ç†ï¼š

```python
data_df[col] = data_df[col].map(lambda x : np.log1p(x))
```

**2.5.7 å…¶ä»–ç¼©æ”¾å¾…æ•´ç†**

* å¹³æ–¹æ ¹ç¼©æ”¾
* åä½™åˆ‡å‡½æ•°ç¼©æ”¾



## References
1. [å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–çš„ä¸€äº›ç†è§£](https://www.jianshu.com/p/540d56ef350f)
2. [Tips-of-Feature-engineering](https://github.com/Pysamlam/Tips-of-Feature-engineering)
3. [ä½¿ç”¨sklearnåšå•æœºç‰¹å¾å·¥ç¨‹](https://www.cnblogs.com/jasonfreak/p/5448385.html)