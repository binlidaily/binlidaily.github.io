---
layout: post
title: Learn Pandas From Scratch
subtitle:
author: Bin Li
tags: [Python, Pandas]
image: 
comments: true
published: True
---

## 数据处理
#### Python 中复制一个 DataFrame
```python
data1 = data_raw.copy(deep = True)
```

#### 读入 *.csv 格式的数据
```python
train_df = pd.read_csv('train_blog_reading.csv', delimiter=',', index_col='uid')
```

#### 查看数据的列名
```python
train_df.columns
```

#### 舍弃某一列
```python
train_df.drop(['Item_Outlet_Sales'], axis=1, inplace=True)
```

#### 通过列元素取值区间截取子集

```
df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]
```

#### 连接训练数据和测试数据

```python
combi = train_df.append(test_df, ignore_index=True)
```

#### 检查所有的缺失数据

```python
combi.isnull().sum()
```

#### 给缺失值填充数据

```python
# Fill NA/NaN values using the specified method
# imputing missing data
# numerical data
combi['Item_Weight'].fillna(combi['Item_Weight'].mean(), inplace = True)
# categorical data
combi['Outlet_Size'].fillna("missing", inplace = True)
```

#### 查看数据每一列的取值和每个取值出现的次数

```python
combi['Item_Fat_Content'].value_counts()
```

#### 将为数不多的取值映射到固定几个取值上

```python
# >> combi['Item_Fat_Content'].value_counts()
# Low Fat    5089
# Regular    2889
# LF          316
# reg         117
# low fat     112
# Name: Item_Fat_Content, dtype: int64

# only two categories: Low Fat, Regular

# dictionary to replace the categories
fat_content_dict = {'Low Fat':0, 'Regular':1, 'LF':0, 'reg':1, 'low fat':0}

combi['Item_Fat_Content'] = combi['Item_Fat_Content'].replace(fat_content_dict, regex=True)
```

还有一种写法：

```python
combi.replace({'Item_Fat_Content': fat_content_dict}, inplace=True)
```

#### 将数据重新排序
由于对数据进行做特征工程（例如利用 Featuretools 自动抽取特征）时会导致数据顺序发生变化，若想变回原来的数据顺序，可以用如下的代码：

```python
feature_matrix = feature_matrix.reindex(index=combi['id'])
# TODO what is this step for?
feature_matrix = feature_matrix.reset_index()
```

#### **iloc, loc and ix**
ix 已经开始过时了。

* loc gets rows (or columns) with particular labels from the index.
* iloc gets rows (or columns) at particular positions in the index (so it only takes integers).

```shell
>>> s.iloc[:3] # slice the first three rows
49   NaN
48   NaN
47   NaN

>>> s.loc[:3] # slice up to and including label 3
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN

>>> s.ix[:3] # the integer is in the index so s.ix[:3] works like loc
49   NaN
48   NaN
47   NaN
46   NaN
45   NaN
1    NaN
2    NaN
3    NaN
```

当然再二维的时候也是类似的延伸。

需要取某一列的所有元素并且得到 Series 类型结果的话可以用：

```python
df.ix[:, n]
```

#### 字符串中提取字段
Name属性是类似的形式 'Braund, Mr. Owen Harris'、'Futrelle, Mrs. Jacques Heath (Lily May Peel)'和'Heikkinen, Miss. Laina'，现在想在Name属性中截取出称呼，可以用如下的方式：
```python
combi['Title'] = combi['Name'].str.extract('([A-Za-z]+)\.', expand=True)
```
`expand=True`使得得到的结果是 Dataframe，否则为 Series。结果就变成了 Mr, Mrs 和 Miss。

#### 去除为空的部分

```python
# drop the *rows* containing null items
# then return the non-null *rows*
df.dropna()
# drop the items that are null
# and return the non-null item in specific column
dr['col_name'].dropna()
```

#### 填充为空的部分

```python
DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)
# Fill NA/NaN values using the specified method
```
#### 存储 csv 文件

```python
my_submission = pd.DataFrame({'PassengerId': passenger_id, 'Survived': y_pred})
my_submission.to_csv('auto_ft_submission.csv', index=False)
```

#### 连续数值类型离散化 cut & qcut 函数
[这个问答](https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut)做了一定的解释，基本上如果要对连续数值类型的特征进行划分，可以尝试利用 cut 和 qcut 两个函数。

cut 函数主要是考虑在分桶的时候根据每个元素的取值能够平滑的划分每个区间，其实从数值角度考虑；而 qcut 是划分分位点，即其实从划分得到的每个区域内，元素个数要均衡的从角度出发划分区间的。

```python
# Binning numerical columns
# q=4 means 4 quantiles 0, 1, 2, 3
# labels=False are numbers, not characters
data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )
```

还有一种自己定义范围的方式：
```python
# Define the bins
mybins = range(0, df.age.max(), 10)

# Cut the data from the DataFrame with the help of the bins
df['age_bucket'] = pd.cut(df.age, bins=mybins)

# Count the number of values per bucket
df['age_bucket'].value_counts()
```

#### 数值特征离散化
直接离散化，从0开始以自然数增长的形式加以区别每个类：

```python
# Factorize the values 
labels, uniques = pd.factorize(trian_df.Class)

# Save the encoded variables in `iris.Class`
trian_df.Class = labels

# Print out the first rows
trian_df.Class.head()
```

#### 数值特征归一化
将离散数据归一化到在零附近。

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler().fit(X)

rescaledX = scaler.transform(X)
```

#### 给 DataFrame 中某一个位置赋值
假如在处理异常值，有一行数值有问题需要更正，我们按要求查找 label 为'音乐'的那一个位置并将其赋值为1，可以如下操作：

```python
train_df.at[train_df.label=='音乐', 'label'] = 1
```

#### Groupby操作的理解
以两个维度作为 groupby 的依据。

## References
[A Hands-On Guide to Automated Feature Engineering using Featuretools in Python](https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/)